#  Handling and Visualizing Spatial Data in R {#spatdata}

## Getting Environmental (Raster) data into R

Some of the data sources listed in the tables from the previous
session can be accessed with the geodata package.
```{r}
library(geodata)

# get worldclim data for mexico at 0.5 minutes
# this fails for me on Mac Mojave because of a certification issue.  Lame.
# But I can download it directly
#mex_tmin <- worldclim_country("Mexico", "tmin", res = 0.5, path = "/tmp")
#worldclim_country("Mexico", "tmax", res = 0.5, path = "/tmp")

# After downloading
library(terra)
#mex_tmin <- rast("~/Downloads/MEX_wc2.1_30s_tmin.tif")
mex_tmax <- rast("~/Downloads/MEX_wc2.1_30s_tmax.tif")
#mex_prec <- rast("~/Downloads/MEX_wc2.1_30s_prec.tif")
#mex_bio <-  rast("~/Downloads/MEX_wc2.1_30s_bio.tif")

#plot(mex_tmin)
```

```{r}
# looks reasonable, let's crop it:
yp_extent <- ext(c(-91, -86.5, 19.5, 21.75 ))
#yp_tmin <- crop(mex_tmin, yp_extent)
yp_tmax <- crop(mex_tmax, yp_extent)
#yp_prec <- crop(mex_prec, yp_extent)
#yp_bio <- crop(mex_bio, yp_extent)

plot(yp_tmax)
```

That is all happy.  Let's put this into a data folder somewhere
```{r}
dir.create("data/spatial/worldclim", recursive = TRUE, showWarnings = FALSE)
writeRaster(yp_tmax, "data/spatial/worldclim/tmax.tif", overwrite = TRUE)

```


## Playing around with the Yucatan Jay

```{r}
library(lubridate)
library(tidyverse)

yjay_full <- read_tsv("data/spatial/Cyanocorax-yucatanicus.tsv.gz")

# that is a big data set, with a lot of columns, and observations from
# a lot of different sources.  Let's whittle it down to something
# that is easier to see.
yjay_thin <- yjay_full %>%
  filter(
    str_detect(collectionCode, "EBIRD"), # keep just the EBIRD-associated records
    year == 2017                         # from the year 2017
  ) %>%
  select(  # whittle it down to just a few columns of data
    gbifID,
    decimalLongitude,
    decimalLatitude,
    day,
    month,
    year,
    individualCount,
    stateProvince,
    locality
  )


# let's make a quick map:
library(rnaturalearth)
library(rnaturalearthdata)
library(sf)

mex_states <- ne_states(
  country = "mexico",
  returnclass = "sf"
)

ggplot() +
  geom_sf(data = mex_states) +
  geom_point(data = yjay_thin, aes(x = decimalLongitude, y = decimalLatitude), colour = "red") +
  theme_bw() +
  coord_sf(xlim = c(-95, -85), ylim = c(12, 22))





```

Cool.  Let's focus on that in the core area:
```{r}
ggplot() +
  geom_sf(data = mex_states) +
  geom_point(data = yjay_thin, aes(x = decimalLongitude, y = decimalLatitude), colour = "red") +
  theme_bw() +
  coord_sf(xlim = c(-94, -86), ylim = c(16, 22))



```

Cool.  Now, it might be fun to look at collection dates.
```{r}
ggplot() +
  geom_sf(data = mex_states) +
  geom_point(data = yjay_thin, aes(x = decimalLongitude, y = decimalLatitude), colour = "red") +
  theme_bw() +
  coord_sf(xlim = c(-94, -86), ylim = c(16, 22)) +
  facet_wrap(~month)

```

## How about population densities?

```{r}
library(geodata)

#pop <- population(2020, 0.5, path = "/tmp")

# the above fails because of the certificate issues on my laptop, but
# the error message gives me the URL:
pop_url <- "https://biogeo.ucdavis.edu/data/geodata/pop/gpw_v4_population_density_rev11_2020_30s.tif"

# so we can just download that from a browser and then read it
pop <- rast("~/Downloads/gpw_v4_population_density_rev11_2020_30s.tif")

yp_pop <- crop(pop, yp_extent)

plot(yp_pop)
```

OK, that is pretty cool.

## Now, How about finding roads?

The NYU Faculty Archive has a great repository of street shapefiles
by province in Mexico.  Here is the link for the page for Yucatan:
[https://archive.nyu.edu/handle/2451/37126](https://archive.nyu.edu/handle/2451/37126)

And this downloads the zipped shapefile:
[https://archive.nyu.edu/bitstream/2451/37126/1/nyu_2451_37126.zip](https://archive.nyu.edu/bitstream/2451/37126/1/nyu_2451_37126.zip)

After download that and unzipping it we get:
```{r}
yuc_roads <- st_read("~/Downloads/nyu_2451_37126/yuc_eje_vial.shp")
```

It looks like Carretera is the word for highway. There are 32 of those.
```{r}
yuc_roads %>% 
  ggplot() + 
  geom_sf()
```
Nope, this data set is not useful to us.

## How about Yucatan counties?

We might be better off just looking at how you can intersect
points (eBird) with some polygons, to see which counties they
occur in.  That shows one simple kind of geometric operation.

It looks like we can get the counties at:
[https://archive.nyu.edu/bitstream/2451/37132/1/nyu_2451_37132.zip](https://archive.nyu.edu/bitstream/2451/37132/1/nyu_2451_37132.zip)

It is a small file.  I will download it and put it into the
repo with the name `yucatan_counties`.

Then we should be able to read it like:
```{r}
yuc_county <- st_read("data/spatial/yucatan_counties/yuc_municipio.shp")

ggplot(yuc_county) +
  geom_sf()
```




Marius and I brainstormed on this.  We will do an example
that involves the Yucatan Jay (or, better yet, ask Richard
if a more interesting/iconic species is available).  ECA has downloaded
occurrences from GBIF (3 Mb compressed) and has put them
into `data`.

We can use that and some of the raster data and some
vector data to do a little exploration of how to use the
`sf` package in combination with ggplot2.

Also, some filtering in the tidyverse.

Ideas:

- Do some filtering of the data set (tidyverse reprise)
- Talk about lubridate for dealing with dates and times
- facet wrap over months to look at occurrences, etc.
- Some very light geometrical operations, for example get all the
observations within X km of some features (like highways).
- Explore the use of different raster backgrounds.


## Appendix: Obtaining and Reducing The Example Spatial Datasets

We don't want our teaching repository to be too large, so I
subsetted our example data down to just the Yucatán region.
The process for doing with the different data sets is shown here.

### Raster Data

To get the environmental data, I used the 'geodata' package, which
makes it easy to download the data sets. Unfortunately, my computer
is old enough that one of the security certificates on it does now
allow direct downloading via R of the datasets, but the error message
it gave told me the URL for the data set, and I could download it
with my browser, then read it in.  I use namespace addressing
(i.e., `package::function()`) to be explicit about which package
each of the functions is in.

#### Worldclim Data

```{r, eval=FALSE}

# get worldclim data for mexico at 0.5 minutes
# this fails for me on Mac Mojave because of a certification issue.  Lame.
mex_tmax <- geodata::worldclim_country(
  country = "Mexico", 
  var = "tmax", 
  res = 0.5, 
  path = "/tmp"
)

# the error from that showed me that the URL was:
# https://biogeo.ucdavis.edu/data/worldclim/v2.1/tiles/iso/MEX_wc2.1_30s_tmax.tif

mex_tmax <- terra::rast("~/Downloads/MEX_wc2.1_30s_tmax.tif")

# I used the same procedure to get the BioClim variables
mex_bioc <-  terra::rast("~/Downloads/MEX_wc2.1_30s_bio.tif")
```

Those rasters are very large, so we focus only on the area
around the Yucatán Peninsula.  To do this we define a latitude and
longitude _extent_, and then `crop()` the raster to that extent, using
functions from the 'terra' package.
```{r}
# Here is the extent that includes the Yucatan Province
yp_extent <- terra::ext(c(-91, -86.5, 19.5, 21.75 ))


# now we crop the raster of all of Mexico down to just
# a small part.
yp_tmax <- crop(mex_tmax, yp_extent)
yp_bioc <- crop(mex_bioc, yp_extent)
```

Finally, we save just that small Yucatán part of the raster in
our repository here, so everyone can use it.
```{r}
dir.create("data/spatial/worldclim", recursive = TRUE, showWarnings = FALSE)
writeRaster(yp_tmax, "data/spatial/worldclim/yp_tmax.tif", overwrite = TRUE)
writeRaster(yp_bioc, "data/spatial/worldclim/yp_bioc.tif", overwrite = TRUE)
```