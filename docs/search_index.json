[["index.html", "Reproducible Research for Conservation — Mérida, Mexico Preface 0.1 Setting up your computer", " Reproducible Research for Conservation — Mérida, Mexico Eric C. Anderson, Christen Bossu, Richard Evan Feldman, Kristen C. Ruegg, Marius Somveille 2022-01-11 Preface This is the website/book associated with the UCMexus Conservation Genomics Workshop 2022, held January 10th and 11th in Mérida, Mexico. 0.1 Setting up your computer This course covers topics in landscape genetics/genomics, and relies heavily on the R programming language. In order to follow along with the code and be successful in running all of the examples, it is imperative to have very recent versions of R and RStudio, and updated versions of a number of packages. The following is a description of the software needed to engage in the course. This setup was tested on a Mac running Mojave 10.14.6, but should also work on most other Mac or Windows operating systems. If you are running Linux, there will be some external dependencies to install (such as the GEOS library), that are actually wrapped up in the binary versions of packages ‘terra’ and ‘sf’ on CRAN for Mac and Windows. 0.1.1 Step 1. Get the latest versions of R and RStudio First, install the latest version of R. Go to https://cran.r-project.org/ and follow the appropriate link to Download and Install, depending on your operating system (Linux, MacOS, or Windows). For Mac, you can download R-4.1.2.pkg and install. For Windows, first go into the base directory and Download R 4.1.2, and install it. THEN, go back to “R for Window” page where you clicked into base, and download and install the Rtools as well. The latter gives you tools for building packages, which is required for a few packages we use. Download and install the latest stable version of RStudio for your operating system. Go to https://www.rstudio.com/products/rstudio/download/ and choose the big blue download button for “RStudio Desktop, Open Source License, Free,” then hit the download button on the next page and follow instructions to install RStudio. 0.1.2 Step 2. Install a number of R packages that are relatively easy to install Our work will require a number of packages that can be found in binary form on CRAN. As such, installing them is typically not to arduous. Sometimes, when installing packages, you may get a message telling you that a later version of the package you want is available in source form than in binary form. Typically, it is still easiest, fastest, and usually reliable, to just use the binary form. So, after R gives you such a message, if you see R asking a question like: Do you want to install from sources the package which needs compilation? (Yes/no/cancel) Usually the appropriate answer is to type no into the console, and hit return. I have found that sometimes, when requesting the installation of a large number of packages, there can be the occasional problem. Sometimes, an error message tells you which package (often a dependency) failed to install. If that is the case, try to install the failed package by itself with the install.packages(), function, directly, and then re-run the install.packages() command that originally failed. You can install packages in a few rounds of different types. Note that I tend to use the RStudio CRAN mirror. 0.1.2.1 RMarkdown related packages To build the entire bookdown project of notes we need the ‘bookdown’ package and some other related package. Installing these also installs a bunch of other things that are essential for using RMarkdown. install.packages( c( &quot;knitr&quot;, &quot;rmarkdown&quot;, &quot;bookdown&quot;, &quot;kableExtra&quot; # for making nice tables in Rmarkdown ), repos = &quot;https://cran.rstudio.com&quot; ) 0.1.2.2 Tidyverse related packages The next big slug of packages to install are the tidyverse packages, and a few other related ones. Typically this can be done by just installing ‘tidyverse’ itself, and then a few more that we need. If you already have the core packages of the tidyverse, (see this section), you might need to reinstall these to ensure you have the latest versions. install.packages( c( &quot;tidyverse&quot;, &quot;broom&quot;, &quot;modelr&quot;, &quot;googledrive&quot;, &quot;googlesheets4&quot;, &quot;ggrepel&quot;, # for fitting labels in space into ggplots &quot;ggpubr&quot;, # ggplot based publication ready plots &quot;cowplot&quot;, # for arranging ggplots together &quot;RColorBrewer&quot;, # a color palette for graphing &quot;viridis&quot;, # a color palette for graphing &quot;data.table&quot; # not really a tidyverse package, but # useful for many things. ), repos = &quot;https://cran.rstudio.com&quot; ) 0.1.2.3 Geospatial-related packages The last collection of packages to install are a few geospatial packages for handling raster and vector spatial data. A few of these, like ‘sf’ and ‘terra,’ are large packages because they include all the underlying libraries for geometric computations, etc. This means that, as long as you get the binary versions from CRAN, for Mac and Windows, you don’t have to bother with installing all those external library dependencies. If you are using Linux, or you try to install ‘terra’ or ‘sf’ from GitHub, then dealing with all those dependencies becomes a much bigger headache. install.packages( c( &quot;terra&quot;, &quot;sf&quot;, &quot;raster&quot;, # this has been largely replaced by terra, but we still use # it in some code. &quot;rasterVis&quot;, &quot;gstat&quot;, &quot;rgdal&quot;, # this may eventually be phased out... &quot;ggspatial&quot;, # for plotting RGB rasters in ggplot, efficiently, among other things &quot;geodata&quot;, # for easy fetching of environmental layers &quot;rnaturalearth&quot;, # for easy fetching of Natural Earth data &quot;rnaturalearthdata&quot; ), repos = &quot;https://cran.rstudio.com&quot; ) 0.1.2.4 The ‘gradientForest’ package Now, we come to a somewhat harder package to install, because it requires some compilation. We use these packages for the gradient random forest analysis on the last day. ‘gradientForest’ requires the dependancy ‘extendedForest,’ an R package for classification and regression based on forest trees using random inputs. Both of these packages require compilation, it seems. With the newest version of R, ‘extendedForest’ is installed automatically as a dependency. To compile packages might require a little more configuration, etc. If you are using a Mac You should be running R 4.0.0 or above. In that case, to build packages, you need the XCode Command line tools. If you don’t already have these, then you should install them. Doing so requires administrator access on your computer. Open the “Terminal” app and type: xcode-select --install into the command line. When asked for your password, provide it. Then click to agree on the software license agreement, and finally click the blue “install” button that comes up on the other screen. Then, you also need to get the gFortran compiler for Mac. Information about this can be found by going to https://mac.r-project.org/tools/. Find the link (inside the yellow boxes) for your appropriate type of mac (intel or ARM64), and download the appropriate installer for the gFortran compiler. Then install it. In my case, since I have an Intel Mac, I downloaded, gfortran-8.2-Mojave.dmg. To install it, you have to double click it and then go inside the folder that creates, to find the gfortran.pkg, which you can double click to launch the installer. Install it to the default location. On my Mac running Mojave (10.14.6) I got errors when trying to compile this package. It was unable to find the C ‘stdlib.h’ header file. So, I ended up also having to do this: sudo installer -pkg /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg -target / If you have compilation problems on your Mac you might try something similar. But you might have to name the headers for your own system. perhaps by tab completing on: sudo installer -pkg /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10 If you have a Windows computer We haven’t tested it, but you should be just fine so long as you installed the Rtools as described above. Finally, if you have set up your system on Mac or Windows, install the package The package we need is not on CRAN. Rather, it is on the R-forge repository. In this case, we might be asked: Package which is only available in source form, and may need compilation of C/C++/Fortran: ‘extendedForest’ Do you want to attempt to install these from sources? (Yes/no/cancel) And, for this one, we need to respond, Yes install.packages(&quot;gradientForest&quot;, repos=&quot;https://R-Forge.R-project.org&quot;) If all goes well, this should compile for you. 0.1.3 Step 3. Make sure you have git and an account on GitHub In a two-day workshop, we don’t have time to go deeply, if much at all, into the many uses of the version control software, git, and the cloud-based code management system GitHub, that is built upon git. But, if you are interested in version control for your analyses, and you are interested in using GitHub to share and present the results of your research, then you really will want to become proficient with both git and GitHub. Fortunately, there is an outstanding, free book on the web that goes into great detail about how to use git and GitHub with R and RStudio. It is available at https://happygitwithr.com/, and it is well worth a read, and particularly following the steps in: Chapter 4. Register a GitHub account. Chapter 6. Install Git (Note: Mac users , if xcode-select --install ran successfully, then git will have been installed). Chapter 7. Introduce yourself to git If you want to use GitHub, you will also have to establish an SSH public/private key pair to authenticate your computer to GitHub. That is described in: Chapter 10: Set up keys for SSH "],["congen-intro.html", "Session 1 Introduction to Conservation Genomics Approaches 1.1 Breakout room activity material 1.2 After Kristen’s Talk and the activity please download the course repository", " Session 1 Introduction to Conservation Genomics Approaches This first presentation in the course is given by Dr. Kristen Ruegg. There is not an accompanying narrative at this webpage, but you can download a complete (24 Mb) PDF of her slides at: https://drive.google.com/uc?export=download&amp;id=1VoE26VT9_judMT0rH2V3jy7MDX3ZgYk7. 1.1 Breakout room activity material The breakout room activity slides (for the separate groups to fill out) are at: https://docs.google.com/presentation/d/1-upYVY70-IxaG7GH246lKe1-GFrgs8emK2IBkO6Qm2Q/edit?usp=sharing The informational slides from Kristen’s talk can be downloaded from here: https://drive.google.com/uc?export=download&amp;id=1cgFgBxHXaz8p0RaBtTX1oUMhBA61nRfW The relevant paper by Allendorf et al. is at: https://drive.google.com/uc?export=download&amp;id=1iQiW6Wm-9FIdz0LLPPyJkETSBiYkhUZx 1.2 After Kristen’s Talk and the activity please download the course repository To prepare for the interactive code sessions, it will be important to download the raw material for this course website as an RStudio Project. This will provide you with all of the data sets and other inputs needed for running all the examples. The easiest way to get this RStudio project is to download it as a .zip file from GitHub,1, using the following steps: In your web browser go to: https://github.com/eriqande/merida-workshop-2022. Click the green “Code” button. From the context menu that pops up when you click that green button, choose Download Zip That will download a compressed folder that is an RStudio project. You must uncompress it. On a Mac, this is done simply by double clicking it. On windows, make sure that all the files stay inside the directory that they come in, which is called merida-workshop-2022-main. Using Finder (on a mac) or File Explorer (on Windows) navigate inside that merida-workshop-2022 directory and double click the file merida-workshop-2022.Proj. The .Rproj extension might not show up on your computer, depending on your settings. At any rate, it is an RStudio Project, “proj” file. Double clicking it will launch RStudio and open the project. If you are comfortable with git, feel free to clone the repo, but for others not comfortable with using git, it will be easiest to simply download it using the directions given.↩︎ "],["basic-introduction-to-r-and-ggplot2.html", "Session 2 Basic Introduction to R and ggplot2 2.1 RStudio Layout 2.2 Data visualization with ggplot2 2.3 Conclusion 2.4 Appendix 1: Exercise Answers", " Session 2 Basic Introduction to R and ggplot2 This is an introduction to R and ggplot2 that was created specificially for the UCMexus Conservation Genomics Workshop 2022. The data used in this tutorial is a typical dataset used in ggplot. 2.1 RStudio Layout The RStudio layout is broken up into four panes: Source, Console, Environment (history/connections) and Files (plots/packages/etc). Within these panes there are additional tabs and we will go over the most important ones. The Source pane is where you can read, edit, and run your R code and the code can be saved as a .R file indicating it is an R script document, or .Rmd file indicating it is an R markdown file with the code split into code chunks that you’ll see throughout this tutorial. An R document is written as a normal text document, however each line is a line of code in R. However if you highlight a line and click Run (or type command-return), then the line of code will be executed and this activity will be displayed in the Console. The Console pane is where the R code is executed and results are displayed, as well as other messages and warnings. The Environment pane lists all available data and variables in the current session. For example, in the example Source pane above I have a line of code another_variable &lt;- 5 which sets the string another_variable to represent the value 5. After executing this line of code, the variable is stored and can be called at any point. Once a variable has been created, it will be displayed in the Environment. another_variable&lt;-5 another_variable ## [1] 5 another_variable*5 ## [1] 25 Note: Whenever opening a script into a new session of R, you will need to re-run the lines of code you wish to use. Opening a .R script does not automatically run the code. The Files pane is used to navigate through directories and can be used to open files directly into RStudio. This will be useful when importing data into R. Next to the Files tab, there is Plots. In the example Code Chunk pane below, I created a plot with the following code (note: the function ggplot() and the mpg dataset are from the ggplot2 package that comes in the tidyverse package. You need to have tidyverse installed and loaded for this code to work on your console. When the code is executed, the resulting scatterplot is displayed below. If this was run in a R script rather than an Rmarkdown, the plot would be displayed in the Plots window on the left. library(tidyverse) ## ── Attaching packages ────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.6 ✓ dplyr 1.0.7 ## ✓ tidyr 1.1.4 ✓ stringr 1.4.0 ## ✓ readr 2.1.1 ✓ forcats 0.5.1 ## ── Conflicts ───────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() p &lt;- ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy)) p Also useful is the Help tab for understanding what a function does. 2.2 Data visualization with ggplot2 We want to understand our data and our analysis. We want to produce graphics for publication and presentation. We want to create plots quickly and get many things “for free.” We want many options to customize and make things look awesome. 2.2.1 Understanding the Ggplot Syntax ggplot2 uses the basic units of the “grammar of graphics” (Leland Wilkinson) to construct data visualizations in a layered approach. The basic units in the “grammar of graphics” consist of: The data or the actual information that is to be visualized. The geometries, shortened to “geoms,” which describe the shapes that represent the data. These shapes can be dots on a scatter plot, bar charts on the graph, or a line to plot the data. Data are mapped to geoms. The aesthetics, or the visual attributes of the plot, including the scales on the axes, the color, the fill, and other attributes concerning appearance The syntax for constructing ggplots could be puzzling if you are a beginner or work primarily with base graphics. The main difference is that, unlike base graphics, ggplot works with dataframes and not individual vectors. All the data needed to make the plot is typically be contained within the dataframe supplied to the ggplot() itself or can be supplied to respective geoms. More on that later. 2.2.2 A plot consists of components The second noticeable feature is that you can keep enhancing the plot by adding more layers (and themes) to an existing plot created using the ggplot() function Here is a verbose definition of a simple plot. Note, this is not something you will see/use. But it’s a nice way to break down the components Ok, let’s dive into this very long code to create a simple plot. Note there are 5 components in this code chunk. ggplot() + layer(data = mtcars, #1 mapping = aes(x = hp, y = qsec), #2 geom = &#39;point&#39;, #3 stat = &#39;identity&#39;, #4 position = &#39;identity&#39;) + #5 theme_bw()+ theme(aspect.ratio = 1) 2.2.2.1 1: Data layer(data = mtcars, #1 A data layer needs data to display. Data is a data.frame or tibble if you are using tidyverse Let’s look at the top of a data set used by tidyverse head(mtcars) ## mpg cyl disp hp drat wt qsec ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 ## vs am gear carb ## Mazda RX4 0 1 4 4 ## Mazda RX4 Wag 0 1 4 4 ## Datsun 710 1 1 4 1 ## Hornet 4 Drive 1 0 3 1 ## Hornet Sportabout 0 0 3 2 ## Valiant 1 0 3 1 2.2.2.2 2: What is a mapping? layer(data = mtcars, #1 mapping = aes(x = hp, y = qsec), #2 An aesthetic mapping connects your data to your plot. (e.g. x, y, color, shape, alpha, size, linetype, etc.). Aesthetics are “things that we can perceive on the graphic.” We use the aes() function to create this mapping. aes(aesthetic = variable). For the code chunk below, we would be mapping hp on the x-axis and qsec on the y-axis. aes(x = hp, y = qsec) 2.2.2.3 3: What is a geom? layer(data = mtcars, #1 mapping = aes(x = hp, y = qsec), #2 geom = ‘point,’ #3 Simply the geometrical shape that you want to draw. 0d: point, text, 1d: path, line (ordered path), 2d: polygon, interval, boxplot etc. geoms translate the aesthetics into an actual plot. Geoms determine what aesthetics you need, and which you can use! (e.g. points cannot display the linetype aesthetic.) Which aesthetics you need/can use is usually obvious, but is also clearly documented. 2.2.2.4 Simplifying the code Instead of writing out the whole layer(), we can use geom_*() functions to create the layer for us. These functions use common defaults. We simply write: ggplot() + geom_point(mapping = aes(x = hp, y = qsec), data = mtcars) + theme_bw() + theme(aspect.ratio = 1) Or, using positional matching: ggplot() + geom_point(aes(hp, qsec), mtcars) + theme_bw() + theme(aspect.ratio = 1) Like geom_point(), there are many such geom layers which we will see in a subsequent part in this tutorial series. Let’s add details to this plot in the form of additional aesthetics. How would you change color of the points to the cyl variable? ggplot() + geom_point(aes(hp, qsec, color = cyl), mtcars) + theme_bw() + theme(aspect.ratio = 1) Continuous versus discrete variables R treats continuous and discrete information differently. ggplot directly uses the class of the variable to figure out how to best display data. In this case, it checks: class(mtcars$cyl) ## [1] &quot;numeric&quot; For it to recognized as a category (with fitting colors) we need a factor: class(factor(mtcars$cyl)) ## [1] &quot;factor&quot; Character strings are silently converted to factors: ggplot() + geom_point(aes(hp, qsec, color = as.character(cyl)), mtcars) + labs(color = &#39;cyl&#39;) + theme_bw() + theme(aspect.ratio = 1) 2.2.2.5 A special stat: stat_smooth We can keep adding additional layers to this plot to make it more informative. One thing that is often useful when you’re looking at data on a scatter plot is a visual representation of how closely the variable on the x axis and the variable on the y axis are related to each other, and the strength of that relationship. We can summarize this using a fitted line. In ggplot, we can automatically add a fitted line using the layer geom_smooth(). By default, geom_smooth() will show the relationship between the x and y variables using locally weighted scatter plot smoothing (also known as LOESS) rather than by drawing a straight line. library(tidyverse) library(ggpubr) ggplot(mtcars,aes(hp, qsec)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; If we want a single straight line to summarize our data, we need to specify that we want geom_smooth() to fit a linear model (or “lm”). ggplot(mtcars,aes(hp, qsec)) + geom_point() + geom_smooth(method=&quot;lm&quot;,se=F, formula = y~x) Now we want to add the actual model equation and r2. ggplot(mtcars,aes(hp, qsec)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = TRUE) + stat_regline_equation(label.y = 27, aes(label = ..eq.label..)) + stat_regline_equation(label.y = 26, aes(label = ..rr.label..)) + theme_bw() + theme(aspect.ratio = 1) ## `geom_smooth()` using formula &#39;y ~ x&#39; For the following exercises, you can use the ggplot cheatsheet. These are amazing resources to have on hand for creating new plots and adding new layers. Page 1: Page 2: Exercise 1: Create the following plot Now, how would you change the code above to change the color of the point layer, withuot affecting the smooth layer since the aes() aesthetic mapping is passed at the point layer. ## `geom_smooth()` using formula &#39;y ~ x&#39; Hint: the code would only change the color of the point layer, it would not affect the color of the smooth layer since the aes() aesthetic mapping is passed at the point layer. Exercise 2: Create the following plot Can you recreate this plot using other aesthetic options? Exercise 3: Create the following plot How about if we wanted to create a line plot with this data? How would you generate the following plot with separate lines representing the “am” variable. 2.2.2.6 Shared data and mapping Now what if we want to add labels. Let’s look at another dataset and think about this- now we have multiple layers. You might’ve noticed this with the geom_smooth function too. When multiple layers share the same data or mapping you can define it inside of ggplot. Everything in ggplot will be used by default for all layers. Use data inside a layer in order to not use the default. Set or map an aesthetic to not use the default. You can map to NULL. This goes for each aesthetic individually! Not for the entire mapping. In many cases I provide the data and mapping in ggplot. g &lt;- ggplot(msleep, aes(sleep_total, log(brainwt), label = name, #make sure to add the label aes! col=vore)) g + geom_point() + # then add a layer of points geom_text(size=3) # and then add labels to the points ## Warning: Removed 27 rows containing missing values ## (geom_point). ## Warning: Removed 27 rows containing missing values ## (geom_text). But this plot is a bit busy and messy. Let’s move around the names to make this more readable (less overlap)! Let’s also fix the axes names. g + #the ggplot object you created above that you can add layers to geom_point() + # then add a layer of points geom_text(check_overlap = T,# automatically reduce overlap (deletes some labels) vjust = &quot;bottom&quot;, # adjust the vertical orientation nudge_y = 0.1, # move the text up a bit so it doesn&#39;t touch the points angle = 30,# tilt the text 30 degrees size = 2) + # make the text smaller (to reduce overlap more) labs(title = &quot;Sleeping habits of mammals given their brain weight&quot;, # plot title x = &quot;Sleep Total&quot;, # x axis label y = &quot;log(brain weight)&quot; # y axis label ) ## Warning: Removed 27 rows containing missing values ## (geom_point). ## Warning: Removed 27 rows containing missing values ## (geom_text). Note above, there are some points without labels. Here’s another way to add labels to your plot with ggrepel. library(ggrepel) g + geom_point() + # then add a layer of points geom_text_repel(vjust = &quot;bottom&quot;, # adjust the vertical orientation nudge_y = 0.1, # move the text up a bit so it doesn&#39;t touch the points angle = 30,# tilt the text 30 degrees size = 2) + # make the text smaller (to reduce overlap more) labs(title = &quot;Sleeping habits of mammals given their brain weight&quot;, # plot title x = &quot;Sleep Total&quot;, # x axis label y = &quot;log(brain weight)&quot; # y axis label ) ## Warning: Removed 27 rows containing missing values ## (geom_point). ## Warning: Removed 27 rows containing missing values ## (geom_text_repel). Let’s move onto the other components of the ggplot syntax. 2.2.2.7 4. What is a stat? layer(data = mtcars, #1 mapping = aes(x = hp, y = qsec), #2 geom = ‘point,’ #3 stat = ‘identity,’ #4 Often, you don’t want to directly plot the actual data, but rather some transformation of the data. stats perform all kinds of data transformations. The most common transformations: binning (StatBin). counting (StatCount). summing (StatSum). averaging (StatSummary). smoothing (StatSmooth). boxplots (StatBoxplot). densities (StatDensity). nothing (StatIdentity). Note geom_* functions have default stats. E.g. ?geom_boxplot shows stat = “boxplot.” ggplot() + geom_boxplot(aes(factor(cyl), hp, fill=factor(cyl)), mtcars) + xlab(&#39;Cylinder&#39;) + theme_bw() + theme(aspect.ratio = 1) Exercise 4a-c: One variable exercises How would you create the following plots? I want you to use a new eBird dataset, however, the syntax still applies. Additionaly, there are multiple ways to read in data. myebird&lt;-read_csv(&quot;data/2.0a/MyEBirdData_tidy.csv&quot;) ## Rows: 523 Columns: 8 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (6): eBirdID, CommonName, Region, Location, Co... ## dbl (1): Count ## time (1): Time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. myebird&lt;-read_delim(&quot;data/2.0a/MyEBirdData_tidy.csv&quot;,delim=&quot;,&quot;) ## Rows: 523 Columns: 8 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (6): eBirdID, CommonName, Region, Location, Co... ## dbl (1): Count ## time (1): Time ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. If you have an excel sheet, you can load a new library and load it using this generic code library(gdata) file &lt;- read.xls(&quot;Excel.file.xlsx&quot;,sheet=2) Let’s take a quick look at this new data set. It has counts of several bird species in Colorado, and the locations and dates of these observations. The head function will allow you to look at the first 6 lines of the data. head(myebird) # ## # A tibble: 6 × 8 ## eBirdID CommonName Count Region Location County Date ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 S63589… Cackling … 3 US-CO South P… Adams 1/19… ## 2 S66111… Cackling … 14 US-CO Colorad… Larim… 3/22… ## 3 S65370… Cackling … 5 US-CO Grandvi… Larim… 3/2/… ## 4 S66112… Cackling … 11 US-CO Prospec… Larim… 3/22… ## 5 S63589… Canada Go… 24 US-CO South P… Adams 1/19… ## 6 S66719… Canada Go… 7 US-CO Arapaho… Larim… 4/6/… ## # … with 1 more variable: Time &lt;time&gt; If you want to know the full dataset dimensions, you can use the dim function. dim(myebird) ## [1] 523 8 In order to create the next plots, we may need to tidy some of the data. While, the module concurrently being run by Richard goes into how to use tidyverse in more depth, this section will provide a small hint of what tidying data can mean and how we can pipe (%&gt;%) this directly into the ggplot() command. With a smaller version of the dataset, we are going to create histograms and density plots with some eBird data specific from Colorado. Histograms are a way to show variation in data and are plotted on a single numeric value. In this case we will just look at the Count column of the data frame. This is plotted on the x-axis and to avoid confusion I change the name of the x-axis since the y-axis is the counts of each value. To make things more interpretable for you as new R users, I am now breaking up the data tidying you can do in Richard’s tidyverse section and assigning that to a variable, then using that tidied data frame variable in the ggplot() function for plotting (again we could pipe into ggplot but that might not be so clear at this point). Remember from our previous examples that ggplot() uses “+” not the pipe function to add more commands/functions. myebird2 &lt;- myebird %&gt;% filter(CommonName %in% c(&quot;Canada Goose&quot;)) myebird3 &lt;- myebird %&gt;% filter(CommonName %in% c(&quot;Canada Goose&quot;,&quot;American Robin&quot;)) First plot. A simple histogram of counts for the Canada Goose, and we use the stats binning. ## Warning: Removed 1 rows containing non-finite values ## (stat_bin). Second plot. How would you change the color of the histogram? ## Warning: Removed 1 rows containing non-finite values ## (stat_bin). Third plot is a stat = density plot, for counts of two bird species. ## Warning: Removed 1 rows containing non-finite values ## (stat_density). 2.2.2.8 5. What is a position? layer(data = mtcars, #1 mapping = aes(x = hp, y = qsec), #2 geom = ‘point,’ #3 stat = ‘identity,’ #4 position = ‘identity’) + #5 Things in a plot can overlap. This is very common with categorical axes and overlap can be resolved by using a position Most commonly you: stack (put on top of each other). fill (stack in proportion, up to 1). dodge (put next to each other). jitter (add random noise). leave as is: identity. For exmaple, the default of geom_bar is stacked: geom_bar(position = “stack”) ggplot(mtcars, aes(factor(cyl), fill = factor(am))) + geom_bar(position = &#39;fill&#39;) To dodge the barplots: ggplot(mtcars, aes(factor(cyl), fill = factor(am))) + geom_bar(position = &#39;dodge&#39;) Now, we are going to include facets. ggplot(mtcars, aes(factor(cyl), fill = factor(am))) + geom_bar(position = &#39;dodge&#39;) + # add the barpot facet_wrap(c(&quot;am&quot;)) # put each level of &quot;am&quot; in a different facet That seems much easier to read! One thing that I would like to change, however, is the colors. Let’s change the color theme of our plot. You can change the color of the fill using the layer scale_fill_manual() ??geom_bar ggplot() + geom_bar(aes(factor(cyl),fill = factor(am)),data=mtcars, position = &#39;dodge&#39;) + # add the barpot scale_fill_manual(values=c(&quot;#BBBBBB&quot;, &quot;#E69F00&quot;)) + facet_wrap(c(&quot;am&quot;)) + theme_bw() We can redefine colors based on the hexidecimal color system again. A useful link is here: http://www.sthda.com/english/wiki/colors-in-r But we can create a reusable palette by adding the colors to a variable. cbpalette&lt;-c(&quot;#00CC66&quot;, &quot;#3333CC&quot;) library(RColorBrewer) mtcars %&gt;% ggplot() + geom_bar(aes(factor(cyl),fill = factor(am)),position = &#39;dodge&#39;) +# add the barpot facet_wrap(c(&quot;am&quot;)) + scale_fill_manual(values=cbpalette) Or you can use the Rcolorbrewer palettes: https://www.r-graph-gallery.com/38-rcolorbrewers-palettes.html library(RColorBrewer) library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor GH.cols &lt;- brewer.pal(n = 9, name = &quot;Purples&quot;)[c(4,8)] show_col(GH.cols) mtcars %&gt;% ggplot() + geom_bar(aes(factor(cyl),fill = factor(am)),position = &#39;dodge&#39;) + facet_wrap(c(&quot;am&quot;)) + scale_fill_manual(values=GH.cols) Let’s add some labels here, where we have to specify position again. ggplot(mtcars, aes(factor(cyl),fill = factor(am))) + geom_bar() + geom_label(aes(label = ..count..), stat = &#39;count&#39;, position = &#39;stack&#39;) + scale_fill_manual(values=GH.cols) Instead of defining the position as a string (e.g. position = “dodge”), you can write it as a full function with arguments (e.g. position = position_dodge(width = 0.5)) or tell ggplot where within the boxplot you’d like to posiiton the counts specifically (e.g position = position_stack(vjust=0.5)). ggplot(mtcars, aes(factor(cyl), fill = factor(am))) + geom_bar() + geom_text(aes(label = ..count..), stat = &#39;count&#39;, position = position_stack(vjust = 0.5)) + xlab(&quot;Cylinder&quot;) + ylab(&quot;Count&quot;) + scale_fill_manual(values=GH.cols) As you can see, the general idea of behind ggplot (starting with just a blank plot and slowly adding more and more layers) also lends itself to a general workflow where we start with a very simple plot and add layers until it looks exactly the way we want. 2.3 Conclusion And that wraps it up for this beginner ggplot2 tutorial. At this point, particularly if you’ve been completing the exercises in the workbook, you should: Understand the underlying philosophy behind ggplot. Be able to make a scatter plot and. Add a fitted line. Add text labels to data points. Title and label your plot axes. Be able to make a bar plot and. Choose whether your bar plot is stacked or grouped. Split a plot across multiple subplots. Create a custom color palette for your plot. Be able to make a line plot. 2.4 Appendix 1: Exercise Answers Exercise 1 g&lt;-ggplot(mtcars,aes(hp, qsec,color = as.character(cyl)) ) + geom_point() + geom_smooth(method = lm, se = TRUE, color=&quot;black&quot;,lty=3) + theme_bw() + theme(aspect.ratio = 1) g ## `geom_smooth()` using formula &#39;y ~ x&#39; Notice how I changed the line type in the code chunk above. Exercise 2 ggplot() + geom_point(aes(hp, qsec, shape = factor(vs), size = mpg), mtcars, alpha = 0.5, col = &#39;red&#39;) + labs(shape = &#39;vs&#39;, size = &#39;qsec&#39;) + theme_bw() + theme(aspect.ratio = 1) Exercise 3 ggplot() + geom_line(aes(hp, qsec,lty = factor(am)), mtcars) + theme_bw() + theme(aspect.ratio = 1) Exercise 4a-c ggplot(myebird2) + geom_histogram(aes(x = Count), bins = 100) + xlab(&quot;Number of individuals seen per observation&quot;) + ggtitle(&quot;Histogram of Canadian Goose observations&quot;) ## Warning: Removed 1 rows containing non-finite values ## (stat_bin). #or myebird2 %&gt;% ggplot() + geom_histogram(aes(x = Count), bins = 100) + xlab(&quot;Number of individuals seen per observation&quot;) + ggtitle(&quot;Histogram of Canadian Goose observations&quot;) ## Warning: Removed 1 rows containing non-finite values ## (stat_bin). ## Warning: Removed 1 rows containing non-finite values ## (stat_bin). Third plot is a stat = density plot, for counts of two bird species. ## Warning: Removed 1 rows containing non-finite values ## (stat_density). "],["introduction-to-the-tidyverse.html", "Session 3 Introduction to the tidyverse 3.1 What on earth is the tidyerse? 3.2 An example data set 3.3 Reading data in the tidyverse 3.4 A word on tibbles 3.5 Back to “tidying” up our genotype data 3.6 What the heck is that %&gt;% thingie?! 3.7 The ‘dplyr’ package: the heart of the tidyverse 3.8 Modeling within the Tidyverse", " Session 3 Introduction to the tidyverse 3.1 What on earth is the tidyerse? Broadly and briefly, the tidyverse is a collection of packages that are tailored to operating on data that are stored in what is called a “tidy” format. Such a format is one that is typical of the manner in which data are stored in relational data bases. (We will be talking more about “tidy” format throughout this session). There is an R package called ‘tidyverse’ that is simply a wrapper that automatically loads the 8 core packages of the tidyverse. In this session we will talk specifically about functions in 5 of those packages, encountering them in this order: ‘readr’ ‘tibble’ ‘tidyr’ ‘dplyr’ ‘purrr’ The other core packages in the tidyverse are: ‘ggplot2’ — the well known graphics plotting package ‘stringr’ — a package for manipulating string data ‘forcats’ — a package to simplify the use of factors These packages provide a unified way of handling data in a tidy format. And, it turns out, almost any data type can be stored and expressed in a tidy format. This means that 99% of your data analysis tasks can be tackled in a tidy fashion. Some “big-data” data sets (like genome sequencing alignments, or doppler radar data, etc.) come in specialized data formats that allow for fast access and compression, and would not by amenable to storage in a tidy format. However, these are specialized cases, and for almost every data analysis application you will encounter, it behooves you to get comfortable in the tidyverse. At any rate, to have access to all the functions from the 8 core packages of the ‘tidyverse’ we just load the package like this: library(tidyverse) 3.2 An example data set We will start our exploration of the tidyverse with a data set that is decidedly not tidy. It is a small data set of genotypes of 34 fish from four different rivers with code names BALGA, BCHNC, BEMME, and BMTMA. The fish have been genotyped at 5 genetic markers, named mk1, mk2, mk3, mk4, and mk5. Because these fish are diploids they have two copies of each gene, and therefore, their genotype at a single markers consists of the allele or allelic type of the two gene copies at that marker. So, in this data set, for example, the mk1_1 column holds information about the first gene copy at marker mk1 and, mk1_2 holds information about the second gene copy at marker mk1. Different alleles are specified as different positive integer values, (like 228) The data set is in the merida-workshop-2022 Rstudio project, stored as a CSV file at data/tidy-intro/genotypes.csv. It looks like this: river,indiv,mk1_1,mk1_2,mk2_1,mk2_2,mk3_1,mk3_2,mk4_1,mk4_2,mk5_1,mk5_2 BALGA,BALGA_001,311,311,228,228,234,234,184,164,211,211 BALGA,BALGA_002,311,225,228,228,230,226,184,184,NA,NA BALGA,BALGA_006,311,311,228,228,234,234,184,168,219,215 BALGA,BALGA_007,311,311,228,220,230,218,184,164,215,215 BALGA,BALGA_008,311,311,228,228,234,230,168,164,NA,NA BALGA,BALGA_009,315,315,220,220,238,230,184,184,231,207 BALGA,BALGA_012,311,311,228,220,230,230,184,164,227,211 BCHNC,BCHNC_001,311,311,228,228,234,230,184,184,227,211 BCHNC,BCHNC_002,311,259,228,228,238,226,184,184,227,215 BCHNC,BCHNC_003,311,311,228,212,238,226,NA,NA,227,227 BCHNC,BCHNC_004,NA,NA,NA,NA,NA,NA,NA,NA,207,207 BCHNC,BCHNC_005,311,299,232,228,234,234,184,184,219,203 BCHNC,BCHNC_006,NA,NA,228,228,NA,NA,168,168,215,215 BCHNC,BCHNC_007,315,311,228,228,234,234,168,168,215,211 BCHNC,BCHNC_008,NA,NA,228,228,NA,NA,168,168,231,219 BCHNC,BCHNC_009,349,311,236,228,234,214,184,168,219,219 BCHNC,BCHNC_010,311,311,232,228,238,230,184,168,219,215 BEMME,BEMME_001,311,311,228,228,238,226,184,184,NA,NA BEMME,BEMME_002,349,299,232,220,246,234,184,168,211,203 BEMME,BEMME_003,311,311,236,228,238,234,184,168,NA,NA BEMME,BEMME_006,311,311,236,228,238,234,184,168,NA,NA BEMME,BEMME_008,349,349,228,228,234,230,184,184,215,211 BEMME,BEMME_009,311,311,228,220,238,238,184,184,235,203 BEMME,BEMME_010,311,311,228,228,242,234,168,168,223,203 BEMME,BEMME_011,307,307,228,228,238,234,184,168,231,215 BMTMA,BMTMA_001,NA,NA,228,220,234,234,196,168,223,211 BMTMA,BMTMA_003,311,311,228,228,234,230,208,196,223,219 BMTMA,BMTMA_005,311,253,228,228,242,230,NA,NA,211,203 BMTMA,BMTMA_006,311,311,228,228,234,226,196,184,223,211 BMTMA,BMTMA_007,NA,NA,228,220,234,230,196,184,223,223 BMTMA,BMTMA_008,311,311,228,228,242,238,196,188,223,223 BMTMA,BMTMA_009,NA,NA,228,228,234,230,200,184,223,219 BMTMA,BMTMA_010,311,311,232,232,230,230,196,184,223,211 BMTMA,BMTMA_011,NA,NA,228,228,NA,NA,184,168,223,203 3.3 Reading data in the tidyverse The package ‘readr’ has a large number of functions for reading in data of different types. They all start with read_. These functions typically read things into tidyverse-specific data frames that are called “tibbles,” about which we will hear more later. We can read the genotypes.csv file using the readr function, read_csv(). Note: this looks like the base R function read.csv(), but it is not the same!! Most notably, read_csv() will never automatically convert strings to factors (Rejoice!) read_csv() is faster than read.csv() read_csv() is more likely to tell you if there are unexpected irregularities found while reading your data set. It also has facilities to tell you exactly where those data-reading problems occurred. read_csv() will return data in a tibble rather than simply a data frame. (More about this soon!). These characteristics are shared amongst all the read_*() functions (such as read_tsv(), read_table(), read_table2() and read_delim()) in the ‘readr’ package. A delightful overview of the readr package can be found on the “readr Cheatsheet” from RStudio available here or by clicking the thumbnail below. Reading the data set with read_csv() is simple: genos &lt;- read_csv(&quot;data/tidy-intro/genotypes.csv&quot;) And then we can display it like this: genos ## # A tibble: 34 × 12 ## river indiv mk1_1 mk1_2 mk2_1 mk2_2 mk3_1 mk3_2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 311 311 228 228 234 234 ## 2 BALGA BALGA_002 311 225 228 228 230 226 ## 3 BALGA BALGA_006 311 311 228 228 234 234 ## 4 BALGA BALGA_007 311 311 228 220 230 218 ## 5 BALGA BALGA_008 311 311 228 228 234 230 ## 6 BALGA BALGA_009 315 315 220 220 238 230 ## 7 BALGA BALGA_012 311 311 228 220 230 230 ## 8 BCHNC BCHNC_001 311 311 228 228 234 230 ## 9 BCHNC BCHNC_002 311 259 228 228 238 226 ## 10 BCHNC BCHNC_003 311 311 228 212 238 226 ## # … with 24 more rows, and 4 more variables: ## # mk4_1 &lt;dbl&gt;, mk4_2 &lt;dbl&gt;, mk5_1 &lt;dbl&gt;, mk5_2 &lt;dbl&gt; Each row coresponds to one individual, and each column corresponds to one of the two gene copies at a genetic marker. 3.4 A word on tibbles It is worth noting how it is that this tibble, this special type of a data frame is displayed when you print it: The first line, # A tibble: 34 × 12, tells how many rows (34) and columns (12) are in the tibble. By default, only the first ten rows are printed. Beneath the name of each column, the type of data in the column is listed. In this case, &lt;dbl&gt; means numeric data and &lt;chr&gt; means the column is a vector of character (string) data (i.e. text). Other possibilities are &lt;int&gt; for integer, &lt;fct&gt; for factor, and &lt;list&gt; for a column that is a list (Richard will talk more about that!) If there are too many columns for the width of the screen, only the contents of the first few columns are printed. The remaining columns are just shown beneath by their names and types. If there a a very large number of columns, the names and types of some of the remaining ones are not shown. In this way, you can quickly get a sense for how a data set is structured by simply printing it to the screen without running the risk of bombing your whole R console by having it print all the lines of a very large data frame! While the readr package always returns a tibble, the as_tibble() function from the ‘tibble’ package provides an easy way to turn an existing data frame into a tibble. For example, to witness how much easier it is to see what is contained in the built-in R data set, iris, by converting it into a tibble, the reader is invited to evaluate the following two lines in their R console and compare the output: iris # the whole data set gets spit out and you end up at bottom of it as_tibble(iris) # a nice tibble summary gets printed Finally, the ‘tibble’ package provides the function tibble() that allows the user to create a tibble from vectors. It is much like the built-in R function, data.frame() except that it deals appropriately with list vectors. For more about tibbles, the best place if you are new is the tibble chapter, in the R For Data Science book. 3.5 Back to “tidying” up our genotype data Our small data set genos might seem pretty neat and tidy—it is easy to look at as a human, and it would not take a whole lot of space to print it out on a page. However, in the current format, doing a number of summaries of the data would require somewhat specialized code. For example, if you wanted to count up the number of occurrences of each allelic type (i.e. each number, like 311) at each of the markers, within each of the rivers, it would not be straightforward! Just imagine counting alleles at markers mk_1: the alleles are in two different columns, and you would have to account for fish being from different rivers. While one could certainly write base R code to traverse the data frame and pick out the appropriate columns and rows to make these summaries, the tidyverse approach is to recognize that converting the data to a different format (often called a “long” format), will make it easier to do most any operation on the data. The main problem of the current data set, genos, is that the observed values at all the markers are the same—they are all just different alleles; however, they occupy lots of different columns. A general principle in tidying data is to strive for having only a single column to represent a certain type of observation. In the case of genos, the column headers mk1_1, mk1_2, ... , mk_5_2 tell us which particular markers and gene copies have a particular allelic type. But another way to represent that is to have a column that gives the marker and the gene copy. Perhaps it is best to simply show what a tidy, long version of these data look like. The core tidyverse package ‘tidyr’ provides many functions for tidying data sets. We will use the pivot_longer() function to turn the column names into data values in new columns: genos_long &lt;- genos %&gt;% pivot_longer( cols = c(mk1_1:mk5_2), names_to = c(&quot;marker&quot;, &quot;gene_copy&quot;), names_sep = &quot;_&quot;, values_to = &quot;allele&quot; ) genos_long ## # A tibble: 340 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk1 1 311 ## 2 BALGA BALGA_001 mk1 2 311 ## 3 BALGA BALGA_001 mk2 1 228 ## 4 BALGA BALGA_001 mk2 2 228 ## 5 BALGA BALGA_001 mk3 1 234 ## 6 BALGA BALGA_001 mk3 2 234 ## 7 BALGA BALGA_001 mk4 1 184 ## 8 BALGA BALGA_001 mk4 2 164 ## 9 BALGA BALGA_001 mk5 1 211 ## 10 BALGA BALGA_001 mk5 2 211 ## # … with 330 more rows Wow, that is a wildly different format! We now only have 5 columns: river: tells us what river the fish is from indiv: the ID of the individual fish marker: the name of the marker (i.e. mk_1, or mk_2, etc.) gene_copy: whether we are looking at the first (1) or the second (2) gene copy of the fish. allele: the allelic type of the gene_copy at the marker in the individual fish from the particular river. Though our data set has only 5 columns, it now has 340 rows. As a consequence, it is not really possible to “look at” a large portion of the data set as a human (as was possible before). Furthermore, if you don’t know how to handle data like these it can seem daunting. But, once you learn the tools of the tidyverse (and particularly of the package ‘dplyr’) this format lets you learn the same, simple approach that will work for many, many different data sets. The ‘tidyr’ package has a number of functions for tidying data. Check out the RStudio tidyr cheatsheet for a great overview of ‘tidyr’ features. 3.6 What the heck is that %&gt;% thingie?! Before we proceed, we want to make sure that everyone understands what the %&gt;% in the above code block is doing. The packages of the tidyverse all import the R symbol %&gt;% from the ‘magrittr’ package. This is referred to as the tidyverse “pipe” because it allows the user to “pipe” the output of one function into the input for another function (in the same way that experience Unix computer users will “pipe” output from one command into the input for another command). In short, the %&gt;% symbol takes whatever is on the left of it and it supplies that as the first argument of the function that is to the right of it. The thing on the left can be an R object or the output of a functio. For a simple, contrived example, suppose you wanted to add the integers from 1 to 10, take the square root of the result, and then calculate the sin of that result, and then take the log of it. Without piping capability, you could write that operation in a horribly nested set of parentheses: log(sin(sqrt(sum(1:10)))) ## [1] -0.09905 which is hard to read because the direction of operations runs right to left. It is much easier to read and understand what is going on by piping results from one function to the next: 1:10 %&gt;% sum() %&gt;% sqrt() %&gt;% sin() %&gt;% log() ## [1] -0.09905 Operating upon tibbles with the pipe let’s you chain multiple different functions/operations upon the data in an easy-to-read fashion. And, it let’s you check intermediate results along the way as you build up more complex expressions. It is a crucial capability in the tidyverse. We will see the tidyverse pipe used extensively in the following. 3.7 The ‘dplyr’ package: the heart of the tidyverse ‘dplyr’ is the tidyverse package that offers the most functionality for operating on data sets. The main ‘dplyr’ functions for performing actions on a tibble are described as “verbs”: and their names are meant to describe the actions they do. There are a lot of other functions within ‘dplyr.’ We can’t cover them all here, but, once again, the RStudio dplyr cheatsheet is the perfect place for an overview. Here, we will cover just the main verbs, namely: select(), filter(), arrange() change the extent of columns, or rows, or the sort order of rows, respectively. mutate() allows for the creation of new columns that are functions of existing columns (and have the same length as the existing columns). summarise() allows for summarising existing columns down to a single value. group_by() allows the ‘dplyr’ verbs to operate on multiple subsets of data defined by the values of grouping variables. I dare say this is the most important concept in all the tidyverse. left_join() the simplest of a whole family of joins (left_join(), full_join(), inner_join(), etc.), that combine rows of two tibbles together according to shared values in certain columns. 3.7.1 Non-standard Evaluation (NSE): You don’t wrap column names in quotation marks Importantly, in the tidyverse, when you operate on a column of a tibble, it is customary to refer to that column by its name, rather than by the number that describes its position (this makes the code more resilient to things like ordering of columns in a tibble). Within the tidyverse, names of columns are given as if they were variable names—they should almost never be wrapped in quotation marks. 3.7.2 select() to select columns of a tibble The select function is used to choose which columns of a tibble are returned. They get returned in a tibble of their own. Examples: # return a tibble with just the river and indiv of # the original genos tibble genos %&gt;% select(river, indiv) ## # A tibble: 34 × 2 ## river indiv ## &lt;chr&gt; &lt;chr&gt; ## 1 BALGA BALGA_001 ## 2 BALGA BALGA_002 ## 3 BALGA BALGA_006 ## 4 BALGA BALGA_007 ## 5 BALGA BALGA_008 ## 6 BALGA BALGA_009 ## 7 BALGA BALGA_012 ## 8 BCHNC BCHNC_001 ## 9 BCHNC BCHNC_002 ## 10 BCHNC BCHNC_003 ## # … with 24 more rows Ranges of columns can be selected by putting a colon between the name of the column at the start of the range and the name of the column at the end of the range. Furthermore, adding a - to a column name in select() will remove it. And adding a - before a column name range (surrounded by parentheses) will remove all those columns. For example: # remove the columns mk1_1 to mk3_2 from the original # genos tibble genos %&gt;% select(-(mk1_1:mk3_2)) ## # A tibble: 34 × 6 ## river indiv mk4_1 mk4_2 mk5_1 mk5_2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 184 164 211 211 ## 2 BALGA BALGA_002 184 184 NA NA ## 3 BALGA BALGA_006 184 168 219 215 ## 4 BALGA BALGA_007 184 164 215 215 ## 5 BALGA BALGA_008 168 164 NA NA ## 6 BALGA BALGA_009 184 184 231 207 ## 7 BALGA BALGA_012 184 164 227 211 ## 8 BCHNC BCHNC_001 184 184 227 211 ## 9 BCHNC BCHNC_002 184 184 227 215 ## 10 BCHNC BCHNC_003 NA NA 227 227 ## # … with 24 more rows 3.7.3 filter() to retain certain rows The filter() function takes any vectorized logical expression involving columns of the tibble. Rows for which that expression evaluate to TRUE are retained, and those that evaluate to FALSE are not. Examples: # only keep rows with river code &quot;BALGA&quot; genos_long %&gt;% filter(river == &quot;BALGA&quot;) ## # A tibble: 70 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk1 1 311 ## 2 BALGA BALGA_001 mk1 2 311 ## 3 BALGA BALGA_001 mk2 1 228 ## 4 BALGA BALGA_001 mk2 2 228 ## 5 BALGA BALGA_001 mk3 1 234 ## 6 BALGA BALGA_001 mk3 2 234 ## 7 BALGA BALGA_001 mk4 1 184 ## 8 BALGA BALGA_001 mk4 2 164 ## 9 BALGA BALGA_001 mk5 1 211 ## 10 BALGA BALGA_001 mk5 2 211 ## # … with 60 more rows # only keep data from markers mk3 and mk4 genos_long %&gt;% filter(marker %in% c(&quot;mk3&quot;, &quot;mk4&quot;)) ## # A tibble: 136 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk3 1 234 ## 2 BALGA BALGA_001 mk3 2 234 ## 3 BALGA BALGA_001 mk4 1 184 ## 4 BALGA BALGA_001 mk4 2 164 ## 5 BALGA BALGA_002 mk3 1 230 ## 6 BALGA BALGA_002 mk3 2 226 ## 7 BALGA BALGA_002 mk4 1 184 ## 8 BALGA BALGA_002 mk4 2 184 ## 9 BALGA BALGA_006 mk3 1 234 ## 10 BALGA BALGA_006 mk3 2 234 ## # … with 126 more rows # remove rows for which allele is NA genos_long %&gt;% filter(!is.na(allele)) ## # A tibble: 300 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk1 1 311 ## 2 BALGA BALGA_001 mk1 2 311 ## 3 BALGA BALGA_001 mk2 1 228 ## 4 BALGA BALGA_001 mk2 2 228 ## 5 BALGA BALGA_001 mk3 1 234 ## 6 BALGA BALGA_001 mk3 2 234 ## 7 BALGA BALGA_001 mk4 1 184 ## 8 BALGA BALGA_001 mk4 2 164 ## 9 BALGA BALGA_001 mk5 1 211 ## 10 BALGA BALGA_001 mk5 2 211 ## # … with 290 more rows 3.7.4 arrange() to reorder the rows of the tibble For example, perhaps you wish to see which of the alleles have the highest numbers. Then: genos_long %&gt;% arrange(desc(allele)) ## # A tibble: 340 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BCHNC BCHNC_009 mk1 1 349 ## 2 BEMME BEMME_002 mk1 1 349 ## 3 BEMME BEMME_008 mk1 1 349 ## 4 BEMME BEMME_008 mk1 2 349 ## 5 BALGA BALGA_009 mk1 1 315 ## 6 BALGA BALGA_009 mk1 2 315 ## 7 BCHNC BCHNC_007 mk1 1 315 ## 8 BALGA BALGA_001 mk1 1 311 ## 9 BALGA BALGA_001 mk1 2 311 ## 10 BALGA BALGA_002 mk1 1 311 ## # … with 330 more rows The ‘dplyr’ function desc() means: arrange rows in descending order of the variable named within it. You can arrange/sort rows according to multiple columns, as well. For example to sort things that show individual genotypes in adjacent rows, but with rivers in reverse alphabetical order, we could do: genos_long %&gt;% arrange(desc(river), indiv, marker, gene_copy) ## # A tibble: 340 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BMTMA BMTMA_001 mk1 1 NA ## 2 BMTMA BMTMA_001 mk1 2 NA ## 3 BMTMA BMTMA_001 mk2 1 228 ## 4 BMTMA BMTMA_001 mk2 2 220 ## 5 BMTMA BMTMA_001 mk3 1 234 ## 6 BMTMA BMTMA_001 mk3 2 234 ## 7 BMTMA BMTMA_001 mk4 1 196 ## 8 BMTMA BMTMA_001 mk4 2 168 ## 9 BMTMA BMTMA_001 mk5 1 223 ## 10 BMTMA BMTMA_001 mk5 2 211 ## # … with 330 more rows 3.7.5 mutate() to create new columns With the mutate() function, it is easy to create a new column that is a function of other columns. The syntax is like: # create a new column named new_column that is a function # or existing columns col1, col2 genos_long %&gt;% mutate(new_column_name = some_function(col1, col2)) In this capacity, ‘dplyr’ is very particular: If the function returns the same number of rows as the tibble, it works. If the function returns a single value, that single value is replicated in each row of the new column. Otherwise, mutate() throws an error. Simple examples. Try these on your own: # make a column, &quot;long_marker_names&quot; that give # a long name in the form &quot;Marker X&quot; for each marker genos_long %&gt;% mutate(long_marker_names = str_replace(marker, &quot;mk&quot;, &quot;Marker &quot;)) # make a new column called gc_ab that gives the gene copies # as &quot;a&quot; and &quot;b&quot;, rather than &quot;1&quot; and &quot;2&quot; genos_long %&gt;% mutate( gc_ab = case_when( gene_copy == 1 ~ &quot;a&quot;, gene_copy == 2 ~ &quot;b&quot;, TRUE ~ NA_character_ ) ) # make a new column called season that tells us that # all of these data were collected in the &quot;Autumn&quot; genos_long %&gt;% mutate(season = &quot;Autumn&quot;) 3.7.6 Compute a single value from many rows: summarise() While mutate() always returns a new column that is the same length as all the other columns in the tibble, summarise() does something quite different: it allows you to make a column that will have only a single row in it. We will see more examples later on, but for now, consider one of the simplest ways to summarise a tibble: count how many rows are in it. That is done with the ‘dplyr’ function n(): genos_long %&gt;% summarise(num_rows = n()) ## # A tibble: 1 × 1 ## num_rows ## &lt;int&gt; ## 1 340 See how that summarise() function takes an entire tibble, and returns a single value (the number of rows of the original tibble). This behavior might not seem all that useful, but as we will see, when combined with grouping it is extraordinarily useful. 3.7.7 group_by() to operate tidily on subsets of a tibble One of the most important aspects of working with tibbles is that of grouping them by variables. In fact, without grouping the “tidy” or “long” format would be nearly impossible to deal with; however with grouping, a huge number of problems can be solved upon tidy data within the tidyverse. Much of the time, we want to apply functions to separate parts of the data set, each part in turn. An example might help: suppose that we want to compute the allele frequencies from genos_long. In other words, we want to count how many times within each river, each allele, at each locus is observed. Each time we say something like at each locus, or within each river, we are implicitly talking about grouping our data. Thus, if we “group our data by river,” we are conceptually, breaking our data set into four distinct tibbles: one with all the rows in which river == \"BALGA\" one with all the rows in which river == \"BCHNC\" one with all the rows in which river == \"BEMME\" one with all the rows in which river == \"BMTMA\" Then (and this is the really important part!) if we pass such a “grouped” tibble to mutate() or summarise(), then those functions will operate on each of those four separate groups, independently, but it will still return the result as a single tibble. To group a tibble, we use the group_by() function. Here we group genos_long by river: genos_long %&gt;% group_by(river) ## # A tibble: 340 × 5 ## # Groups: river [4] ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk1 1 311 ## 2 BALGA BALGA_001 mk1 2 311 ## 3 BALGA BALGA_001 mk2 1 228 ## 4 BALGA BALGA_001 mk2 2 228 ## 5 BALGA BALGA_001 mk3 1 234 ## 6 BALGA BALGA_001 mk3 2 234 ## 7 BALGA BALGA_001 mk4 1 184 ## 8 BALGA BALGA_001 mk4 2 164 ## 9 BALGA BALGA_001 mk5 1 211 ## 10 BALGA BALGA_001 mk5 2 211 ## # … with 330 more rows From the output that gets printed out, it looks like almost nothing has changed about the tibble, except there is this extra line: # Groups: river [4] This tells us that after running genos_long through group_by(river), the result is a tibble that is grouped by the river column, and there are 4 groups. But, now, if we count the number of rows in each of those groups we start to see how powerful this approach can be: genos_long %&gt;% group_by(river) %&gt;% summarise(num_rows = n()) ## # A tibble: 4 × 2 ## river num_rows ## &lt;chr&gt; &lt;int&gt; ## 1 BALGA 70 ## 2 BCHNC 100 ## 3 BEMME 80 ## 4 BMTMA 90 OK! That is telling us about the sizes of those 4 groups, and we can conceptually think of each group as being a “sub-tibble” with the number of rows as listed above. Note that you can group tibbles by more than one column. When grouped on multiple columns, the groups are formed from all the observed combinations of values in the different columns. For example: genos_long %&gt;% group_by(river, marker, allele) ## # A tibble: 340 × 5 ## # Groups: river, marker, allele [93] ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk1 1 311 ## 2 BALGA BALGA_001 mk1 2 311 ## 3 BALGA BALGA_001 mk2 1 228 ## 4 BALGA BALGA_001 mk2 2 228 ## 5 BALGA BALGA_001 mk3 1 234 ## 6 BALGA BALGA_001 mk3 2 234 ## 7 BALGA BALGA_001 mk4 1 184 ## 8 BALGA BALGA_001 mk4 2 164 ## 9 BALGA BALGA_001 mk5 1 211 ## 10 BALGA BALGA_001 mk5 2 211 ## # … with 330 more rows apparently groups the data into 93 different groups. Now, we are ready to try computing allele frequencies (i.e. counting alleles) at different markers. Remember, we want to count how many times within each river and at each locus that each allele is observed. (Note we also don’t want to count missing data for this so we can filter it out in the beginning). Hence: genos_long %&gt;% filter(!is.na(allele)) %&gt;% group_by(river, marker, allele) %&gt;% summarise(num_alleles = n()) ## `summarise()` has grouped output by &#39;river&#39;, &#39;marker&#39;. You can override using the `.groups` argument. ## # A tibble: 84 × 4 ## # Groups: river, marker [20] ## river marker allele num_alleles ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 BALGA mk1 225 1 ## 2 BALGA mk1 311 11 ## 3 BALGA mk1 315 2 ## 4 BALGA mk2 220 4 ## 5 BALGA mk2 228 10 ## 6 BALGA mk3 218 1 ## 7 BALGA mk3 226 1 ## 8 BALGA mk3 230 6 ## 9 BALGA mk3 234 5 ## 10 BALGA mk3 238 1 ## # … with 74 more rows Wow! That is fast and easy. It is relatively straightforward to think about how summarise() works on grouped tibbles, but it is just as important to know that mutate() also operates on the separate groups of a grouped tibble. Note, that the result above has the notation: # Groups: river, marker [20] So, the result is still a grouped tibble, but in this case it is grouped by river, marker. We had originally grouped it by river, marker, allele, BUT by default, the summarise() function will return a tibble in which the last grouping variable (in this case allele) is no longer in effect. In our case, this is convenient, because such a grouping is good for computing relative frequencies of the alleles. We can just add that on to the chain with the pipe. (And while we are at it, sort things by allele frequency.) genos_long %&gt;% filter(!is.na(allele)) %&gt;% group_by(river, marker, allele) %&gt;% summarise(num_alleles = n()) %&gt;% mutate(freq = num_alleles / sum(num_alleles)) %&gt;% ungroup() %&gt;% arrange(river, marker, desc(freq)) ## `summarise()` has grouped output by &#39;river&#39;, &#39;marker&#39;. You can override using the `.groups` argument. ## # A tibble: 84 × 5 ## river marker allele num_alleles freq ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 BALGA mk1 311 11 0.786 ## 2 BALGA mk1 315 2 0.143 ## 3 BALGA mk1 225 1 0.0714 ## 4 BALGA mk2 228 10 0.714 ## 5 BALGA mk2 220 4 0.286 ## 6 BALGA mk3 230 6 0.429 ## 7 BALGA mk3 234 5 0.357 ## 8 BALGA mk3 218 1 0.0714 ## 9 BALGA mk3 226 1 0.0714 ## 10 BALGA mk3 238 1 0.0714 ## # … with 74 more rows A few things are worth noting: sum(num_allele) sums the number of alleles in each river at each marker (because that is how the tibble is grouped). Hence, num_allele / sum(num_allele) gives the relative frequency of each allele (in the river at the marker). ungroup() removes any grouping criteria from a tibble. It is handy if you don’t want to have any groups in your tibble any longer. The arrange function lets us sort the rows in a useful fashion. 3.7.7.1 Exercises for you The reason that the tidy format is wonderful is because you can get a number of different results by following the same principles, but grouping things differently. Try using the tools of ‘dplyr’ to compute the following. A skeleton has been provided. Put the missing code into the ... in each. A tibble with the relative frequency of missing data within each river genos_long %&gt;% group_by(...) %&gt;% summarise(fract_miss = sum(is.na(allele) / n()) A tibble with the relative frequency of missing data at each marker genos_long %&gt;% group_by(...) %&gt;% summarise(fract_miss = ...) 3.7.8 Joins Joins are what you use to add columns from one tibble to another tibble by matching the values in one or more shared columns. We will just show the simplest join, the left_join(). Suppose that we have data about each fish, that tells us: whether they were caught in the autumn or the spring whether they were a juvenile or an adult the length in centimeters. We have an example of such a data set that we can read in: fish_meta &lt;- read_csv(&quot;data/tidy-intro/fish-metadata.csv&quot;) fish_meta ## # A tibble: 1,252 × 4 ## indiv season stage length ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BEMME_001 autumn juvenile 12.9 ## 2 BEMME_002 spring juvenile 6.7 ## 3 BEMME_003 spring adult 23 ## 4 BEMME_004 autumn adult 25.5 ## 5 BEMME_005 autumn juvenile 10 ## 6 BEMME_006 spring juvenile 11.3 ## 7 BEMME_008 spring juvenile 10 ## 8 BEMME_009 spring juvenile 10.2 ## 9 BEMME_010 autumn juvenile 9.3 ## 10 BEMME_011 spring adult 21 ## # … with 1,242 more rows OK! This has season, stage, and length for 1,252 fish. That is a lot more than the 34 fish we have genotype data for. So, how can we pick out the relevant rows from fish_meta to join them on to the columns of genos_long? We use left_join(). Note that in both genos_long and in fish_meta, the individual ID is in the column indiv. This means that we can join the two tibbles by the indiv column in each tibble. Thus: genos_long2 &lt;- genos_long %&gt;% left_join(fish_meta, by = &quot;indiv&quot;) genos_long2 ## # A tibble: 340 × 8 ## river indiv marker gene_copy allele season stage ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 BALGA BALGA_001 mk1 1 311 spring juve… ## 2 BALGA BALGA_001 mk1 2 311 spring juve… ## 3 BALGA BALGA_001 mk2 1 228 spring juve… ## 4 BALGA BALGA_001 mk2 2 228 spring juve… ## 5 BALGA BALGA_001 mk3 1 234 spring juve… ## 6 BALGA BALGA_001 mk3 2 234 spring juve… ## 7 BALGA BALGA_001 mk4 1 184 spring juve… ## 8 BALGA BALGA_001 mk4 2 164 spring juve… ## 9 BALGA BALGA_001 mk5 1 211 spring juve… ## 10 BALGA BALGA_001 mk5 2 211 spring juve… ## # … with 330 more rows, and 1 more variable: ## # length &lt;dbl&gt; Now, you could do all sorts of things like explore allele frequency differences in different seasons or from different life stages. 3.7.9 Wrap Up We have just scratched the surface of what is possible with the tidyverse. One thing we haven’t mentioned much is that the columns of a tibble can, themselves, be list-vectors. This can be very powerful when doing statistical modeling, as the outputs of different models can be stored in lists that are themselves columns of tibbles. Manipulating such lists is done with the purrr package, which is part of the tidyverse. Richard will give us an example of that for the rest of the session. 3.8 Modeling within the Tidyverse Load required packages library(viridis) # preferred color palette for graphing ## Loading required package: viridisLite ## ## Attaching package: &#39;viridis&#39; ## The following object is masked from &#39;package:scales&#39;: ## ## viridis_pal Import bird data. The data provides information on the species richness of birds and butterflies and NDVI at 30 sites. Our goal is to run a linear model that estimates the magnitude, direction, and uncertainty in the species richness-NDVI relationship for each taxa and then visualize the relationship. data_all &lt;- read_csv(&quot;data/data.richard.new.csv&quot;) ## New names: ## * `` -&gt; ...1 ## Rows: 60 Columns: 5 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Site, Taxa ## dbl (3): ...1, NDVI, Richness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. data_all ## # A tibble: 60 × 5 ## ...1 Site NDVI Taxa Richness ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Site.1 0.66 Birds 14 ## 2 2 Site.2 0.4 Birds 10 ## 3 3 Site.3 0.36 Birds 9 ## 4 4 Site.4 0.82 Birds 16 ## 5 5 Site.5 0.55 Birds 12 ## 6 6 Site.6 0.29 Birds 7 ## 7 7 Site.7 0.51 Birds 11 ## 8 8 Site.8 0.74 Birds 16 ## 9 9 Site.9 1 Birds 25 ## 10 10 Site.10 0.76 Birds 16 ## # … with 50 more rows The old way is to run a model on each taxa separately: # subset the Birds data model_birds &lt;- lm(Richness ~ NDVI, data = data_all[data_all$Taxa == &quot;Birds&quot;, ]) # subset the Butterflies model_butterflies &lt;- lm(Richness ~ NDVI, data = data_all[data_all$Taxa == &quot;Butterflies&quot;, ]) Extracting results from the summary file is not easy. # You have to remember that subscript [[4]] corresponds to the main results key_results_birds &lt;- as.data.frame(summary(model_birds)[[4]]) The broom package presents model results in a tidy way. library(broom) broom::tidy() returns parameter estimates. birds_tidy &lt;- model_birds %&gt;% tidy() birds_tidy ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.952 1.22 0.780 4.42e- 1 ## 2 NDVI 19.9 1.77 11.2 6.75e-12 broom::glance() includes R2, AIC, and more. birds_glance &lt;- model_birds %&gt;% glance() birds_glance ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.819 0.812 1.70 127. 6.75e-12 ## # … with 7 more variables: df &lt;dbl&gt;, logLik &lt;dbl&gt;, ## # AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, ## # df.residual &lt;int&gt;, nobs &lt;int&gt; broom::augment() returns fitted values birds_augment &lt;- model_birds %&gt;% augment() birds_augment ## # A tibble: 30 × 8 ## Richness NDVI .fitted .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14 0.66 14.1 -0.0743 0.0334 1.73 3.41e-5 ## 2 10 0.4 8.91 1.09 0.111 1.72 2.90e-2 ## 3 9 0.36 8.11 0.890 0.136 1.72 2.49e-2 ## 4 16 0.82 17.3 -1.26 0.0583 1.71 1.79e-2 ## 5 12 0.55 11.9 0.113 0.0484 1.73 1.17e-4 ## 6 7 0.29 6.72 0.282 0.188 1.73 3.90e-3 ## 7 11 0.51 11.1 -0.0920 0.0603 1.73 9.98e-5 ## 8 16 0.74 15.7 0.335 0.0389 1.73 8.18e-4 ## 9 25 1 20.8 4.17 0.152 1.50 6.35e-1 ## 10 16 0.76 16.1 -0.0625 0.0425 1.73 3.12e-5 ## # … with 20 more rows, and 1 more variable: ## # .std.resid &lt;dbl&gt; To run models or implement any function on the different groups within a data frame, one must master tidyr::nest() and purrr::map(). nest() creates a list of data frames with each corresponding to a different group. Example: create a data frame for each taxa. data_taxa &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() data_taxa ## # A tibble: 2 × 2 ## # Groups: Taxa [2] ## Taxa data ## &lt;chr&gt; &lt;list&gt; ## 1 Birds &lt;tibble [30 × 4]&gt; ## 2 Butterflies &lt;tibble [30 × 4]&gt; The grouping variable is left out of the nested data frame. All other data is now in a separate data frame, nested with the whole data frame. You can inspect what lies behind the “data” column by using unnest() to display the original data. data_original_birds &lt;- data_taxa %&gt;% ungroup() %&gt;% slice(n = 1) %&gt;% unnest(data) # slice(n = 1) means extract the first row of the data_taxa data frame data_original_birds ## # A tibble: 30 × 5 ## Taxa ...1 Site NDVI Richness ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Birds 1 Site.1 0.66 14 ## 2 Birds 2 Site.2 0.4 10 ## 3 Birds 3 Site.3 0.36 9 ## 4 Birds 4 Site.4 0.82 16 ## 5 Birds 5 Site.5 0.55 12 ## 6 Birds 6 Site.6 0.29 7 ## 7 Birds 7 Site.7 0.51 11 ## 8 Birds 8 Site.8 0.74 16 ## 9 Birds 9 Site.9 1 25 ## 10 Birds 10 Site.10 0.76 16 ## # … with 20 more rows map() applies a function to each nested data frame. Example: find the mean number of species of each taxa mean_taxa &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(mean_richness = map(data, function(x) { mean(x %&gt;% pull(Richness)) } )) # map indicates the data frame column with the nested data (i.e., data) and applies a function to that data. # data becomes the &quot;x&quot; in the function. # pull() extracts the variable of interest and returns a vector. mean_taxa ## # A tibble: 2 × 3 ## # Groups: Taxa [2] ## Taxa data mean_richness ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 Birds &lt;tibble [30 × 4]&gt; &lt;dbl [1]&gt; ## 2 Butterflies &lt;tibble [30 × 4]&gt; &lt;dbl [1]&gt; The results are mutated as another list column in the original data frame. They can be unnested. mean_taxa &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(mean_richness = map(data, function(x) { mean(x %&gt;% pull(Richness)) } )) %&gt;% unnest(mean_richness) mean_taxa ## # A tibble: 2 × 3 ## # Groups: Taxa [2] ## Taxa data mean_richness ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 Birds &lt;tibble [30 × 4]&gt; 14.2 ## 2 Butterflies &lt;tibble [30 × 4]&gt; 10.7 More complicated functions are easier to specify outside of the pipe. Example: find the sites with the highest and lowest bird and butterfly species richness # Each element of the list column data will correspond to data frame &quot;x&quot; in the function. get_sites &lt;- function(x) { x %&gt;% filter(Richness %in% c(max(Richness), min(Richness))) %&gt;% mutate(Type = ifelse(Richness == max(Richness), &quot;High&quot;, &quot;Low&quot;)) %&gt;% arrange(desc(Richness)) } # The function get.sites is applied to each element in the list column data. sites_taxa &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(Types = map(data, get_sites)) %&gt;% unnest(Types) %&gt;% select(-data) sites_taxa ## # A tibble: 4 × 6 ## # Groups: Taxa [2] ## Taxa ...1 Site NDVI Richness Type ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Birds 9 Site.9 1 25 High ## 2 Birds 6 Site.6 0.29 7 Low ## 3 Butterflies 48 Site.18 0.81 20 High ## 4 Butterflies 32 Site.2 0.4 6 Low Another example, this time nesting the data based on sites and finding whether a site has more bird or butterfly species. get_taxa &lt;- function(x) { Top_taxa&lt;- x %&gt;% filter(Richness == max(Richness)) %&gt;% pull(Taxa) # Need to indicate sites where birds and butterflies have the same species richness if(length(Top_taxa) &gt; 1) { paste(Top_taxa[1], Top_taxa[2], sep = &quot; + &quot;) } else { Top_taxa } } sites_richness &lt;- data_all %&gt;% group_by(Site) %&gt;% nest() %&gt;% mutate(Animal = map(data, get_taxa)) %&gt;% unnest(Animal) %&gt;% select(-data) sites_richness ## # A tibble: 30 × 2 ## # Groups: Site [30] ## Site Animal ## &lt;chr&gt; &lt;chr&gt; ## 1 Site.1 Birds ## 2 Site.2 Birds ## 3 Site.3 Birds ## 4 Site.4 Birds ## 5 Site.5 Birds ## 6 Site.6 Butterflies ## 7 Site.7 Birds ## 8 Site.8 Birds ## 9 Site.9 Birds ## 10 Site.10 Birds ## # … with 20 more rows Are there any sites where birds and butterflies have equal species richness? site_animals &lt;- sites_richness %&gt;% filter(Animal == &quot;Birds + Butterflies&quot;) site_animals ## # A tibble: 1 × 2 ## # Groups: Site [1] ## Site Animal ## &lt;chr&gt; &lt;chr&gt; ## 1 Site.25 Birds + Butterflies Are there more sites with higher bird or butterfly species richness? richness_summary &lt;- sites_richness %&gt;% group_by(Animal) %&gt;% summarize(No_Sites = n()) richness_summary ## # A tibble: 3 × 2 ## Animal No_Sites ## &lt;chr&gt; &lt;int&gt; ## 1 Birds 26 ## 2 Birds + Butterflies 1 ## 3 Butterflies 3 Run a linear model to estimate the relationship between NDVI and richness for each taxa. “lm” can be specified as a function in map(). The model is mutated as a list column. We can mutate all model results by mapping tidy, glance, and augument onto each model. richness_models &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(Models = map(data, ~lm(Richness ~ NDVI, data = .))) %&gt;% mutate(Tidied = map(Models, tidy), Glanced = map(Models, glance), Augmented = map(Models, augment)) richness_models ## # A tibble: 2 × 6 ## # Groups: Taxa [2] ## Taxa data Models Tidied Glanced Augmented ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 Birds &lt;tibbl… &lt;lm&gt; &lt;tibbl… &lt;tibble… &lt;tibble … ## 2 Butterflies &lt;tibbl… &lt;lm&gt; &lt;tibbl… &lt;tibble… &lt;tibble … Is NDVI more strongly associated with bird or butterfly species richness? Let’s compare the parameter estimates and their precision, which can be returned as a 95% confidence interval by specifying conf.int=TRUE and conf.level=0.95 (the default). richness_models &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(Models = map(data, ~lm(Richness ~ NDVI, data = .))) %&gt;% mutate(Tidied = map(Models, tidy, conf.int = TRUE, conf.level = 0.95), Glanced = map(Models, glance), Augmented = map(Models, augment)) %&gt;% unnest(Tidied) parameter_estimates &lt;- richness_models %&gt;% select(Taxa, term, estimate, conf.low, conf.high) %&gt;% filter(term == &quot;NDVI&quot;) parameter_estimates ## # A tibble: 2 × 5 ## # Groups: Taxa [2] ## Taxa term estimate conf.low conf.high ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Birds NDVI 19.9 16.3 23.5 ## 2 Butterflies NDVI 12.7 8.91 16.4 Graph the results: compare the relationship with NDVI for both species. Rather than use the fitted values, predict richness for all NDVI values from 0 to 1 at intervals of 0.01. The precision of the prediction can be estimated as confidence intervals (interval=“confidence”) or prediction intervals (interval=“prediction”) richness_models &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(Models = map(data, ~lm(Richness ~ NDVI, data = .))) %&gt;% mutate(Tidied = map(Models, tidy), Glanced = map(Models, glance), Predictions = map(Models, augment, newdata = tibble(NDVI = seq(0, 1, 0.01)), se_fit = TRUE, interval = &quot;confidence&quot;)) # Rather than use the original NDVI values for prediction, # specify a new data frame (newdata) with the NDVI values of interest richness_predictions &lt;- richness_models %&gt;% unnest(Predictions) %&gt;% select(Taxa, NDVI, .fitted, .lower, .upper) %&gt;% rename(Richness = .fitted, low.95 = .lower, high.95 = .upper) p &lt;- richness_predictions %&gt;% ggplot(aes(x = NDVI, y = Richness, col = Taxa)) + geom_line(size = 2) + # Visualize confidence interval geom_ribbon(aes(ymin = low.95, ymax = high.95, fill = Taxa), show.legend = FALSE, color = NA, alpha = 0.3) + # Add original data geom_point(data = data_all, aes(x = NDVI, y = Richness, col = Taxa), size = 3) + scale_y_continuous(name = &quot;Species richness\\n&quot;) + scale_x_continuous(name = &quot;\\nNDVI&quot;) + scale_color_viridis_d() + scale_fill_viridis_d() + theme_classic() + theme(axis.line.x = element_line(colour = &quot;black&quot;), axis.line.y = element_line(colour = &quot;black&quot;), text = element_text(size = 12)) p #ggsave(&quot;Richness_vs_NDVI.pdf&quot;, device = &quot;pdf&quot;, width = 6, height = 3, units = &quot;in&quot;, dpi = 600) An exercise to try out: to data_all, add a new column providing the temperature of each site. Then use an analysis and figure to describe how the relationship between NDVI and bird and butterfly species richness is different at low and high temperatures. Hint: to create temperature values, you can sample from a normal distribution. (It’s not entirely realistic as temperature and NDVI should be weakly correlated but for the purposes of the exercise, it is okay). Cross-validation. One of the best ways to assess model performance is via cross-validation. A good model should be able to predict new data, i.e., observations that were not used to construct the model. When you don’t have new data, you can split your original data set into a training set used to build the model and a test set used to validate the model. You can make several splits - folds - to avoid the problem of any particular split containing extreme values. The package modelr has functions for cross-validation library(modelr) ## ## Attaching package: &#39;modelr&#39; ## The following object is masked from &#39;package:broom&#39;: ## ## bootstrap We’ll cross validate a model of the relationship between bird species richness and NDVI. We’ll split the data five times, a five-fold cross-validation. birds_cv &lt;- data_all %&gt;% filter(Taxa == &quot;Birds&quot;) %&gt;% crossv_kfold(k = 5) birds_cv ## # A tibble: 5 × 3 ## train test .id ## &lt;named list&gt; &lt;named list&gt; &lt;chr&gt; ## 1 &lt;resample [24 x 5]&gt; &lt;resample [6 x 5]&gt; 1 ## 2 &lt;resample [24 x 5]&gt; &lt;resample [6 x 5]&gt; 2 ## 3 &lt;resample [24 x 5]&gt; &lt;resample [6 x 5]&gt; 3 ## 4 &lt;resample [24 x 5]&gt; &lt;resample [6 x 5]&gt; 4 ## 5 &lt;resample [24 x 5]&gt; &lt;resample [6 x 5]&gt; 5 We can see there are five training sets of 24 rows and 5 test sets of 6 rows. We can construct a model for the 24 row data set and then test how well each model predicts the “new” points corresponding to the 6 rows held out of the training data set. Root Mean Square Error (RMSE) measures the “distance” between the predicted and observed value. The model will predict the expected species richness for a held-out NDVI value and RMSE measures the distance between the predicted and observed species richness. Luckily, there is a built-in rmse function that takes two arguments, the model constructed from the training data set and the new, test, data set. To map a function with two arguments, we need map2. birds_cv &lt;- data_all %&gt;% filter(Taxa == &quot;Birds&quot;) %&gt;% crossv_kfold(k = 5) %&gt;% mutate(model = map(train, ~lm(Richness ~ NDVI, data = .)), rmse_calc = map2(model, test, ~rmse(.x, .y))) birds_cv ## # A tibble: 5 × 5 ## train test .id model rmse_calc ## &lt;named list&gt; &lt;named l&gt; &lt;chr&gt; &lt;named&gt; &lt;named l&gt; ## 1 &lt;resample [24 x 5]&gt; &lt;resampl… 1 &lt;lm&gt; &lt;dbl [1]&gt; ## 2 &lt;resample [24 x 5]&gt; &lt;resampl… 2 &lt;lm&gt; &lt;dbl [1]&gt; ## 3 &lt;resample [24 x 5]&gt; &lt;resampl… 3 &lt;lm&gt; &lt;dbl [1]&gt; ## 4 &lt;resample [24 x 5]&gt; &lt;resampl… 4 &lt;lm&gt; &lt;dbl [1]&gt; ## 5 &lt;resample [24 x 5]&gt; &lt;resampl… 5 &lt;lm&gt; &lt;dbl [1]&gt; Although, the rmse function makes cross-validation easy, we can calculate it manually to get better insight into what it is measuring. Remember the augment function predicts the y-values for a given set of x-values. birds_cv &lt;- data_all %&gt;% filter(Taxa == &quot;Birds&quot;) %&gt;% crossv_kfold(k = 5) %&gt;% mutate(model = map(train, ~lm(Richness ~ NDVI, data = .)), predictions = map2(model, test, ~augment(.x, newdata = .y))) %&gt;% unnest(predictions) birds_cv ## # A tibble: 30 × 11 ## train test .id model ...1 Site NDVI Taxa ## &lt;named&gt; &lt;named&gt; &lt;chr&gt; &lt;name&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 &lt;resam… &lt;resam… 1 &lt;lm&gt; 1 Site… 0.66 Birds ## 2 &lt;resam… &lt;resam… 1 &lt;lm&gt; 11 Site… 0.5 Birds ## 3 &lt;resam… &lt;resam… 1 &lt;lm&gt; 14 Site… 0.61 Birds ## 4 &lt;resam… &lt;resam… 1 &lt;lm&gt; 18 Site… 0.81 Birds ## 5 &lt;resam… &lt;resam… 1 &lt;lm&gt; 27 Site… 1 Birds ## 6 &lt;resam… &lt;resam… 1 &lt;lm&gt; 30 Site… 0.79 Birds ## 7 &lt;resam… &lt;resam… 2 &lt;lm&gt; 7 Site… 0.51 Birds ## 8 &lt;resam… &lt;resam… 2 &lt;lm&gt; 12 Site… 0.67 Birds ## 9 &lt;resam… &lt;resam… 2 &lt;lm&gt; 13 Site… 0.85 Birds ## 10 &lt;resam… &lt;resam… 2 &lt;lm&gt; 19 Site… 0.86 Birds ## # … with 20 more rows, and 3 more variables: ## # Richness &lt;dbl&gt;, .fitted &lt;dbl&gt;, .resid &lt;dbl&gt; We can inspect if there are certain NDVI values for which our model is over or underpredicting species richness. Augment gives the residuals (.resid), which is the difference between the predicted and observed value. birds_cv &lt;- data_all %&gt;% filter(Taxa == &quot;Birds&quot;) %&gt;% crossv_kfold(k = 5) %&gt;% mutate(model = map(train, ~lm(Richness ~ NDVI, data = .)), predictions = map2(model, test, ~augment(.x, newdata = .y))) %&gt;% unnest(predictions) %&gt;% ggplot(aes(x = NDVI, y = .resid)) + geom_point(size = 2) + geom_hline(yintercept = 0) + # Perfect prediction would have a residual of zero scale_y_continuous(name = &quot;Residual\\n&quot;) + scale_x_continuous(name = &quot;\\nNDVI&quot;) + theme_classic() + theme(axis.line.x = element_line(colour = &quot;black&quot;), axis.line.y = element_line(colour = &quot;black&quot;), text = element_text(size = 12)) birds_cv Finally, we can manually calculate RMSE RMSE = square.root(sum(fitted - observed)^2)/N) birds_cv &lt;- data_all %&gt;% filter(Taxa == &quot;Birds&quot;) %&gt;% crossv_kfold(k = 5) %&gt;% mutate(model = map(train, ~lm(Richness ~ NDVI, data = .)), predictions = map2(model, test, ~augment(.x, newdata = .y)), rmse = map(predictions, function(x){ sqrt((sum(x$.resid^2))/nrow(x))} )) %&gt;% unnest(rmse) birds_cv ## # A tibble: 5 × 6 ## train test .id model predictions rmse ## &lt;named li&gt; &lt;named l&gt; &lt;chr&gt; &lt;named&gt; &lt;named list&gt; &lt;dbl&gt; ## 1 &lt;resample… &lt;resampl… 1 &lt;lm&gt; &lt;tibble [6 … 0.649 ## 2 &lt;resample… &lt;resampl… 2 &lt;lm&gt; &lt;tibble [6 … 0.758 ## 3 &lt;resample… &lt;resampl… 3 &lt;lm&gt; &lt;tibble [6 … 1.10 ## 4 &lt;resample… &lt;resampl… 4 &lt;lm&gt; &lt;tibble [6 … 3.43 ## 5 &lt;resample… &lt;resampl… 5 &lt;lm&gt; &lt;tibble [6 … 0.781 "],["reporg.html", "Session 4 Organizing Research Projects for Reproducibility 4.1 Goals for this session 4.2 Why do this? 4.3 What can this look like? 4.4 Reproducibility Edict #1: Don’t Save Your R Workspace and then Reload It 4.5 Paths, an important concept for reproducibility 4.6 The Working Directory 4.7 Reproducibility Edict #2: Don’t Change Directories in Your Code 4.8 Relative Paths 4.9 Reproducibility Edict #3: Always Use Relative Paths from the Project Directory to Files inside the Project Directory 4.10 Reproducibility Edict #4: Put data files into a data directory 4.11 Reproducibility Edict #5: Write Analysis Scripts as .R or .Rmd files 4.12 Download an example project to play with 4.13 Reproducibility Edict #6: Write your code with style! 4.14 What About Shared Data Sets? 4.15 Reproducibility Edict #7: Use git for Version Control 4.16 In the meantime, a short demo of git/GitHub on RStudio 4.17 What to keep under version control and what not to 4.18 Create a README to describe the parts of your analysis project 4.19 GitHub and Very large data sets", " Session 4 Organizing Research Projects for Reproducibility 4.1 Goals for this session Learn strategies to make our research and particularly our data analyses reproducible. Reproducible here means that we can easily send someone a directory filled with data and scripts, and it is nearly effortless for that person to re-run your entire analysis, regenerating all of your results, figures, and tables, and it is clear what all the different files and parts of your analysis project are, and the order that they must be run in. there is no part of the analysis that is done “by hand.” (i.e., no data formatting is done by copying and pasting columns of data in Excel.) The focus here is on using RStudio for make usefully reproducible projects within R. The ideal we will be striving for is to be able to start from a raw data set and then write a computer program that conducts all the cleaning, manipulation, and analysis of the data, and presentation of the results, in an automated fashion. 4.2 Why do this? Carrying out analyses in this way carries a number of advantages to the researcher: Newly-collected data can be integrated easily into your analysis. If a mistake is found in one section of your analysis, it is not terribly onerous to correct it and then re-run all the downstream analyses. Revising a manuscript to address referee comments can be done quickly. Years after publication, the exact steps taken to analyze the data will still be available should anyone ask you how, exactly, you did an analysis! If you have to conduct similar analyses and produce similar reports on a regular bias with new data each time, you might be able to do this readily by merely updating your data and then automatically producing the entire report. If someone finds an error in your work, they can fix it and then easily show you exactly what they did to fix it. Making research reproducible is good for the community: Others that would like to confirm your results can do so easily. Someone wanting to apply your methods to their own data can easily do so. Reproducibility makes it easier for everyone to learn more quickly from what their colleagues have done. Finally: Most journals require, at a minimum, that the raw data used in a paper be publicly archived and accessible. Increasingly, journals require all the analysis code as well. If you have emphasized reproducibility throughout your project, these last two are quite easy. 4.3 What can this look like? Using R, RStudio and RMarkdown, and freely available tools and services such as git and GitHub, it is possible to publish a reproducible project “website” that is part of a GitHub repository that holds everything needed to reproduce all the results. One example from a project I did in the last few years is here: https://eriqande.github.io/thompson-et-al-2020-chinook-salmon-migration-timing/index.html It took a fair bit to put that together, but I find that I go back to it all the time to direct colleagues and others to the code for certain analyses. 4.4 Reproducibility Edict #1: Don’t Save Your R Workspace and then Reload It R has a feature that lets you save your entire workspace, and then, at a later time, reload it so that you can keep working. Don’t do it. It is too easy to create R objects but not save the code that produced them when saving and loading .Rdata. “Save the code, not the workspace.” Also, in RStudio’s Global Options/Prefernces, make sure to disable automatic saving of your workspace in RStudio and never automatically load .Rdata when you open RStudio: 4.5 Paths, an important concept for reproducibility On your computer system, files can be stored inside directories. A file’s location relative to the root of your computer’s filesystem is called its absolute path. For example, on Windows: # the root is C: &quot;C:\\Documents and Settings\\Eric\\My Documents\\mydoc.txt&quot; Or on Mac: # the root is / /Users/eriq/Documents/git-repos/merida-workshop-2022/merida-workshop-2022.Rproj Absolute Paths are, almost by definition, NOT reproducible. Everyone has their computer set up differently. If I am trying to run a script that my friend, Devon, gave me, and one of his lines of code is: read_csv(&quot;/Users/devon/Documents/stuff/data.csv&quot;) Then, that will not work for me. In order to make that work, I would have to get the file data.csv separately, put the it somewhere on my computer, and then change the line of code above to reflect that file’s location on my computer. What a hassle!! 4.6 The Working Directory When R is running, it finds itself “stationed” in what is called the “working directory.” For any R session, at any time, that directory can be found with: getwd() ## [1] &quot;/Users/eriq/Documents/git-repos/merida-workshop-2022&quot; Everybody try that. 4.6.1 A Hugely Important Point! When you open an RStudio project, the working directory is always automatically set to the Project Directory—that is, the directory that holds the .Rproj file. If you have disabled automatic loading of the .Rdata file, then the R environment will also be completely empty—a blank slate!—when you open an RStudio project. That is how every analysis should start. 4.6.2 Skeletal anatomy of an RStudio Project The Project is basically everything that is inside the Project Directory. The .Rproj file is just a text file listing a few preferences, but your operating system recognizes it as an RStudio file. So, when you double-click it, it will open the project up in RStudio. (This is a good way to open RStudio projects…) 4.7 Reproducibility Edict #2: Don’t Change Directories in Your Code The companion function to getwd() is setwd() which lets you change the working directory. Generally you almost never want to use setwd() in your code. 4.8 Relative Paths A relative path is a path that starts from the working directory. For example, if the working directory contains a file called time-series.csv, then this will read it: read_csv(&quot;time-series.csv&quot;) If the working directory holds a directory called data, inside of which is a file called abundance.csv, then this will work: read_csv(&quot;data/abundance.csv&quot;) 4.9 Reproducibility Edict #3: Always Use Relative Paths from the Project Directory to Files inside the Project Directory You should plan on having an RStudio Project for every project that you work on, where, roughly, a project correponds to the work undertaken for a paper, publication, thesis chapter, report, etc. Everything needed to reproduce the analysis should be included withing the Project Directory. Assume the working directory is the Project Directory. Reading files, use relative paths. This way, the Project Directory has all the data and files needed to run the whole analysis. You can just give an RStudio Project to someone and they can run everything without changing paths, etc. # good read_csv(&quot;data/abundance.csv&quot;) read_csv(&quot;time-series.csv&quot;) source(&quot;R/my-functions.R&quot;) # bad: read_csv(&quot;/Users/eriq/files/data/abundance.csv&quot;) read_csv(&quot;C:/Documents and Settings/data/abundance.csv&quot;) source(&quot;../../../useful_stuff/my-functions.R&quot;) Note, the last “bad” example above uses a relative path to a file that is outside of the Project Directory. 4.10 Reproducibility Edict #4: Put data files into a data directory This data directory should be at the top level of the Project Directory. 4.11 Reproducibility Edict #5: Write Analysis Scripts as .R or .Rmd files These scripts/RMarkdown-documents/notebooks should be at the top level of the Project Directory These should be named with a leading number (I like three digits, like 001, 002, etc.) that gives the order in which they should be executed. Sometimes later scripts use, as inputs, outputs from earlier scripts, in which case the order that they are run in becomes paramount. 4.12 Download an example project to play with At this juncture, let’s all download a simple RStudio project to play with. The project is at: https://github.com/eriqande/small_project You can either: navigate there with a browser, click the green “code” button and “Download ZIP”; then expand that zip archive into a single Project Directory. When you have it, navigate into the small_project directory and double click the small_project.Rproj file to open the project in RStudio. if you have git, then you can open the project directly from RStudio using: File-&gt;New Project-&gt;Version Control-&gt;Git and then put: https://github.com/eriqande/small_project.git into the Repository URL box. 4.12.1 Let’s have a moment Take a moment to open up that project, and try to evaluate the notebooks: 001-read-and-format.Rmd 002-plot-data.Rmd Let’s discuss the structure of the project and have time for questions. In particular let’s talk about the R Notebook format: a combination of explanatory text and code chunks that provides a way of cleanly documenting your findings. 4.13 Reproducibility Edict #6: Write your code with style! R code is a lot easier to read when it is formatted in a consistent style. Your homework is to read the [Tidyverse Style Guide)[https://style.tidyverse.org/]. It takes about an hour and is extremely informative. Following the guidelines in the style guide will make it easier for you (and others) to read and understand your code. 4.14 What About Shared Data Sets? A quick sidebar about the ‘googlesheets4’ package. For some projects, you might want to retrieve data that your research team keeps centralized. Often that will be in a data base. In that case there are ways of accessing those data bases through R (but that is beyond the scope of today’s exploration.) Other times, pertinent information might be stored in a shared Google Sheet (that is Google’s Spreadsheet Format). For example, your lab might keep meta data about samples of birds or fish in a Google Sheet that is shared among lab members and is being frequently updated. For some projects (like for permit reporting requirements) it might be nice to have R code that directly accesses the current version of that document. You can! If you don’t already have ‘googlesheets4’ package, then: install.packages(&quot;googlesheets4&quot;) Imagine that someone has shared this Google Sheet with you. Click the link to view it. This is a publicly accessible sheet of data, so no authentication is needed to view it. Thus you can do: library(googlesheets4) # this is just storing the web address of the sheet sheet_url &lt;- &quot;https://docs.google.com/spreadsheets/d/1-7qG0X8M0TmyzB4Jaad6pNhV_SmcOY6T5JJk53dFU50/edit?usp=sharing&quot; # this tells R to not bother with Google authentication for this session gs4_deauth() fish_sheet &lt;- read_sheet(sheet_url) ## ✓ Reading from &quot;wgs-chinook-samples&quot;. ## ✓ Range &#39;wgs-chinook-samples&#39;. fish_sheet ## # A tibble: 160 × 8 ## vcf_name ID_Berk NMFS_DNA_ID BOX_ID BOX_POSITION ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 DPCh_plate… CH_Plat… T144767 T1512 1A ## 2 DPCh_plate… CH_Plat… T144804 T1512 5F ## 3 DPCh_plate… CH_Plat… T145109 T1515 7G ## 4 DPCh_plate… CH_Plat… T145118 T1515 8H ## 5 DPCh_plate… CH_Plat… T144863 T1513 1A ## 6 DPCh_plate… CH_Plat… T144906 T1513 6D ## 7 DPCh_plate… CH_Plat… T145128 T1515 10B ## 8 DPCh_plate… CH_Plat… T145142 T1515 11H ## 9 DPCh_plate… CH_Plat… T144959 T1514 1A ## 10 DPCh_plate… CH_Plat… T144980 T1514 3F ## # … with 150 more rows, and 3 more variables: ## # Population &lt;chr&gt;, Concentration (ng/ul) &lt;dbl&gt;, ## # run_type &lt;chr&gt; If you are using this to access a sheet that is not publicly shared, then omit the gs4_deauth() command (in a new R session), and R will send you to your browser to authenticate with a google account. (It is quite simple and user friendly!) More information is at the googlesheets4 web page. 4.15 Reproducibility Edict #7: Use git for Version Control RStudio provides one the cleanest, nicest interfaces, anywhere, for using git. Instructions to install git can be found within the comprehensive online book, by Jenny Bryan, happygitwithr. Accomplishing that with everyone in the short time we have would be difficult. But I encourage those who do not yet use git and GitHub to run through the happygitwithr book to get it all set up and start using it. 4.16 In the meantime, a short demo of git/GitHub on RStudio Just to show, visually, what it looks like, using our small project example. 4.17 What to keep under version control and what not to In general, keep data and code under version control do not keep outputs and intermediate files under version control Exceptions that we will talk about: Very large data sets HTML files from RMarkdown that you might want to use in a website. 4.18 Create a README to describe the parts of your analysis project You should always have a README file that describes the project, so that people you give it to will have a starting point, and you will be able to refresh your memory if you come back to it after a long time. The best way to make such a README is to use an RMarkdown file, README.Rmd that creates a markdown file README.md that can be rendered by GitHub. Such a file can be made from within RStudio by choosing File-&gt;New File-&gt;R Markdown...-&gt;From Template-&gt;GitHub Document An exampleREADME.Rmdis in thesmall-project`. 4.19 GitHub and Very large data sets If you have a very large data file, that you don’t want to put on GitHub (or it is too large for GitHub to accept), you can use the the googledrive package. This lets you store large data sets on Google Drive, and then download them automatically from your R code, when you run the scripts in your project. This also allows for authentication so that you can keep private, large data sets on Google Drive, safely. Here is a link to a small data set that I put up, publicly available on Google Drive. It is a serialized R data object (.rds) file. If you click on that link, your browser will be sent to a page that asks if you want to download the data set. But, you could access it directly from within R with the ‘googledrive’ package like this. First, get the package if you don’t already have it. # if you don&#39;t have the googledrive package install.packages(&quot;googledrive&quot;) Then, download it and read it (with read_rds()) in one line: library(googledrive) ## ## Attaching package: &#39;googledrive&#39; ## The following objects are masked from &#39;package:googlesheets4&#39;: ## ## request_generate, request_make # since the data are publicly available on my google drive # we will not worry about authentication. To get private # data at a different link, you would remove this and # run through the authentication process. drive_deauth() # here is the URL for the google drive file. url &lt;- &quot;https://drive.google.com/file/d/1071-7teo1dIPl9cJNigTYrBGN7I0E08U/view?usp=sharing&quot; download_results &lt;- drive_download(url, overwrite = TRUE) ## File downloaded: ## • &#39;RoSA-popgen-survey-data.rds&#39; ## &lt;id: 1071-7teo1dIPl9cJNigTYrBGN7I0E08U&gt; ## Saved locally as: ## • &#39;RoSA-popgen-survey-data.rds&#39; readr::read_rds(download_results$local_path[1]) ## # A tibble: 3,223 × 4 ## Indiv Population rosa pop_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 CH12366 IGH LLLLLLLL Iron Gate hatchery fall ## 2 CH12367 IGH LLLLLLLL Iron Gate hatchery fall ## 3 CH12368 IGH LLLLLLLL Iron Gate hatchery fall ## 4 CH12369 IGH LLLLLLLL Iron Gate hatchery fall ## 5 CH12370 IGH LLLLLLLL Iron Gate hatchery fall ## 6 CH12371 IGH LLLLLLLL Iron Gate hatchery fall ## 7 CH12372 IGH LLLLLLLL Iron Gate hatchery fall ## 8 CH12373 IGH LLLLLLLL Iron Gate hatchery fall ## 9 CH12374 IGH LLLLLLLL Iron Gate hatchery fall ## 10 CH12375 IGH LLLLLLLL Iron Gate hatchery fall ## # … with 3,213 more rows More details can be found at the googledrive web page "],["envdata.html", "Session 5 Finding and Obtaining Environmental Data for Conservation 5.1 Environmental Layers. A general overview", " Session 5 Finding and Obtaining Environmental Data for Conservation This is an introduction to environmental layers for conservation genomics analysis, which was created for the UCMexus Conservation Genomics Workshop 2022. This overview covers coordinate systems, projections, raster data and various examples of commonly-used environmental layers. 5.1 Environmental Layers. A general overview This section provides a brief narrative of Marius Someveille’s PowerPoint presentation. We are trying to succinctly capture the main points in this web/bookdown version so that it might easily be translated with Google Translate. Environmental layers in this context are raster data they carry information about particular “cells” or “pixels” of the earth’s surface. All of these terms will be defined in detail later. For now, here are a few pictures of what environmental layers might look like when they are plotted and displayed: 5.1.1 Environmental layers in conservation genetics 5.1.1.1 Landscape connectivity analysis (gene flow) Connectivity is essential for long term persistence of populations and metapopulations as it affects gene flow. Functional connectivity is the degree to which individuals move through a landscape, which is affected by landscape features and environmental heterogeneity. Landscape genetics tests which landscape and environmental factors best explain genetic connectivity among populations and highlights which landscape features need to be maintained or enhanced to facilitate connectivity (gene flow) for different species. For example, Robertson et al. (2018) studied frogs in multiple locations in North America, shown here: The positions of samples are shown here on a map that was created from land cover data. However, many other layers area available, giving information of, for example, topography, temperature, and moisture levels in the different locations. Landscape genetics approaches were used to identify which of these features were most associated with connectivity between populations in each of the different regions (i.e, different colors on the map). They summarized those results as follows: 5.1.1.2 Environmental Niche Analysis Ruegg et al. (2021) shows another example of the use of environmental layers in conservation genetics. In this case, genetic differences resolved separate breeding populations of the Willow Flycatcher, Empidonax traillii, in North America, and also made it possible to identify where birds from each breeding population spend their winters, as shown in the following figure (in which the different breeding populations are denoted by different colors): With the breeding and wintering areas for each subpopulation identified, climate layers (temperature and precipitation) were used to identify the climatic conditions that defined the birds breeding and wintering niches for the subpopulations. Population trends were found to be associated with overlap between breeding and wintering niches, with the subpopulation showing the least niche overlap being the only subpopulation not showing a decreasing trend. 5.1.1.3 Genome-environment association to study vulnerability Associations between genetic variation and environmental variation (that we will hear about from Christen Bossu!) can be used to identify places where there is expected to be a mismatch between expected genetic patterns and environmental conditions after changes due to climate change. One of the first papers documenting this was Bay et al. (2018). In their study, a variety of environmental predictors (BIO18, BIO15, etc.), whose values across space are represented by environmental layer data, were found to be associated with genetic variation, as shown in the following montage of figures from the paper: 5.1.1.4 Environmental layers as raster data Digital environmental layers are obtained by abstracting reality in either vector or raster format, as shown below: The most common format encountered for environmental layers is raster, which is composed of a grid of cells like this: A cell value can represent a measured value at the center point of the cell (e.g. elevation) or for the whole cell square (average, max, category). The cell values represent the phenomenon portrayed by the raster dataset such as a category (e.g. grassland, forest, road), magnitude (e.g. noise pollution, rainfall), height (e.g. elevation), or spectral value. Cell values can be either positive or negative, integer, or floating point. Commonly used format to store rasters on a computer are: TIFF, Hierarchichal Data Format (HDF), JPEG, NetCDF. 5.1.1.5 Coordinate system and projections For geographic analysis, we use a coordinate system that uses a three-dimensional spherical surface to determine location on the Earth. Each point on Earth is defined by a latitude value and a longitude value, which are measures of angle as shown in the following figure: The units of the geographic coordinate system are units of angle as follows: 1 degree = 60 arc-minutes = 3600 arc-seconds. To visualize geographic data as maps, we need to draw it on a two-dimensional surface using a projected coordinate system. Map projections tell how to distort the earth so that it is displayed on a flat surface. A coordinate reference system (CRS) then defines how the projected map is related to real places on the earth. Many projections exist, here are some examples: Good practice: raster data storing environmental layers should have a projection, and choose one projection for the whole analysis, i.e. converting all of the geographic data into the same projection. 5.1.1.6 Parameters of raster data Rasters storing environmental layers are defined by multiple parameters. The extent is defined by the top, bottom, left, and right coordinates of the rectangular area covered by a raster. The spatial resolution is defined by the width and height of cells/pixels (cells are square and the width and height of all cells have the same value), usually given in degree / arc-minute / arc-second. For environmental variables that have layers at different time, it is possible to stack these layers and define a temporal extent. i.e. the first and last date, and temporal resolution, which is the time step (e.g. daily, weekly, monthly, yearly). 5.1.1.7 Examples of climate layers Source Method Variables Extent Spatial resolution Temporal resolution WorldClim Interpolation of observations from weather stations Temperature Precipitation Solar radiation BIOCLIM Global 30 sec (~1km2) 2.5 min 5 min 10 min (~340 km2) Monthly 1970 – 2000 Chelsa Quasi-mechanistical downscale of global circulation model Temperature Precipitation BIOCLIM Global 30 arc sec (~1km2) Monthly 1979 – 2018 Prism Statistical model using weather station data and atmospheric process Temperature Precipitation USA 30 arc sec (~1km2) Monthly 1981–2010 5.1.2 Examples of atmospheric layers Source Method Variables Extent Spatial resolution Temporal resolution NOAA Global Forecast System Global atmospheric model Wind direction Wind speed Global 0.5 deg 6 times / day 2011 – present WorldClim Interpolation of observations from weather stations Wind speed Water vapor pressure Global 30 sec (~1km2) 2.5 min 5 min 10 min (~340 km2) Monthly 1970 – 2000 5.1.3 Examples of vegetation layers Source Method Variables Extent Spatial resolution Temporal resolution VIP30 (NASA) Remote sensing using spectrometer on board satellites NDVI EVI Global 0.05 deg ~5.6km Monthly 1981 – 2014 Copernicus (ESA) Remote sensing using spectrometer on board satellites NDVI EVI Global 300m 10-day Present GEDI (NASA) Remote sensing using lidar (laser ranging) onboard the ISS Vertical profile of vegetation: canopy height, canopy vertical structure, ground elevation 51.6 deg N and S latitudes 25m Annual Present 5.1.4 Examples of landscape layers Source Method Variables Extent Spatial resolution Temporal resolution SRTM (NASA) Remote sensing using radar onboard satellites Elevation Global 1 arc-second (~30m) NA MODIS (NASA) Remote sensing using spectrometer onboard satellites Snow cover Global 30 arc-sec (~1km2) Copernicus (ESA) remote sensing + random forest for classification Land cover Global 100m NA NASA Gridded Population of the World Input data are census tables Human population density Global 30 arc-sec (~1km2) NA 2000–2020 5.1.5 Marius’s links Here are the links that Marius showed us for some high quality environmental data sets: CHELSA: https://chelsa-climate.org/ USGS Source for NASA data, etc.: https://lpdaac.usgs.gov/. search VIP30 for NDVI and SRTM for elevation. Copernicus Land Cover Data: https://land.copernicus.eu/global/products/lc. The R package for wind that downloads the NOAA wind data is ‘rWind.’ 5.1.6 Example Code Here is the example code that Marius used. The data sets are quite large, so they were not included in this repository. (He also keeps them in a centralized location on his laptop—hence the absolute paths!—since they are so large). # Load packages library(terra) library(rnaturalearth) # Load environmental raster Prec_01_2015 &lt;- terra::rast(&quot;/Users/mariussomveille/Desktop/CHELSA_pr_01_2015_V.2.1.tif&quot;) # Mexico Mexico &lt;- ne_countries(type = &quot;countries&quot;, country = &quot;mexico&quot;, scale=50) plot(Prec_01_2015) plot(Mexico, add=T) # Projection crs(Prec_01_2015) terra::crs(Prec_01_2015) &lt;- crs(Mexico) &lt;- &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; # Change extent ext(Prec_01_2015) Prec_01_2015 &lt;- terra::crop(Prec_01_2015, ext(Mexico)) # c(-170, -30, 0, 60) # Change spatial resolution res(Prec_01_2015) Prec_01_2015_2 &lt;- terra::aggregate(Prec_01_2015, fact=100, fun=mean, na.rm=T) res(Prec_01_2015_2) # Modify values (rescale between 0 and 1) Prec_01_2015_2 &lt;- Prec_01_2015_2 * 1/Prec_01_2015_2@ptr$range_max ## WIND library(rWind) library(lubridate) birdwsep &lt;- ymd_hms(paste(2014, 1, 11, 12,00,00, sep=&quot;-&quot;)) bw = wind.dl_2(birdwsep,-120, -85, 10, 35) birdwindavg = wind2raster(bw)[[1]] crs(birdwindavg) &lt;- &quot;+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0&quot; ## NDVI NDVI_01_2014 &lt;- terra::rast(&quot;/Users/mariussomveille/Desktop/VIP30.A2014001.004.2016246195842.hdf&quot;) terra::crs(NDVI_01_2014) &lt;- proj4string(birdwindavg) NDVI_01_2014 &lt;- terra::crop(NDVI_01_2014[[1]], terra::ext(terra::rast(birdwindavg))) NDVI_01_2014 &lt;- NDVI_01_2014 * 1/NDVI_01_2014@ptr$range_max NDVI_01_2014 &lt;- terra::aggregate(NDVI_01_2014, fact=10, fun=mean, na.rm=T) ## SRTM elevation &lt;- terra::rast(&quot;/Volumes/Marius_SSD/SRTM_1km_ASC/srtm_1km.asc&quot;) terra::crs(elevation) &lt;- proj4string(birdwindavg) elevation &lt;- terra::crop(elevation, terra::ext(terra::rast(birdwindavg))) References Bay RA, Harrigan RJ, Le Underwood V et al. (2018) Genomic signals of selection predict climate-driven population declines in a migratory bird. Science, 359, 83–86. Robertson JM, Murphy MA, Pearl CA et al. (2018) Regional variation in drivers of connectivity for two frog species (rana pretiosa and r. Luteiventris) from the US pacific northwest. Molecular ecology, 27, 3242–3256. Ruegg K, Anderson EC, Somveille M et al. (2021) Linking climate niches across seasons to assess population vulnerability in a migratory bird. Global Change Biology. "],["spatdata.html", "Session 6 Handling and Visualizing Spatial Data in R 6.1 Handling Spatial Rasters in R 6.2 Vector Spatial Data 6.3 Yucatan Jay Occurrences 6.4 Geometric operations 6.5 Joint Operations with Raster and Vector data I: Masking a Raster with a Polygon 6.6 Joint Operations with Raster and Vector data II: Extracting raster values at a set of spatial points 6.7 Appendix: Obtaining and Reducing The Example Spatial Datasets", " Session 6 Handling and Visualizing Spatial Data in R In the previous session, Marius gave a very nice overview of environmental data, which is typically stored in raster form. He described properties of raster data and how to find data sets relevant to biological questions. In this session we give an overview of how to handle spatial data in R. Our topics are: Handling raster data Some background on spatial vector data Handling vector data in R. Geometric operations on spatial data. Plotting spatial data. Let’s load the main libraries that we will use for this session: library(tidyverse) library(terra) library(sf) 6.1 Handling Spatial Rasters in R Regardless of where an environmental data set comes from, if it is stored as a spatial raster it can be read into R and manipulated as an R object. Today, the main package for handling raster data in R is ‘terra.’ I have stored a few small raster data sets showing parts of the Yucatán Peninsula in this repository for examples. This section provides some details about how these small data sets were obtained. 6.1.1 Reading and Printing Raster Data Let us read in two different raster data sets. The first shows average maximum temperature across space around the Yucatán Peninsula in each of the twelve months, and the second shows the values of different BioClim bioclimatic variables over space. We read them in using the rast() function from the ‘terra’ package: yuc_tmax &lt;- rast(&quot;data/spatial/worldclim/yp_tmax.tif&quot;) yuc_bioc &lt;- rast(&quot;data/spatial/worldclim/yp_bioc.tif&quot;) These are stored as SpatRaster objects. This is a particular class from the ‘terra’ package. When we print them we see some relevant information about them: yuc_tmax ## class : SpatRaster ## dimensions : 270, 540, 12 (nrow, ncol, nlyr) ## resolution : 0.008333, 0.008333 (x, y) ## extent : -91, -86.5, 19.5, 21.75 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : yp_tmax.tif ## names : MEX_1, MEX_2, MEX_3, MEX_4, MEX_5, MEX_6, ... ## min values : 27.2, 27.9, 29.0, 30.4, 31.6, 31.9, ... ## max values : 31.2, 32.4, 34.7, 36.6, 37.5, 35.3, ... Or yuc_bioc ## class : SpatRaster ## dimensions : 270, 540, 19 (nrow, ncol, nlyr) ## resolution : 0.008333, 0.008333 (x, y) ## extent : -91, -86.5, 19.5, 21.75 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : yp_bioc.tif ## names : wc2.1~bio_1, wc2.1~bio_2, wc2.1~bio_3, wc2.1~bio_4, wc2.1~bio_5, wc2.1~bio_6, ... ## min values : 24.962, 7.425, 59.325, 149.395, 32.400, 14.400, ... ## max values : 26.95, 15.27, 73.53, 219.19, 37.50, 20.00, ... This output gives information about the parameters of environmental data that Marius has just told us about: for example, the extent (extent) of the raster, the coordinate reference system and projection (coord. ref.) and the resolution (resolution) of the environmental data. There are two lines that I want to emphasize, in particular. dimensions : 270, 540, 12 (nrow, ncol, nlyr) This tells us the number of rows and columns in the raster, and also very importantly, tells us how many layers are in this SpatRaster object. In this case there are 12 layers, one for each month. The line names : MEX_1, MEX_2, MEX_3, MEX_4, MEX_5, MEX_6, ... Gives us the names for each layer. There are many ways to manipulate SpatRaster objects. We won’t go into many, at this point, other than to say that you can extract specific layers like yuc_tmax$MEX_10 ## class : SpatRaster ## dimensions : 270, 540, 1 (nrow, ncol, nlyr) ## resolution : 0.008333, 0.008333 (x, y) ## extent : -91, -86.5, 19.5, 21.75 (xmin, xmax, ymin, ymax) ## coord. ref. : lon/lat WGS 84 (EPSG:4326) ## source : yp_tmax.tif ## name : MEX_10 ## min value : 30.4 ## max value : 33.3 For a great introduction to all the ways you can manipulation rasters with functions from the ‘terra’ package, see the ‘terra’ documentation at: https://rspatial.org/terra/spatial/index.html. 6.1.2 Plotting Raster Data with plot() The easiest way to plot raster data, in order to get an overview of the values in a raster, is to use the plot() function, which, if the ‘terra’ package is available, has a special set of methods for dealing with SpatRaster objects. This plot() function is optimized for raster data, and renders photos quite quickly. It uses a default color scheme for the continuous numeric values in each cell of the raster, and it plots each layer as a separate panel. For example, let’s look at the maximum temperature data: plot(yuc_tmax) The 12 different panels here correspond to the twelve different layers in the SpatRaster, each of these corresponding to one of the twelve months of the year. Take a moment to investigate those plots and ponder questions like: Where is it the coolest? Where it is the warmest? Which month appears to have the most/least variation in temperature across the Peninsula? It is also worth plotting the BioClim data to see how much these bioclimatic variables differ across the Peninsula: # Use maxnl == 20 to make sure all 19 layers are plotted plot(yuc_bioc, maxnl = 20) This is really cool to see! Especially for someone that is not super familiar with the Yucatán—there is a lot of biologically relevant climatic variation across the peninsula. So, what do all these, bio_1 through bio_16 mean? They are defined as follows: BIO1 = Annual Mean Temperature BIO2 = Mean Diurnal Range (Mean of monthly (max temp - min temp)) BIO3 = Isothermality (BIO2/BIO7) (×100) BIO4 = Temperature Seasonality (standard deviation ×100) BIO5 = Max Temperature of Warmest Month BIO6 = Min Temperature of Coldest Month BIO7 = Temperature Annual Range (BIO5-BIO6) BIO8 = Mean Temperature of Wettest Quarter BIO9 = Mean Temperature of Driest Quarter BIO10 = Mean Temperature of Warmest Quarter BIO11 = Mean Temperature of Coldest Quarter BIO12 = Annual Precipitation BIO13 = Precipitation of Wettest Month BIO14 = Precipitation of Driest Month BIO15 = Precipitation Seasonality (Coefficient of Variation) BIO16 = Precipitation of Wettest Quarter BIO17 = Precipitation of Driest Quarter BIO18 = Precipitation of Warmest Quarter BIO19 = Precipitation of Coldest Quarter So, this is really neat. You can very quickly see that, although there is not a huge amount of temperature variation, there is quite a bit of variation in precipitation across the peninsula. 6.1.3 Plotting raster data with ggplot2 ggplot only knows how to plot data that are stored in data frames or tibbles. This means that you cannot hand it a SpatRaster object directly for plotting. There is some support for ggplot from the authors of the ‘terra’ package, in a package called ‘rasterVis.’ This doesn’t seem quite ready for prime time yet (i.e., the version on CRAN has some bugs in it.) Alternatively, if you have very large rasters with colors defined by levels of red, blue, and green, there are fast alternatives to include those within ggplot (See CH’s session next!) Otherwise, the simplest approach to getting all the features of ggplot for plotting rasters is just to convert the information in the raster to a data frame. This can be done with as.data.frame(), as described by the help information at ?terra::as.data.frame(). When applied to a SpatRaster object, as.data.frame() creates a column in a data frame for every layer of the raster. If you, additionally, set xy = TRUE, then the center coordinates of each raster cell are included in that data frame. Thus, we can make a tibble out of a raster and then put everything into four columns, such that it is easy for ggplot to handle it: yuc_bioc_tib &lt;- as.data.frame( yuc_bioc, xy = TRUE ) %&gt;% as_tibble() %&gt;% pivot_longer( cols = -c(x, y), names_to = &quot;layer&quot;, values_to = &quot;value&quot; ) # see what that looks like: yuc_bioc_tib ## # A tibble: 1,718,455 × 4 ## x y layer value ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 -88.1 21.6 wc2.1_30s_bio_1 25.7 ## 2 -88.1 21.6 wc2.1_30s_bio_2 11.1 ## 3 -88.1 21.6 wc2.1_30s_bio_3 70.6 ## 4 -88.1 21.6 wc2.1_30s_bio_4 179. ## 5 -88.1 21.6 wc2.1_30s_bio_5 33 ## 6 -88.1 21.6 wc2.1_30s_bio_6 17.3 ## 7 -88.1 21.6 wc2.1_30s_bio_7 15.7 ## 8 -88.1 21.6 wc2.1_30s_bio_8 27.0 ## 9 -88.1 21.6 wc2.1_30s_bio_9 24.9 ## 10 -88.1 21.6 wc2.1_30s_bio_10 27.5 ## # … with 1,718,445 more rows In that format, ggplot knows just what do do with it, and you can plot it quite easily. First, let’s just plot one of the variables, BIO_1: yuc_bioc_tib %&gt;% filter(layer == &quot;wc2.1_30s_bio_1&quot;) %&gt;% ggplot(aes(x = x, y = y, fill = value)) + scale_fill_viridis_c(option = &quot;magma&quot;) + geom_raster() + theme_bw() + coord_sf() # here, this maintains the aspect ratio One important thing to note: if you want to plot all the different bioclim variables in ggplot using faceting, there is a problem — the colors will apply to the whole range of possible values, so that most won’t show up well. This is especially obvious with the bioclim variables which are on totally different scales: ggplot(yuc_bioc_tib, aes(x = x, y = y, fill = value)) + scale_fill_viridis_c(option = &quot;magma&quot;) + geom_raster() + theme_bw() + coord_sf() + facet_wrap(~layer, ncol = 5) Ha! That looks terrible. On the other hand, if every layer is plotting the same variable, then it can be quite advantageous to have the same color bar for all layers. For example, when looking at the maximum temperature each month, it is actually better to have the same colour scale for all plots: # get a tibble of the tmax values yuc_tmax_tib &lt;- as.data.frame( yuc_tmax, xy = TRUE ) %&gt;% as_tibble() %&gt;% pivot_longer( cols = -c(x, y), names_to = &quot;layer&quot;, values_to = &quot;value&quot; ) %&gt;% mutate( # here make a factor of them so they came out in the # right order layer_f = factor(layer, levels = unique(layer)) ) # plot them ggplot(yuc_tmax_tib, aes(x = x, y = y, fill = value)) + scale_fill_viridis_c(option = &quot;magma&quot;) + geom_raster() + theme_bw() + coord_sf() + facet_wrap(~layer_f, ncol = 3) That makes it a lot easier to see the annual variation in maximum temperature. 6.1.3.1 Quickly back to ’terra’s plot() function It is worth noting that a similar effect can be achieved by setting the range in terra’s plot() function: plot(yuc_tmax, range = c(27,38)) In summary, the plot() function from the terra package is fast and efficient for plotting rasters. However, if you are more comfortable making plots with ‘ggplot2,’ you can use that for plotting rasters, but you need to convert the raster to a data frame. 6.2 Vector Spatial Data Raster data consist of a whole grid of cells or pixels. In this way, they are very much like a digital photograph, or a JPG. When you zoom in on a raster, things get pixelated, just like a digital photograph. Contrast this to when you zoom in on the text of a PDF file. The edges of the letters stay smooth even at very high values of zoom. That is because those letters are not defined by lots of pixels (some white and some black), but rather, by a vector specification of the curves or lines that make up each letter. Spatial data has such an analogue known as vector spatial data. This type of spatial data is not defined in a series of cells, but rather in terms of a number of different points. This can be much more efficient for storing spatial features that are lines or boundaries. For example, two points in space can define a single line, which will not grow “fuzzy” when you zoom in on it. Vector spatial data is particularly appropriate for representing things like country boundaries, rivers, coastlines, the shorelines of lakes, highways and roads, and points in space (for example, sampling locations of birds, etc.) 6.2.1 Vector data features There are three main basic types of vector spatial data features: Points: simply points on the landscape. (Like sampling locations.) Linestrings: lines connecting a series of points (good for rivers or highways, etc.) Polygons: lines connecting a series of points that eventually come back to themselves to create a closed shape. (good for state boundaries, lakes, coastlines, etc.). Polygons can be quite complex, as they may include holes (like a donut!), but, for the most part, those complexities can remain hidden from you, the R user. 6.2.2 Shapefiles The most typical format for storing vector spatial data is the Shapefile, which is commonly used for GIS programs, like ArcView. A shapefile has the extension .shp and is a binary format describing the vector data. It is not human readable. Typically, along with the shapefile will be a number of other files with the same first part of their name, but with different extensions, like .shx, .dbf, .prj. For example, this course repository has a shapefile for the Mexican states in the directory data/spatial/ne_mexico_states whose contents look like: ne_mexico_states/ ├── ne_mexico_states.dbf ├── ne_mexico_states.prj ├── ne_mexico_states.shp └── ne_mexico_states.shx It is worth pointing out that the .prj file contains information about the projection of the points, and .dbf file contains data that correspond to each of the geographic features in the shapefile. This section provides some details about how the Mexican states shapefile was obtained. 6.2.3 Spatial vector data in R There are a few different packages for handling spatial vector data in R. ‘terra’ can deal with it. However, I am particularly fond of the package ‘sf,’ which implements simple features for R. Simple features is a defined standard for representing spatial data. The sf package stores spatial vector data in a special kind of data frame that has any number of columns that represent data specific to each row, and then a special list-column that contains the geometry associated with the row. This geometry is just a series of points that define points, linestings, or polygons, or collections of multiple points, linestrings, or polygons. All the functions in the ‘sf’ package start with st_. Excellent introductory documentation for ‘sf’ is available at https://r-spatial.github.io/sf/articles/sf1.html, and they have a great cheat sheet for the package too: 6.2.4 An example: Mexican states We can use the st_read() function to read in a shapefile of Mexican states. mex_states &lt;- st_read(&quot;data/spatial/ne_mexico_states/ne_mexico_states.shp&quot;) ## Reading layer `ne_mexico_states&#39; from data source ## `/Users/eriq/Documents/git-repos/merida-workshop-2022/data/spatial/ne_mexico_states/ne_mexico_states.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 33 features and 83 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -118.4 ymin: 14.55 xmax: -86.7 ymax: 32.71 ## Geodetic CRS: GCS_unknown It turns out that there are a lot of columns in that result. To make it easier to look at, let’s just look at just a few of the columns. We can do all the typical tidyverse operations on this object, so we use select() to get a few columns, and we make sure we get the geometry column, which has the spatial information in it! ms_thin &lt;- mex_states %&gt;% select( name, type, type_en, postal, geometry ) ms_thin ## Simple feature collection with 33 features and 4 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -118.4 ymin: 14.55 xmax: -86.7 ymax: 32.71 ## Geodetic CRS: GCS_unknown ## First 10 features: ## name type type_en postal ## 1 Sonora Estado State SO ## 2 Baja California Estado State BC ## 3 Chihuahua Estado State CH ## 4 Coahuila Estado State CO ## 5 Tamaulipas Estado State TM ## 6 Nuevo León Estado State NL ## 7 Quintana Roo Estado State QR ## 8 Campeche Estado State CM ## 9 Tabasco Estado State TB ## 10 Chiapas Estado State CS ## geometry ## 1 MULTIPOLYGON (((-114.8 32.5... ## 2 MULTIPOLYGON (((-114.7 32.7... ## 3 MULTIPOLYGON (((-109 31.33,... ## 4 MULTIPOLYGON (((-103.3 29.0... ## 5 MULTIPOLYGON (((-99.71 27.6... ## 6 MULTIPOLYGON (((-99.81 27.7... ## 7 MULTIPOLYGON (((-88.3 18.48... ## 8 MULTIPOLYGON (((-89.15 17.9... ## 9 MULTIPOLYGON (((-90.99 17.8... ## 10 MULTIPOLYGON (((-91.43 17.2... Here, we see that each state takes up 1 row, and the state boundary is stored as a series of points in a “MULTIPOLYGON” sf data structure. 6.2.5 Plotting simple features with ggplot() The authors of the ‘sf’ package worked with Hadley Wickham to provide the geom_sf() function to plot simple feature objects. The geom_sf() object does not require that aesthetics for x and y be set, because it uses the geometry information directly. For example, we can plot the boundaries of the Mexican states like this: ggplot(data = ms_thin) + geom_sf(fill = NA) + theme_bw() + coord_sf() Note that we can also map aesthetics like fill or color to different values in the simple features data frame: ggplot(data = ms_thin) + geom_sf(aes(fill = name)) + theme_bw() + coord_sf() 6.2.6 A word about coord_sf() The coord_sf() function let’s you define the coordinate system for plotting sf objects. By default, it plots a planar system of longitude and latitude, but it also accepts different projections with its crs argument. Using a different crs will reproject any simple features data from whatever coordinate reference system them are in to the new one. For example, we can plot the Mexican states using a Lambert Conformal Conic (LCC) projection, like this: # define the projection with this long string mex_lcc &lt;- &quot;+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-102 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs&quot; # pass that string as the `crs` argument to coord_sf() ggplot(data = ms_thin) + geom_sf(aes(fill = name)) + theme_bw() + coord_sf(crs = mex_lcc) Note how the latitude lines are curved after projection. 6.2.7 A word about resolution of spatial vector data You can zoom in indefinitely on vector defined fonts (like in a PDF file) because the curves that make up the letter are defined by equations. That is not the case with most spatial vector data. Rather, spatial objects are defined by the points within them. Landforms are not perfectly straight lines, nor can they be represented by simple equations. In fact, landforms have a fractal nature. So, the number of points used to represent a shape (like a coastline) in space determines its resolution. We can demonstrate this by viewing the northern Yucatán coastline at different resolutions. I previously downloaded Natural Earth Data maps of different resolutions (details here) Thus, spatial vector data still have a resolution, which is determined by how many points are available to represent curvy lines. If you are plotting maps of the entire world, low resolution works fine. If you are making maps of smaller areas, then higher resolution is needed. For very fine scale coastlines, you can use the U.S. National Oceanic and Atmospheric Administration’s Global Self-consistent, Hierarchical, High-resolution Geography Database (GSHHG). This is a very large data set, but clearly has much higher resolution: In short, if you are working in small areas, then higher resolution sources are needed. 6.3 Yucatan Jay Occurrences Today it is possible to collect a lot of data while using a GPS device. This gives you an unprecedented ability to collect georeferenced data. In most cases, georeferenced data will correspond to samples that were collected at different points. In other words, they represent point data. These data are almost always given as variables with a latitude and a longitude. 6.3.1 Example data set with Latitude and Longitude points For an example of such data, I have downloaded 25,135 observations of the Yucatán Jay from the GBIF data set and saved it in the course repository. It can be read in like this: yjay_full &lt;- read_tsv( &quot;data/spatial/Cyanocorax-yucatanicus.tsv.gz&quot;, progress = FALSE, show_col_types = FALSE ) ## Warning: One or more parsing issues, see `problems()` ## for details There are a lot of columns and rows in that. So, let’s make it a little smaller by selecting only a few columns, and limiting it to EBIRD observations from the year 2017. yjay_thin &lt;- yjay_full %&gt;% filter( str_detect(collectionCode, &quot;EBIRD&quot;), # keep just the EBIRD-associated records year == 2017 # from the year 2017 ) %&gt;% select( # whittle it down to just a few columns of data gbifID, decimalLongitude, decimalLatitude, day, month, year, individualCount, stateProvince, locality ) # and look at the first few rows: yjay_thin ## # A tibble: 3,032 × 9 ## gbifID decimalLongitude decimalLatitude day month ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.30e9 -88.2 21.6 21 5 ## 2 3.30e9 -87.0 20.7 30 9 ## 3 3.29e9 -88.0 21.6 20 10 ## 4 3.29e9 -87.0 20.8 12 12 ## 5 3.29e9 -88.2 20.8 28 7 ## 6 3.29e9 -87.4 20.2 24 2 ## 7 3.29e9 -88.2 21.6 21 5 ## 8 3.29e9 -87.7 18.7 20 11 ## 9 3.29e9 -88.5 17.8 14 4 ## 10 3.28e9 -87.9 21.4 5 5 ## # … with 3,022 more rows, and 4 more variables: ## # year &lt;dbl&gt;, individualCount &lt;dbl&gt;, ## # stateProvince &lt;chr&gt;, locality &lt;chr&gt; 6.3.2 Plotting Lat-Long data If you have lat-long data, you can typically plot them with ggplot() simply as points, using geom_point(), so long as your underlying maps and other spatial data are all in a standard lat-long coordinate system. The geographic coordinates from GBIF (the Yucatán Jay data) use the WGS84 standard, which is the same as the coordinate reference system of ms_thin, as seen by: st_crs(ms_thin) ## Coordinate Reference System: ## User input: GCS_unknown ## wkt: ## GEOGCRS[&quot;GCS_unknown&quot;, ## DATUM[&quot;World Geodetic System 1984&quot;, ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ID[&quot;EPSG&quot;,6326]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;longitude&quot;,east, ## ORDER[1], ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]], ## AXIS[&quot;latitude&quot;,north, ## ORDER[2], ## ANGLEUNIT[&quot;Degree&quot;,0.0174532925199433]]] Therefore, we can plot the Jay occurrence data like this: ggplot() + geom_sf( # getting the background map data = ms_thin, fill = NA ) + geom_point( # placing the points data = yjay_thin, mapping = aes(x = decimalLongitude, y = decimalLatitude), fill = &quot;deepskyblue3&quot;, shape = 21, stroke = 0.2 ) + theme_bw() + coord_sf() 6.3.3 But, geom_point() won’t work with Different Projections / Coordinate Reference Sysytems If you try to plot these with the LCC project from above things don’t work out: ggplot() + geom_sf( # getting the background map data = ms_thin, fill = NA ) + geom_point( # placing the points data = yjay_thin, mapping = aes(x = decimalLongitude, y = decimalLatitude), fill = &quot;deepskyblue3&quot;, shape = 21, stroke = 0.2 ) + theme_bw() + coord_sf(crs = mex_lcc) # using projected coordinates! The points are nowhere to be found (Actually, I think that they are all piled up on top of one another where the single blue dot is visible!), because the x and y coordinates in an LCC projection are totally different than they are in WGS84! This shows that if you want to do spatial things on georeferenced data that simply have lat-long coordinates, you need to make a spatial object out of those data and treat them as spatial objects. Fortunately, that is fairly easy. 6.3.4 Turning something into a simple features object The Jay data can be made an sf object by adding a geometry column to it. This is done with st_as_sf(): yjay_sf &lt;- yjay_thin %&gt;% st_as_sf( coords = c( # tell it which columns have the latitude and longitude &quot;decimalLongitude&quot;, &quot;decimalLatitude&quot; ), crs = 4326 # this is the EPSG shorthand for WGS 84 ) # have a look at it: yjay_sf ## Simple feature collection with 3032 features and 7 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -92.98 ymin: 16.94 xmax: -86.79 ymax: 21.61 ## Geodetic CRS: WGS 84 ## # A tibble: 3,032 × 8 ## gbifID day month year individualCount ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3297327310 21 5 2017 1 ## 2 3295142622 30 9 2017 2 ## 3 3291491655 20 10 2017 1 ## 4 3291004959 12 12 2017 8 ## 5 3289867554 28 7 2017 8 ## 6 3289002522 24 2 2017 NA ## 7 3286797902 21 5 2017 2 ## 8 3286393532 20 11 2017 6 ## 9 3285803459 14 4 2017 2 ## 10 3284777960 5 5 2017 1 ## # … with 3,022 more rows, and 3 more variables: ## # stateProvince &lt;chr&gt;, locality &lt;chr&gt;, ## # geometry &lt;POINT [°]&gt; Now, that yjay_sf variable can be treated as a spatial object and plotted with geom_sf() like this: ggplot() + geom_sf( # getting the background map data = ms_thin, fill = NA ) + geom_sf( # placing the points as spatial objects data = yjay_sf, fill = &quot;deepskyblue3&quot;, shape = 21, stroke = 0.2 ) + theme_bw() + coord_sf(crs = mex_lcc) # using projected coordinates! That’s Nice! Not only does treating these points as spatial objects let us plot it in different coordinate reference systems, but, it also allows us to to spatial geometric operations on them. Several examples of which we will show in the next section. 6.4 Geometric operations The sf package supports a number of geometric operations. We will show just a few of them here. 6.4.1 Spatial Intersection One very powerful operation is a spatial intersection. These intersections can be quite complex, though intersecting point data with polygon data will lead simply to point data, so, for intersecting points, things are not terribly complex. For example, what if we want to know which Mexican states each of these observed Yucatán Jays occurred in. The GBIF data actually tells us that information in the stateProvice column, but let’s pretend for a moment that we don’t have that information — only where they birds were caught: yjay_just_coord &lt;- yjay_sf %&gt;% select(gbifID, geometry) # now, we just have the IDs and the positions of the observations yjay_just_coord ## Simple feature collection with 3032 features and 1 field ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -92.98 ymin: 16.94 xmax: -86.79 ymax: 21.61 ## Geodetic CRS: WGS 84 ## # A tibble: 3,032 × 2 ## gbifID geometry ## &lt;dbl&gt; &lt;POINT [°]&gt; ## 1 3297327310 (-88.23 21.55) ## 2 3295142622 (-87.03 20.67) ## 3 3291491655 (-87.98 21.58) ## 4 3291004959 (-86.96 20.77) ## 5 3289867554 (-88.21 20.76) ## 6 3289002522 (-87.43 20.21) ## 7 3286797902 (-88.23 21.55) ## 8 3286393532 (-87.71 18.71) ## 9 3285803459 (-88.53 17.77) ## 10 3284777960 (-87.87 21.41) ## # … with 3,022 more rows So, if that was all we knew, and we wanted to figure out which states all those birds were observed in, we would intersect the location of each observation with the polygons that define the states. jays_in_states &lt;- st_intersection( yjay_just_coord, # this requires the spatial object that we made ms_thin ) ## Warning: attribute variables are assumed to be ## spatially constant throughout all geometries # look at the result jays_in_states ## Simple feature collection with 2425 features and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -92.98 ymin: 17.94 xmax: -86.79 ymax: 21.59 ## Geodetic CRS: WGS 84 ## # A tibble: 2,425 × 6 ## gbifID name type type_en postal ## * &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3295142622 Quintana Roo Estado State QR ## 2 3291004959 Quintana Roo Estado State QR ## 3 3286393532 Quintana Roo Estado State QR ## 4 3282675523 Quintana Roo Estado State QR ## 5 3281473008 Quintana Roo Estado State QR ## 6 3279106041 Quintana Roo Estado State QR ## 7 3277142270 Quintana Roo Estado State QR ## 8 3275579212 Quintana Roo Estado State QR ## 9 3273312480 Quintana Roo Estado State QR ## 10 3252526920 Quintana Roo Estado State QR ## # … with 2,415 more rows, and 1 more variable: ## # geometry &lt;POINT [°]&gt; No way! That is so cool! This new simple features tibble now has all the birds that were seen in Mexico, and, for each one, it has added the columns from the Mexican states data. So, we know for each bird, which state (in the column name) is was found in. Now, for example, if we wanted to plot these points and color them by stateProvince we could do this: ggplot() + geom_sf( # getting the background map data = ms_thin, fill = NA ) + geom_sf( # placing the points as spatial objects data = jays_in_states, mapping = aes(fill = name), # coloring by name column shape = 21, stroke = 0.2 ) + theme_bw() + coord_sf(xlim = c(-95, -85), ylim = c(16, 23)) Cool! 6.4.2 Treat the result as a tibble One very important thing to keep in mind is that a simple features collection works quite nicely within the tidyverse. So, for example, if you wanted to retain only those observations of jays that occurred in the state of Yucatàn, you can use a simple filter(): yuc_state_jays &lt;- jays_in_states %&gt;% filter(name == &quot;Yucatán&quot;) # have a look: yuc_state_jays ## Simple feature collection with 1082 features and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -90.4 ymin: 19.99 xmax: -87.57 ymax: 21.59 ## Geodetic CRS: WGS 84 ## # A tibble: 1,082 × 6 ## gbifID name type type_en postal ## * &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3297327310 Yucatán Estado State YU ## 2 3291491655 Yucatán Estado State YU ## 3 3289867554 Yucatán Estado State YU ## 4 3286797902 Yucatán Estado State YU ## 5 3284777960 Yucatán Estado State YU ## 6 3283999442 Yucatán Estado State YU ## 7 3243749415 Yucatán Estado State YU ## 8 3176970726 Yucatán Estado State YU ## 9 2795705902 Yucatán Estado State YU ## 10 2794893768 Yucatán Estado State YU ## # … with 1,072 more rows, and 1 more variable: ## # geometry &lt;POINT [°]&gt; 6.4.3 The same filtering can be done on the state polygons By way of demonstration, we could also filter our data set of state boundaries to have just the state of Yucatán: yuc_boundary &lt;- ms_thin %&gt;% filter(name == &quot;Yucatán&quot;) And then we could plot the two together quite easily: ggplot() + geom_sf(data = yuc_boundary) + geom_sf( data = yuc_state_jays, fill = &quot;deepskyblue3&quot;, shape = 21, stroke = 0.2 ) + theme_bw() + coord_sf() + ggtitle(&quot;Year 2017 EBIRD observations of YUJA in Yucatán state&quot;) 6.5 Joint Operations with Raster and Vector data I: Masking a Raster with a Polygon In the previous section, we saw an example of a geometric operation between two vector spatial features—the intersection between polygons and points. Another class of important operations are those involving raster data and vector data. Suppose that we want to make a plot of the BioClim variables that only includes the state of Yucatán. The way this is typically done is to mask a raster using the shape of the polygon for the state of Yucatán. The ‘terra’ package offers the mask() function for this operation. You can think of it as a sort or intersection between the raster and the polygon: all the raster cells inside the polygon stay the same avlue, and all the rest get the value NA. The function terra::mask() uses R’s S4 system for deciding how to behave. Its first argument, x is a raster that you want to mask, and when its second argument, mask, is of type SpatVector, it uses that SpatVector to to mask the raster. A SpatVector is a data type used by the package ‘terra.’ It can be thought of as an alternative way of storing spatial vector data, and ‘sf’ objects (like yuc_boundary) can be converted into a SpatVector using ’terra’s vect() function, like vect(yuc_boundary). Making a new raster by masking is as easy as: yuc_masked_bioc &lt;- mask( x = yuc_bioc, mask = vect(yuc_boundary) ) And we can plot that to see what it looks like: plot(yuc_masked_bioc) 6.6 Joint Operations with Raster and Vector data II: Extracting raster values at a set of spatial points If you have sampled a boatload of birds across a large space, you might be interested in extracting environmental data from the locations where they were sampled. Understanding the environmental and climatic conditions where a species has been observed (versus where it is not found) forms the basis for niche modeling and other types of inference. Thus, extracting values of a raster cell that contains an individual spatial point is an important task. We will take a small example problem here: suppose that we wish to extract the BioClim values at the point where every Yucatán Jay in yuc_state_jays was observed. For this operation, we use the function extract() from the ‘terra’ package. This also uses R’s S4 system of object orientiation. When you pass the extract() function a SpatRaster as the first argument, and a SpatVector as the second argument, it returns the values of the cells overlapped by the SpatVector. In the case of spatial points, this returns a single value for each point. Note that the ‘tidyr’ package also has a function called extract(), so it is wise to always be explicit and use namespace addressing when you want ’terra’s extract() function, i.e., call it terra::extract(). When deployed like this, extract() returns a data frame with one row for each bird observation. We turn it into a tibble so it prints nicely: terra::extract(yuc_bioc, vect(yuc_state_jays)) %&gt;% as_tibble() ## # A tibble: 1,082 × 20 ## ID wc2.1_30s_bio_1 wc2.1_30s_bio_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 25.8 11.6 ## 2 2 NaN NaN ## 3 3 25.8 14.0 ## 4 4 25.8 11.6 ## 5 5 25.5 13.0 ## 6 6 25.8 14 ## 7 7 25.6 11.6 ## 8 8 25.5 14.1 ## 9 9 26.0 13.2 ## 10 10 26.0 13.2 ## # … with 1,072 more rows, and 17 more variables: ## # wc2.1_30s_bio_3 &lt;dbl&gt;, wc2.1_30s_bio_4 &lt;dbl&gt;, ## # wc2.1_30s_bio_5 &lt;dbl&gt;, wc2.1_30s_bio_6 &lt;dbl&gt;, ## # wc2.1_30s_bio_7 &lt;dbl&gt;, wc2.1_30s_bio_8 &lt;dbl&gt;, ## # wc2.1_30s_bio_9 &lt;dbl&gt;, wc2.1_30s_bio_10 &lt;dbl&gt;, ## # wc2.1_30s_bio_11 &lt;dbl&gt;, wc2.1_30s_bio_12 &lt;dbl&gt;, ## # wc2.1_30s_bio_13 &lt;dbl&gt;, wc2.1_30s_bio_14 &lt;dbl&gt;, … If you wanted to insert those results seamlessly into the original simple features collection, with some nice tidyverse idioms, you could do like this: jays_and_bioc &lt;- yuc_state_jays %&gt;% bind_cols(terra::extract(yuc_bioc, vect(.))) jays_and_bioc ## Simple feature collection with 1082 features and 25 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -90.4 ymin: 19.99 xmax: -87.57 ymax: 21.59 ## Geodetic CRS: WGS 84 ## # A tibble: 1,082 × 26 ## gbifID name type type_en postal ## * &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3297327310 Yucatán Estado State YU ## 2 3291491655 Yucatán Estado State YU ## 3 3289867554 Yucatán Estado State YU ## 4 3286797902 Yucatán Estado State YU ## 5 3284777960 Yucatán Estado State YU ## 6 3283999442 Yucatán Estado State YU ## 7 3243749415 Yucatán Estado State YU ## 8 3176970726 Yucatán Estado State YU ## 9 2795705902 Yucatán Estado State YU ## 10 2794893768 Yucatán Estado State YU ## # … with 1,072 more rows, and 21 more variables: ## # geometry &lt;POINT [°]&gt;, ID &lt;dbl&gt;, ## # wc2.1_30s_bio_1 &lt;dbl&gt;, wc2.1_30s_bio_2 &lt;dbl&gt;, ## # wc2.1_30s_bio_3 &lt;dbl&gt;, wc2.1_30s_bio_4 &lt;dbl&gt;, ## # wc2.1_30s_bio_5 &lt;dbl&gt;, wc2.1_30s_bio_6 &lt;dbl&gt;, ## # wc2.1_30s_bio_7 &lt;dbl&gt;, wc2.1_30s_bio_8 &lt;dbl&gt;, ## # wc2.1_30s_bio_9 &lt;dbl&gt;, wc2.1_30s_bio_10 &lt;dbl&gt;, … 6.7 Appendix: Obtaining and Reducing The Example Spatial Datasets We don’t want our teaching repository to be too large, so I subsetted a few of our example data down to just the Yucatán region. The process for doing this with the different data sets is shown here. 6.7.1 Raster Data To get the environmental data, I used the ‘geodata’ package, which makes it easy to download the data sets. Unfortunately, my computer is old enough that one of the security certificates on it does now allow direct downloading via R of the datasets, but the error message it gave told me the URL for the data set, and I could download it with my browser, then read it in. I use namespace addressing (i.e., package::function()) to be explicit about which package each of the functions is in. 6.7.1.1 Worldclim Data # get worldclim data for mexico at 0.5 minutes # this fails for me on Mac Mojave because of a certification issue. Lame. #mex_tmax &lt;- geodata::worldclim_country( # country = &quot;Mexico&quot;, # var = &quot;tmax&quot;, # res = 0.5, # path = &quot;/tmp&quot; #) # the error from that showed me that the URL was: # https://biogeo.ucdavis.edu/data/worldclim/v2.1/tiles/iso/MEX_wc2.1_30s_tmax.tif mex_tmax &lt;- terra::rast(&quot;~/Downloads/MEX_wc2.1_30s_tmax.tif&quot;) # I used the same procedure to get the BioClim variables mex_bioc &lt;- terra::rast(&quot;~/Downloads/MEX_wc2.1_30s_bio.tif&quot;) Those rasters are very large. If we whittle it down to focus on a smaller area, the files will be smaller. So, we focus only on the area around the Yucatán Peninsula. To do this we define a latitude and longitude extent, and then crop() the raster to that extent, using functions from the ‘terra’ package. # Here is the extent that includes the Yucatan Province. # The ext() function can take lat/lon as # xmin, xmax, ymin, ymax, as done here. yp_extent &lt;- terra::ext(c(-91, -86.5, 19.5, 21.75 )) # now we crop the raster of all of Mexico down to just # a small part. yp_tmax &lt;- crop(mex_tmax, yp_extent) yp_bioc &lt;- crop(mex_bioc, yp_extent) Finally, we save just that small Yucatán part of the raster in our repository here, so everyone can use it. dir.create(&quot;data/spatial/worldclim&quot;, recursive = TRUE, showWarnings = FALSE) writeRaster(yp_tmax, &quot;data/spatial/worldclim/yp_tmax.tif&quot;, overwrite = TRUE) writeRaster(yp_bioc, &quot;data/spatial/worldclim/yp_bioc.tif&quot;, overwrite = TRUE) 6.7.2 Vector data 6.7.2.1 Mexico State Boundaries For polygons and lines for countries, states, coastlines, etc., a wonderful source is Natural Earth Data, a beautiful, consistent, and open-source set of vector (and raster) data for making beautiful maps. There are several R packages that make it fairly easy to obtain the Natural Earth data. In paricular, ‘rnaturalearth’ and ‘rnaturalearthdata.’ I obtained polygon data for the different Mexican states/provinces with this command: mex_states &lt;- rnaturalearth::ne_states( country = &quot;mexico&quot;, returnclass = &quot;sf&quot; ) That command returns the contents of the shapefile as a simple features object. Then we saved that to the repository as a shapefile. dir.create(&quot;data/spatial/ne_mexico_states&quot;, recursive = TRUE, showWarnings = FALSE) st_write(mex_states, dsn = &quot;data/spatial/ne_mexico_states/ne_mexico_states.shp&quot;, append=FALSE) 6.7.2.2 Yucatan coastline at different resolutions # get coastline maps at different resolutions sf_dat &lt;- list( res10 = rnaturalearth::ne_coastline(scale = 10, returnclass = &quot;sf&quot;), res50 = rnaturalearth::ne_coastline(scale = 50, returnclass = &quot;sf&quot;), res110 = rnaturalearth::ne_coastline(scale = 110, returnclass = &quot;sf&quot;) ) # now, crop each of those coastlines to the part we want sf_cropped &lt;- lapply( sf_dat, function(x) st_crop( x, c(xmin = -91, ymin = 19.5, xmax = -86.5, ymax = 21.75) ) ) p10 &lt;- ggplot(sf_cropped$res10) + geom_sf(fill = NA) + coord_sf() + ggtitle(&quot;High(ish) Res&quot;) + theme_bw() p50 &lt;- ggplot(sf_cropped$res50) + geom_sf(fill = NA) + coord_sf() + ggtitle(&quot;Medium Res&quot;) + theme_bw() p110 &lt;- ggplot(sf_cropped$res110) + geom_sf(fill = NA) + coord_sf() + ggtitle(&quot;Low Res&quot;) + theme_bw() g &lt;- cowplot::plot_grid(p110, p50, p10, nrow = 3) ggsave(g, filename = &quot;images/5.0/vector-resolutions.svg&quot;, width = 6, height = 11) # Use the full-resolution GSHHG for comparison gshhg_full &lt;- st_read(&quot;~/Downloads/gshhg-shp-2.3.7/GSHHS_shp/f/GSHHS_f_L1.shp&quot;) # remove a couple of invalid geometries gshhg_clean &lt;- gshhg_full %&gt;% filter(., st_is_valid(.)) # now, crop that gshhg_cropped &lt;- gshhg_clean %&gt;% st_crop(c(xmin = -91, ymin = 19.5, xmax = -86.5, ymax = 21.75)) # and plot it: full &lt;- ggplot(gshhg_cropped) + geom_sf(fill = NA) + coord_sf() + ggtitle(&quot;GSHHG&quot;) + theme_bw() + annotate( &quot;rect&quot;, xmin = -87.5, xmax = -86.5, ymin = 21.25, ymax = 21.75, colour = &quot;red&quot;, fill = NA ) part &lt;- ggplot(gshhg_cropped) + geom_sf(fill = NA) + coord_sf( xlim = c(-87.5, -86.5), ylim = c(21.25, 21.75) ) + ggtitle(&quot;GSHHG zoomed way in!&quot;) + theme_bw() g2 &lt;- cowplot::plot_grid(full, part, nrow = 2) ggsave(g2, filename = &quot;images/5.0/gshhg.svg&quot;, width = 6, height = 8) 6.7.2.3 Yucatán Municipality Boundaries A quick web-search found an open-source data set with municipalies archived at New York University: https://archive.nyu.edu/bitstream/2451/37132/1/nyu_2451_37132.zip This is a small file that I downloaded and then put into the repo with the name yuc_municipio. It can be loaded like this: yuc_muni &lt;- st_read(&quot;data/spatial/yuc_municipio/yuc_municipio.shp&quot;) We didn’t end up using this in this session/notebook, but it is a fun data set if you want to play with it. Here is something to try: count up how many jay observations there were in each of the municipalities. 6.7.3 Species Occurrence Data 6.7.3.1 Yucatan Jay Occurrences These data were download from https://www.gbif.org/ and searched for the species “Yucatan Jay.” This returns one result which links to the page for the data set. I downloaded in and then gzipped it and saved it into data/spatial. "],["geassoc.html", "Session 7 Genetic and Environmental Associations 7.1 Background 7.2 Motivation 7.3 Housekeeping 7.4 Running Gradient Forest 7.5 Basic Gradient Forest Preparation 7.6 Defining “Genomic Vulnerability/Genomic offset” Given Future Climate Predictions", " Session 7 Genetic and Environmental Associations 7.1 Background Now that we know how to get environmental layers and plot them in space, for many of the species we are working on, we are not entirely sure which environmental variables are important drivers of populaiton genetic structure or underlying phenotypic traits of interest. Let’s back up. Now, we have access to multiple layers of environmental layers that span a species range: And many of us now have genomic data. Here, we used genome-wide reduced representation sequencing to create a spatially explicit map of population structure, called the genoscape, of the Willow Flycatcher. What we ultimately want to investigate is how environmental variables are correlated with genetic variation. Standing genetic variation associated with current climate gradients theoretically indicates adaptive capacity. However, modeling future adaptation from individual loci is difficult because we don’t understand all GxE interactions. A recent example illustrated that not only is geography (“isolation by distance”) strongly correlated genetic structure, but environment plays a role in differentiation too. Specifically, precipitation is a strong predictor of Yellow Warbler genetic structure. 7.2 Motivation The R package gradientForest traditionally quantifies multi-species compositional turnover along environmental gradients. In the era of next-generation sequencing, where genome-wide data is common, we can extend this community ecology framework to determine which variants are associated with specific environmental variables. So rather than thinking of the large-scale species turnover in space, we can map fine-scale allele frequency changes along environemntal gradients. Gradient forest is a machine learning approach for classification and regression based on decision trees. We’ve all seen and used decision trees, for mundane tasks like waking up to an alarm each morning to more intricate decisions like optimal pairing of individuals in a captive breeding program to increase/maintain genetic diversity of a population. Regardless of which decision tree you are implementing, the assumption remains, the higher the split, the more important the predictor. Because decision trees are prone to overiftting, gradient forest uses random forests to test associations. Random forests use a random subset of predictors for each decision tree. Choose a subset of data and predictors. Find predictor (and value) that optimizes split. Create many trees and aggregate. Random forests and genomic data Brieuc et al. 2018 used this approach to rank the importance of SNPs for predicting a specific binary phenotype, e.g. disease resistance in fish. This is an example of one random tree, which can ultimately create a predictive polygenic model. Another interesting study used random forests to find the predictive variants for cold hardiness, which was subsequently used to predict cold injury in the Sitka Spruce (Holliday et al. 2012). What if we have multiple SNPs AND multiple phenotypes or environments? Gradient forest extends random forest to incorporate multiple response variables. Gradient uses turnover functions to look at total allele turnover across environmental gradients. For instance, GI5, a candidate gene in Balsam Poplar, clearly shows great allele frequency transitions across the mean summer temperature (Fitzpatrick and Keller 2015) If we look at different subsets of genetic variation in Balsam Poplar space reference SNPs, GI5, a candidate gene for genetic control of bud set and bud flush (Olson et al. 2013) you can see that different environmental variables predict genetic variation of the reference and candidate genes. Note different colors = locally adapted genotypes. Below is code for quantifying and visualizing intraspecific gene-environment variation across space. As part of The Bird Genoscape Project, we compare present-day gene-environment associations to predicted future changes to identify regions of genomic vulnerability. For now, this tutorial focuses on analyzing and plotting the allelic variation as estimated from gradient forest for a single species, the Willow Flycatcher, but is easily adpated to include other species and methods for esimating variation. 7.3 Housekeeping We will need the following R packages. gradientForest requires the dependancy extendedForest, an R package classification and regression based on forest trees using random inputs. With the newest version of R, extended forest is installed automatically as a dependency. Install and load the following packages in Rstudio. You can use the Packages function to the right to install most packages except for gradient forest, which we will install from R-Forge. As discussed in Setting Up Your Computer, if you have a Mac, you will need the XCode command line tools. If you have a PC, you’ll need Rtools You will also need gfortran on your computer. See the instructions in Setting Up Your Computer if you don’t have that set up already. Also, you need to have the package ‘gradientForest’ installed as described in the Preface section The ‘gradientForest’ package. #install.packages(&quot;gradientForest&quot;, repos=&quot;http://R-Forge.R-project.org&quot;) library(gradientForest) library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:terra&#39;: ## ## copy, shift ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last ## The following object is masked from &#39;package:purrr&#39;: ## ## transpose library(raster) ## Loading required package: sp ## ## Attaching package: &#39;raster&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select library(gstat) library(rgdal) ## Please note that rgdal will be retired by the end of 2023, ## plan transition to sf/stars/terra functions using GDAL and PROJ ## at your earliest convenience. ## ## rgdal: version: 1.5-28, (SVN revision 1158) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 3.2.1, released 2020/12/29 ## Path to GDAL shared files: /Library/Frameworks/R.framework/Versions/4.1/Resources/library/rgdal/gdal ## GDAL binary built with GEOS: TRUE ## Loaded PROJ runtime: Rel. 7.2.1, January 1st, 2021, [PJ_VERSION: 721] ## Path to PROJ shared files: /Library/Frameworks/R.framework/Versions/4.1/Resources/library/rgdal/proj ## PROJ CDN enabled: FALSE ## Linking to sp version:1.4-6 ## To mute warnings of possible GDAL/OSR exportToProj4() degradation, ## use options(&quot;rgdal_show_exportToProj4_warnings&quot;=&quot;none&quot;) before loading sp or rgdal. ## Overwritten PROJ_LIB was /Library/Frameworks/R.framework/Versions/4.1/Resources/library/rgdal/proj ## ## Attaching package: &#39;rgdal&#39; ## The following object is masked from &#39;package:terra&#39;: ## ## project library(sf) library(RColorBrewer) library(rasterVis) ## Loading required package: lattice library(tidyverse) And we will need to load a realistic, polygon shapefile (an outline of the Willow Flycatcher’s breeding range in North America for this exercise) and set its projection. Note, you will need to adjust the path to the shapefile below. This is the path to my shape file on my computer. That will produce this map: wifl_shape &lt;-st_read(&quot;data/6.0/WIFLrev.shp&quot;) ## Reading layer `WIFLrev&#39; from data source ## `/Users/eriq/Documents/git-repos/merida-workshop-2022/data/6.0/WIFLrev.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 5 features and 14 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -128.4 ymin: 29.04 xmax: -67.01 ymax: 53.46 ## Geodetic CRS: WGS 84 ggplot(wifl_shape) + geom_sf(fill=NA) + theme_bw() FIGURE 7.1: A basic outline of Willow Flycatcher breeding range we will use for this exercise 7.4 Running Gradient Forest We will use gradient forest to test whether a subset of genomic variation can be explained by environmental variables and to visualize climate-associated genetic variation across the breeding range of the Willow Flycatcher. To run gradient forest we need two main files. First we need the environmental data/climate variables associated with each population. Populations are defined by their latitude and longitude. For each sampling location, we obtained environmental data from publically available databases. Specifically, we chose variables with known impacts on bird physiology and ecology and that were publically available at high resolution. These variables included 19 climate variables downloaded from WorldClim (Hijmans et al. 2005) as well as vegetation indices (Carroll et al. 2004) (NDVI and NDVIstd), Tree Cover (Sexton et al. 2013) and elevation data from the Global Land Cover Facility (http://www.landcover.org). We also downloaded and used a measure of surface moisture characteristics from the NASA Scatterometer Climate Record Pathfinder project (QuickSCAT, downloaded from scp.byu.edu). Read in the environmental variables, and take a look at the dimensions and the actual data. Ewifl&lt;-read_csv(&#39;data/6.0/wiflforest.env.csv&#39;) ## Rows: 21 Columns: 23 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (23): BIO1, BIO2, BIO3, BIO4, BIO5, BIO6, BIO7,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. dim(Ewifl) ## [1] 21 23 How many sites do we have environmental data for? How many environmental variables? head(Ewifl) ## # A tibble: 6 × 23 ## BIO1 BIO2 BIO3 BIO4 BIO5 BIO6 BIO7 BIO8 BIO9 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 193 165 43 7888 392 15 377 95 227 ## 2 205 165 44 7792 395 23 372 300 242 ## 3 52 180 50 6365 237 -116 353 131 78 ## 4 86 166 47 6468 290 -63 353 13 171 ## 5 168 107 54 3298 265 65 200 132 206 ## 6 166 158 44 7032 360 4 356 94 258 ## # … with 14 more variables: BIO10 &lt;dbl&gt;, BIO11 &lt;dbl&gt;, ## # BIO12 &lt;dbl&gt;, BIO13 &lt;dbl&gt;, BIO14 &lt;dbl&gt;, ## # BIO15 &lt;dbl&gt;, BIO16 &lt;dbl&gt;, BIO17 &lt;dbl&gt;, ## # BIO18 &lt;dbl&gt;, BIO19 &lt;dbl&gt;, NDVI_Mean &lt;dbl&gt;, ## # NDVI_StDev &lt;dbl&gt;, SRTM &lt;dbl&gt;, TreeCover &lt;dbl&gt; Second, we have genomic data formatted as allele frequencies for each population in the same order. The published Willow Flycatcher data includes 37855 variants, and is too large to run a gradient forest analyses within the time frame of a tutorial, so we are supplying a subsetted dataset limited to 10,000 SNPs. Gwifl&lt;-read_csv(&#39;data/6.0/wiflforest.allfreq.sample.csv&#39;) ## Rows: 21 Columns: 10000 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (10000): V21252, V9139, V22510, V5860, V3386, V... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. dim(Gwifl) ## [1] 21 10000 head(Gwifl) ## # A tibble: 6 × 10,000 ## V21252 V9139 V22510 V5860 V3386 V14932 V14581 V9277 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.15 0.333 0.25 0.45 0.3 0.3 0.4 0.125 ## 2 0.107 0.25 0.25 0.571 0.308 0.607 0.357 0.143 ## 3 0.115 0.231 0.167 0.654 0.308 0.208 0.192 0.136 ## 4 0 0.1 0.0833 0.667 0.417 0.167 0 0.1 ## 5 0.25 0.667 0.125 0.625 0 0.75 0.25 0 ## 6 0.192 0.0833 0.0385 0.115 0.192 0.269 0.154 0.0909 ## # … with 9,992 more variables: V25872 &lt;dbl&gt;, ## # V23716 &lt;dbl&gt;, V25026 &lt;dbl&gt;, V11453 &lt;dbl&gt;, ## # V2266 &lt;dbl&gt;, V371 &lt;dbl&gt;, V21324 &lt;dbl&gt;, ## # V28422 &lt;dbl&gt;, V8773 &lt;dbl&gt;, V5906 &lt;dbl&gt;, ## # V15909 &lt;dbl&gt;, V20940 &lt;dbl&gt;, V15435 &lt;dbl&gt;, ## # V9026 &lt;dbl&gt;, V12438 &lt;dbl&gt;, V2181 &lt;dbl&gt;, ## # V23539 &lt;dbl&gt;, V19510 &lt;dbl&gt;, V15571 &lt;dbl&gt;, … These are allele frequencies for each site. What is the range of allele frequencies for allele V3386? You might need to write extra code to answer this question. Next we have some housekeeping to define the predictor variables in the regression, which are the environmental variables, and the response variables, which in this case are the frequencies of each variant, analyzed in gradient forest. Now we run gradientForest with 10 bootstrapped trees to be generated by randomForest. This will take about 5 minutes. preds &lt;- colnames(Ewifl) specs &lt;- colnames(Gwifl) nSites &lt;- dim(Gwifl)[1] nSpecs &lt;- dim(Gwifl)[2] # set depth of conditional permutation lev &lt;- floor(log2(nSites*0.368/2)) lev ## [1] 1 wiflforest=gradientForest(cbind(Ewifl,Gwifl), predictor.vars=preds, response.vars=specs, ntree=10, transform = NULL, compact=T,nbin=101, maxLevel=lev,trace=T) ## Calculating forests for 10000 species ## ...................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## .... ## Warning in randomForest.default(x = X, y = spec_vec, ## maxLevel = maxLevel, : The response has five or fewer ## unique values. Are you sure you want to do regression? ## ................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## .................................. ## Warning in randomForest.default(x = X, y = spec_vec, ## maxLevel = maxLevel, : The response has five or fewer ## unique values. Are you sure you want to do regression? ## ..................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## .............. ## Warning in randomForest.default(x = X, y = spec_vec, ## maxLevel = maxLevel, : The response has five or fewer ## unique values. Are you sure you want to do regression? ## ......................................... ## ........................... ## Warning in randomForest.default(x = X, y = spec_vec, ## maxLevel = maxLevel, : The response has five or fewer ## unique values. Are you sure you want to do regression? ## ............................ ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## .............................................. ##ntree=default is 10, more is better and reduces variance ##gradient forest bins variants, large scale patterns will be seen ##if one variant of large effect, may not be found in this analysis Now we can ask which environmental variables are significantly associated with the allelic structure of Willow Flycatchers (or at least the subset we are looking at). This first plot and most useful from gradientForest is the predictor overall importance plot, which shows a simple barplot of the ranked importances of the physical variables. This shows the mean accuracy importance and the mean importance weighted by SNP R^2. The most reliable importances are the R^2 weighted importances. plot(wiflforest,plot.type=&quot;Overall.Importance&quot;) FIGURE 7.2: Overall importance of predictor variables For the Willow Flycatcher subsetted data in this tutorial, what are the top 5 variables? Use the code below to look at Cuumulative importance, aka which variants are strongly associated with the top two variables. The cumulative importance is plotted separately for all species and averaged over all species. The cumulative importance can be interpreted as a mapping from an environmental gradient on to biological gradient. plot(wiflforest,plot.type=&quot;Cumulative.Importance&quot;) FIGURE 7.3: Individual and cumulative importance of SNPs associated with top 2 environmental variables Which SNP has the greatest importance? For plotting purposes, only the first two top environmental variables are plotted. So the plots on the left are variants and turnover functions associated with Bio4, and the plots on the right are associated Bio3. You can confirm this by looking at the range of your top environemntal variables and comparing it to the x-axis in the plots above using this code below: Ewifl %&gt;% dplyr::select(BIO4,BIO3) %&gt;% summarise(rangeBio4=range(BIO4),rangeBio3=range(BIO3)) ## # A tibble: 2 × 2 ## rangeBio4 rangeBio3 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3298 27 ## 2 11946 54 Note, while gradient forest can look at variants associated with environmental variables like this, we tend to choose the top uncorrelated environmental variables and run latent factor mixed models to identify candidate genes associated with each environmental variable. Why? In gradient forest, we “bin” the variation, so we are looking for large patterns, not individual variants. 7.5 Basic Gradient Forest Preparation So now that you see how gradient forest is run, let’s plot the results in space. Gradientforest here was based off of 21 locations in the WIFL range. I’ve randomly chosen 100,000 points within the WIFL range and gotten the evironmental data for these 100,000 sites, which will allow us to have a more continuous view of important environmental associations in WIFL space. Load your 100,000 random points with associated environmental predictors. This can be created with another script that was included with the tutorial material (bioclim&amp;randoms.R), but we’ll use the ready-made grid created specifically for Willow Flycatchers. #WIFL current random points, filter out &#39;bad&#39; points birdgrid&lt;- read_csv(&quot;data/6.0/wiflcurrentRH.csv&quot;) %&gt;% #load your random points with assocaited env predictors filter(BIO4!=&quot;-9999&quot;) %&gt;% #remove the &quot;bad data&quot; as.data.frame() #create the data frame ## Rows: 100000 Columns: 12 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (12): X, Y, BIO2, BIO3, BIO4, BIO5, BIO8, BIO9,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ##Let&#39;s look at bird grid, untransformed environmental variable head(birdgrid) ## X Y BIO2 BIO3 BIO4 BIO5 BIO8 BIO9 BIO11 ## 1 -76.17 44.09 103 26 9755 260 93 -5 -60 ## 2 -103.38 42.94 164 36 9842 320 183 -43 -43 ## 3 -69.11 44.32 113 29 9282 261 28 187 -54 ## 4 -109.13 33.49 197 52 6719 287 175 121 9 ## 5 -106.01 33.40 169 46 7305 314 214 84 37 ## 6 -119.89 48.97 102 32 7206 167 -92 52 -98 ## BIO15 BIO17 BIO18 ## 1 14 201 224 ## 2 64 32 167 ## 3 14 256 256 ## 4 60 47 200 ## 5 65 42 156 ## 6 25 105 117 dim(birdgrid) ## [1] 99541 12 We don’t have to use all the predictors in the next steps, in this case, we’ll use the top 4 uncorrelated variables. These variables were selected by moving down the list of ranked importance for the full model and discarding variables highly correlated (|Pearson’s r| &gt; 0.7) with a variable of higher importance. You can see the correlations among environmental variables across your sample sites. While it is clearly hard to read here, if you export this and open in excel, you can color code and keep track of the correlations among the ranked variables. head(cor(Ewifl,method=&quot;pearson&quot;)) ## BIO1 BIO2 BIO3 BIO4 BIO5 ## BIO1 1.00000 0.06378 0.41991 -0.3227 0.70423 ## BIO2 0.06378 1.00000 0.20607 0.2063 0.51230 ## BIO3 0.41991 0.20607 1.00000 -0.8978 -0.07301 ## BIO4 -0.32265 0.20626 -0.89779 1.0000 0.33424 ## BIO5 0.70423 0.51230 -0.07301 0.3342 1.00000 ## BIO6 0.81738 -0.18653 0.73399 -0.7881 0.22968 ## BIO6 BIO7 BIO8 BIO9 BIO10 BIO11 ## BIO1 0.8174 -0.3214 0.41036 0.6098 0.83681 0.8838 ## BIO2 -0.1865 0.4967 -0.07870 0.2901 0.21572 -0.0240 ## BIO3 0.7340 -0.7323 -0.33935 0.7733 -0.07723 0.7565 ## BIO4 -0.7881 0.9472 0.39863 -0.6709 0.24630 -0.7257 ## BIO5 0.2297 0.4142 0.46587 0.3243 0.92631 0.3598 ## BIO6 1.0000 -0.7907 0.02865 0.7580 0.37954 0.9787 ## BIO12 BIO13 BIO14 BIO15 BIO16 ## BIO1 -0.06788 -0.009306 -0.3315 0.47177 -0.01376 ## BIO2 -0.62811 -0.527137 -0.5750 0.03587 -0.54400 ## BIO3 0.11332 0.283245 -0.4859 0.62916 0.27646 ## BIO4 -0.43212 -0.542562 0.1622 -0.52172 -0.54341 ## BIO5 -0.49614 -0.468213 -0.4053 0.10623 -0.47815 ## BIO6 0.29580 0.381466 -0.2247 0.56762 0.38053 ## BIO17 BIO18 BIO19 NDVI_Mean NDVI_StDev ## BIO1 -0.3092 -0.3012 0.05711 -0.25183 -0.3159 ## BIO2 -0.6153 -0.3973 -0.50485 -0.71683 -0.5122 ## BIO3 -0.4319 -0.6322 0.38306 -0.12471 -0.3679 ## BIO4 0.0850 0.4621 -0.64819 -0.19234 0.1161 ## BIO5 -0.4351 -0.2278 -0.43996 -0.57552 -0.3845 ## BIO6 -0.1567 -0.4205 0.48703 0.03524 -0.1907 ## SRTM TreeCover ## BIO1 -0.55867 -0.22919 ## BIO2 0.62444 -0.31430 ## BIO3 0.04009 0.04665 ## BIO4 0.17262 -0.19766 ## BIO5 -0.19354 -0.41770 ## BIO6 -0.54098 0.01015 For the remaining time in this tutorial we are focusing on the uncorrelated variables determined for the full Willow Flycatcher dataset (Ruegg et al. 2017), BIO11 (Mean temperature of coldest quarter), BIO5 (Max temperature of warmest month), BIO4 (Temperature seasonality (standard deviation *100)), and BIO17 (Precipitation of driest quarter). Predict across your random sites using your Forest model: predictors_uncor&lt;-c(&quot;BIO11&quot;,&quot;BIO5&quot;,&quot;BIO4&quot; ,&quot;BIO17&quot;) currentgrid &lt;-cbind(birdgrid[,c(&quot;X&quot;,&quot;Y&quot;)], predict(wiflforest,birdgrid[,predictors_uncor])) #predict across your random sites using your forest ##Let&#39;s look at currentgrid, transformed environmental variables- see how they were transformed head(currentgrid) ## X Y BIO11 BIO5 BIO4 ## 1 -76.17 44.09 0.0005014 0.0009497 0.020477 ## 2 -103.38 42.94 0.0012774 0.0029031 0.020645 ## 3 -69.11 44.32 0.0005505 0.0009977 0.019145 ## 4 -109.13 33.49 0.0045129 0.0022390 0.003701 ## 5 -106.01 33.40 0.0058957 0.0028932 0.004479 ## 6 -119.89 48.97 -0.0001031 -0.0022453 0.004341 ## BIO17 ## 1 0.0051517 ## 2 0.0007602 ## 3 0.0059949 ## 4 0.0013844 ## 5 0.0012526 ## 6 0.0026725 dim(currentgrid) ## [1] 99541 6 We can summarize the variation using principal components. We then convert those PC axes to RGB values for plotting. #Getting the PCs and Converting Them to RGB Color Values PCs=prcomp(currentgrid[,3:6]) #run your PCs on your top four variables # set up a colour palette for the mapping a1 &lt;- PCs$x[,1] a2 &lt;- PCs$x[,2] a3 &lt;- PCs$x[,3] r &lt;- a1+a2 g &lt;- -a2 b &lt;- a3+a2-a1 r &lt;- (r-min(r)) / (max(r)-min(r)) * 255 g &lt;- (g-min(g)) / (max(g)-min(g)) * 255 b &lt;- (b-min(b)) / (max(b)-min(b)) * 255 wiflcols=rgb(r,g,b,max=255) wiflcols2=col2rgb(wiflcols) wiflcols3=t(wiflcols2) gradients=cbind(currentgrid[c(&quot;X&quot;,&quot;Y&quot;)],wiflcols3) The multi-dimensional biological space can be plotted by taking the principle component of the transformted grid and presenting the first two dimensions in a biplot. You’ve already defined the PCs above and the RGB color palette. Different coordinate positions in the biplot represent differing allele frequency compositions as associated with the predictors. ##Biplot specifics nvs &lt;- dim(PCs$rotation)[1] vec &lt;- c(&quot;BIO11&quot;,&quot;BIO5&quot;,&quot;BIO4&quot; ,&quot;BIO17&quot;) lv &lt;- length(vec) vind &lt;- rownames(PCs$rotation) %in% vec scal &lt;-100 xrng &lt;- range(PCs$x[, 1], PCs$rotation[, 1]/scal) *1.2 yrng &lt;- range(PCs$x[, 2], PCs$rotation[, 2]/scal) *1.2 plot((PCs$x[, 1:2]), xlim = xrng, ylim = yrng, pch = &quot;.&quot;, cex = 4, col = rgb(r, g, b, max = 255), asp = 1) points(PCs$rotation[!vind, 1:2]/scal, pch = &quot;+&quot;) arrows(rep(0, lv), rep(0, lv), PCs$rotation[vec,1]/scal, PCs$rotation[vec, 2]/scal, length = 0.0625) jit &lt;- 0.0015 text(PCs$rotation[vec, 1]/scal + jit * sign(PCs$rotation[vec,1]), PCs$rotation[vec, 2]/scal + jit * sign(PCs$rotation[vec,2]), labels = vec) FIGURE 7.4: A principle component plot of the top 4 uncorrelated variables Save this image using the code chunk below. What environmental variables are associated with the colors green, pink and blue? Knowing this will help with the interpretation of plotting this in space. pdf(&quot;stored_results/6.0/WIFL.PCA_uncorrelated.pdf&quot;) nvs &lt;- dim(PCs$rotation)[1] vec &lt;- c(&quot;BIO11&quot;,&quot;BIO5&quot;,&quot;BIO4&quot; ,&quot;BIO17&quot;) lv &lt;- length(vec) vind &lt;- rownames(PCs$rotation) %in% vec scal &lt;-100 xrng &lt;- range(PCs$x[, 1], PCs$rotation[, 1]/scal) *1.2 yrng &lt;- range(PCs$x[, 2], PCs$rotation[, 2]/scal) *1.2 plot((PCs$x[, 1:2]), xlim = xrng, ylim = yrng, pch = &quot;.&quot;, cex = 4, col = rgb(r, g, b, max = 255), asp = 1) points(PCs$rotation[!vind, 1:2]/scal, pch = &quot;+&quot;) arrows(rep(0, lv), rep(0, lv), PCs$rotation[vec,1]/scal, PCs$rotation[vec, 2]/scal, length = 0.0625) jit &lt;- 0.0015 text(PCs$rotation[vec, 1]/scal + jit * sign(PCs$rotation[vec,1]), PCs$rotation[vec, 2]/scal + jit * sign(PCs$rotation[vec,2]), labels = vec) dev.off() ## quartz_off_screen ## 2 With this, we can draw our first gradient forest map: wiflcols &lt;- rgb(r,g,b,max=255) wiflcols2 &lt;- col2rgb(wiflcols) wiflcols3&lt;- t(wiflcols2) gradients&lt;-cbind(currentgrid[c(&quot;X&quot;,&quot;Y&quot;)],wiflcols3) wiflmap&lt;-gradients coordinates(wiflmap)&lt;- ~X+Y #setting the coordinates proj4string(wiflmap) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) par(mar=c(0,0,0,0)) plot(wiflmap,col=rgb(r,g,b,max=255),pch=15) FIGURE 7.5: Our gradient forest model of SNP variation in the Willow Flycatcher breeding range How would you interpret the different colors on thie WIFL map? Is the endangered southwest Willow Flycatcher locally adapted? Let’s plot this another way, on a map. To do this, we need some additional shapefiles of North America. The domain here allows you to crop the shapefiles to just your species range. library(sf) coastlines &lt;- st_read(&quot;data/6.0/ne_shapefiles/ne_10m_coastline.shp&quot;) ## Reading layer `ne_10m_coastline&#39; from data source ## `/Users/eriq/Documents/git-repos/merida-workshop-2022/data/6.0/ne_shapefiles/ne_10m_coastline.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 4133 features and 3 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -180 ymin: -85.22 xmax: 180 ymax: 83.63 ## Geodetic CRS: WGS 84 domain &lt;- c( xmin = -140, xmax = -55, ymin = 15, ymax = 50 ) coast_cropped &lt;- st_crop(coastlines, domain) ## Warning: attribute variables are assumed to be ## spatially constant throughout all geometries This code allows you to plot, but also change the domain to what will encompass your species shapefile. Suffice it to say I went through a fair number of iterations to ge it right. Once you have the shapefiles and cropped domain, you can create a map, project the map, and then crop it to what region you want. Using the above code, the domain can/should be larger than what you will be plotting. You can further crop later! library(ggspatial) countries_cropped &lt;- st_read(&quot;data/6.0/ne_shapefiles/ne_10m_admin_0_boundary_lines_land.shp&quot;) %&gt;% st_crop(domain) ## Reading layer `ne_10m_admin_0_boundary_lines_land&#39; from data source `/Users/eriq/Documents/git-repos/merida-workshop-2022/data/6.0/ne_shapefiles/ne_10m_admin_0_boundary_lines_land.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 462 features and 18 fields ## Geometry type: MULTILINESTRING ## Dimension: XY ## Bounding box: xmin: -141 ymin: -55.12 xmax: 141 ymax: 70.08 ## Geodetic CRS: WGS 84 ## Warning: attribute variables are assumed to be ## spatially constant throughout all geometries states_cropped &lt;- st_read(&quot;data/6.0/ne_shapefiles/ne_10m_admin_1_states_provinces_lines.shp&quot;) %&gt;% st_crop(domain) ## Reading layer `ne_10m_admin_1_states_provinces_lines&#39; from data source `/Users/eriq/Documents/git-repos/merida-workshop-2022/data/6.0/ne_shapefiles/ne_10m_admin_1_states_provinces_lines.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 10114 features and 21 fields ## Geometry type: MULTILINESTRING ## Dimension: XY ## Bounding box: xmin: -178.1 ymin: -49.25 xmax: 178.4 ymax: 81.13 ## Geodetic CRS: WGS 84 ## Warning: attribute variables are assumed to be ## spatially constant throughout all geometries mapg &lt;- ggplot() + geom_sf(data = coast_cropped) + geom_sf(data = countries_cropped, fill = NA) + geom_sf(data = states_cropped, fill = NA) + ggspatial::layer_spatial(wiflmap,col=rgb(r,g,b,max=255),pch=15) + geom_sf(data=wifl_shape,fill=NA) + theme_bw() mapg FIGURE 7.6: Our gradient forest model of SNP variation in the Willow Flycatcher breeding range on a map Now, let’s see how our developing map looks under such a projection. We just define lamproj as a coordinate reference system and pass it in to coord_sf() lamproj &lt;- &quot;+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-100 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs&quot; #Plotting with projection, and cutting out rectangle (Don&#39;t need a lot of extra space) rectangled &lt;-mapg + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;)+ coord_sf( crs = st_crs(lamproj), xlim = c(-2200476.8, 2859510), ylim = c(-1899797, 1809910.8), expand = FALSE) rectangled Rename this file. I put the plot I plotted in here, but let’s make sure this works for you!! ggsave(&quot;stored_results/6.0/WIFL.GF_map.pdf&quot;) ## Saving 7 x 5 in image 7.6 Defining “Genomic Vulnerability/Genomic offset” Given Future Climate Predictions The plots above are the genotype-environment relationship given present-day climate rasters. However, we know rapid fluctuations in temperature and precipitation associated with climate change can alter the suitability of particular regions making it necessary for individuals to either adapt, disperse or die if the changes are extreme enough. Those species that possess standing genetic variation for climate-related traits (i.e. have adaptive capacity) are most likely to have the ability to adapt to rapidly changing environments. Those species that unable to adapt are deemed most vulneraable. To investigate which populations might be most vulnerable to future climate change, we defined the metric “genomic vulnerability” as the mismatch between current and predicted future genomic variation based on genotype-environment relationships modeled across contemporary populations. We followed the method presented in Fitzpatrick and Keller (2015) to calculate genomic vulnerability using an extension of the gradient forest analysis. Populations with the greatest mismatch are least likely to adapt quickly enough to track future climate shifts, potentially resulting in population declines or extirpations. Here, we read in the future climate predictions for each random point. Since different amounts of heat-trapping gases released into the atmosphere by human activities produce different projected increases in Earth’s temperature, we provided two such predicted models: the 2.6 emission predictions and the 8.5 emission predictions for the year 2050. The lowest recent emissions pathway (RCP), 2.6, assumes immediate and rapid reductions in emissions and would result in about 2.5°F of warming in this century. The highest emissions pathway, RCP 8.5, roughly similar to a continuation of the current path of global emissions increases, is projected to lead to more than 8°F warming by 2100, with a high-end possibility of more than 11°F. Each of these is a composite of multiple prediction models and have rather different predictions. future1&lt;-read_csv(&quot;data/6.0/wiflfuture2-6-2050.csv&quot;) %&gt;% filter(BIO4!=&quot;-9999&quot;) %&gt;% as.data.frame() ## Rows: 100000 Columns: 12 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (12): X, Y, BIO2, BIO3, BIO4, BIO5, BIO8, BIO9,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(future1) ## X Y BIO2 BIO3 BIO4 BIO5 BIO8 BIO9 BIO11 ## 1 -76.17 44.09 102 26 9584 283 65 138 -34 ## 2 -103.38 42.94 166 36 9942 342 171 -15 -24 ## 3 -69.11 44.32 112 29 9209 282 42 173 -31 ## 4 -109.13 33.49 202 51 6941 310 195 133 24 ## 5 -106.01 33.40 174 46 7508 338 235 109 53 ## 6 -119.89 48.97 101 31 7244 192 -70 82 -79 ## BIO15 BIO17 BIO18 ## 1 16 209 221 ## 2 63 35 155 ## 3 17 259 273 ## 4 69 33 202 ## 5 71 30 153 ## 6 33 97 107 future2&lt;-read_csv(&quot;data/6.0/wiflfuture8-5-2050.csv&quot;) %&gt;% filter(BIO4!=&quot;-9999&quot;) %&gt;% as.data.frame() ## Rows: 100000 Columns: 12 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (12): X, Y, BIO2, BIO3, BIO4, BIO5, BIO8, BIO9,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(future2) ## X Y BIO2 BIO3 BIO4 BIO5 BIO8 BIO9 BIO11 ## 1 -76.17 44.09 101 26 9652 299 46 175 -19 ## 2 -103.38 42.94 166 35 10238 359 183 -6 -15 ## 3 -69.11 44.32 111 29 9211 295 45 209 -18 ## 4 -109.13 33.49 203 51 7078 323 208 142 33 ## 5 -106.01 33.40 175 45 7602 351 249 127 63 ## 6 -119.89 48.97 101 30 7521 210 -64 111 -70 ## BIO15 BIO17 BIO18 ## 1 16 215 224 ## 2 62 41 153 ## 3 19 257 269 ## 4 69 30 198 ## 5 75 27 152 ## 6 37 87 90 We then use our gradient forest model to transform these into “biological space” just as we did for random points across contemporary environments. futuregrid=cbind(future1[,c(&quot;X&quot;,&quot;Y&quot;)],predict(wiflforest,future1[,predictors_uncor])) Finally, we calculate genomic vulnerability. Mathematically, for a given point this is the Euclidean distance between the current biological space (currentgrid) and the future biological space (futuregrid): coords&lt;-birdgrid[,c(&quot;X&quot;,&quot;Y&quot;)] euc &lt;- matrix(data=NA,nrow=nrow(futuregrid),ncol=3) for(j in 1:nrow(currentgrid)) { euc[j,] &lt;- c(as.numeric(coords[j,]),as.numeric(dist(rbind(currentgrid[j,],futuregrid[j,])))) } euc &lt;- data.frame(euc) names(euc) &lt;- c(&quot;Long&quot;,&quot;Lat&quot;,&quot;Vulnerability&quot;) This again takes a moment. Once we have this metric, we can easily map genomic vulnerability across the range. First by choosing a color palette, then by converting a dataframe of distances to a spatial polygon and giving it the same extent as the map it’s going to be plotted on and finally, mapping in ggplot. library(scales) colramp2=brewer.pal(n = 9, name = &quot;RdYlBu&quot;)[c(1:9)] Futcolors&lt;-colorRampPalette(colramp2) show_col(Futcolors(100)) FIGURE 7.7: Color Ramp for plotting euc_sp&lt;-st_as_sf(euc,coords=c(&quot;Long&quot;, &quot;Lat&quot;)) st_crs(euc_sp) &lt;- st_crs(coast_cropped) mapg &lt;- ggplot() + geom_sf(data = coast_cropped) + geom_sf(data = countries_cropped, fill = NA) + geom_sf(data = states_cropped, fill = NA) + geom_point(data=euc,aes(x=Long,y=Lat,color=Vulnerability),pch=15,size=1) + scale_colour_gradientn(colours = Futcolors(100))+ geom_sf(data=wifl_shape,fill=NA) + theme_bw() mapg FIGURE 7.8: Heat map of genomic vulnerability under the 2050 2.6 emission future prediction How does this look if we plot the second scenario of higher gas emissions? Compare and contrast the two scenarios, keeping in mind that the colors are scaled within each scenario. futuregrid2=cbind(future2[,c(&quot;X&quot;,&quot;Y&quot;)],predict(wiflforest,future2[,predictors_uncor])) coords&lt;-birdgrid[,c(&quot;X&quot;,&quot;Y&quot;)] euc2 &lt;- matrix(data=NA,nrow=nrow(futuregrid2),ncol=3) for(j in 1:nrow(currentgrid)) { euc2[j,] &lt;- c(as.numeric(coords[j,]),as.numeric(dist(rbind(currentgrid[j,],futuregrid2[j,])))) } euc2 &lt;- data.frame(euc2) names(euc2) &lt;- c(&quot;Long&quot;,&quot;Lat&quot;,&quot;Vulnerability&quot;) euc_sp2&lt;-st_as_sf(euc2,coords=c(&quot;Long&quot;, &quot;Lat&quot;)) st_crs(euc_sp2) &lt;- st_crs(coast_cropped) mapg2 &lt;- ggplot() + geom_sf(data = coast_cropped) + geom_sf(data = countries_cropped, fill = NA) + geom_sf(data = states_cropped, fill = NA) + geom_point(data=euc2,aes(x=Long,y=Lat,color=Vulnerability), pch=15) + scale_colour_gradientn(colours = Futcolors(100))+ geom_sf(data=wifl_shape,fill=NA) + theme_bw() mapg2 FIGURE 7.9: Heat map of genomic vulnerability under the 2050 8.5 emission future prediction "],["conclude.html", "Session 8 Concluding Remarks / Parting Words", " Session 8 Concluding Remarks / Parting Words Something here. Bay RA, Harrigan RJ, Le Underwood V et al. (2018) Genomic signals of selection predict climate-driven population declines in a migratory bird. Science, 359, 83–86. Robertson JM, Murphy MA, Pearl CA et al. (2018) Regional variation in drivers of connectivity for two frog species (rana pretiosa and r. Luteiventris) from the US pacific northwest. Molecular ecology, 27, 3242–3256. Ruegg K, Anderson EC, Somveille M et al. (2021) Linking climate niches across seasons to assess population vulnerability in a migratory bird. Global Change Biology. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
