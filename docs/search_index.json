[["index.html", "Reproducible Research for Conservation — Mérida, Mexico Preface", " Reproducible Research for Conservation — Mérida, Mexico Eric C. Anderson, Christen Bossu, Richard Evan Feldman, Kristen C. Ruegg, Marius Somveille 2021-12-31 Preface This is the site/book associated with a course scheduled for January 2022. More on this later… "],["congen-intro.html", "Session 1 Introduction to Conservation Genomics Approaches", " Session 1 Introduction to Conservation Genomics Approaches Something here. "],["introduction-to-the-tidyverse.html", "Session 2 Introduction to the tidyverse 2.1 What on earth is the tidyerse? 2.2 An example data set 2.3 Reading data in the tidyverse 2.4 A word on tibbles 2.5 Back to “tidying” up our genotype data 2.6 What the heck is that %&gt;% thingie?! 2.7 The ‘dplyr’ package: the heart of the tidyverse 2.8 Modeling within the Tidyverse", " Session 2 Introduction to the tidyverse 2.1 What on earth is the tidyerse? Broadly and briefly, the tidyverse is a collection of packages that are tailored to operating on data that are stored in what is called a “tidy” format. Such a format is one that is typical of the manner in which data are stored in relational data bases. (We will be talking more about “tidy” format throughout this session). There is an R package called ‘tidyverse’ that is simply a wrapper that automatically loads the 8 core packages of the tidyverse. In this session we will talk specifically about functions in 5 of those packages, encountering them in this order: ‘readr’ ‘tibble’ ‘tidyr’ ‘dplyr’ ‘purrr’ The other core packages in the tidyverse are: ‘ggplot2’ — the well known graphics plotting package ‘stringr’ — a package for manipulating string data ‘forcats’ — a package to simplify the use of factors These packages provide a unified way of handling data in a tidy format. And, it turns out, almost any data type can be stored and expressed in a tidy format. This means that 99% of your data analysis tasks can be tackled in a tidy fashion. Some “big-data” data sets (like genome sequencing alignments, or doppler radar data, etc.) come in specialized data formats that allow for fast access and compression, and would not by amenable to storage in a tidy format. However, these are specialized cases, and for almost every data analysis application you will encounter, it behooves you to get comfortable in the tidyverse. At any rate, to have access to all the functions from the 8 core packages of the ‘tidyverse’ we just load the package like this: library(tidyverse) 2.2 An example data set We will start our exploration of the tidyverse with a data set that is decidedly not tidy. It is a small data set of genotypes of 34 fish from four different rivers with code names BALGA, BCHNC, BEMME, and BMTMA. The fish have been genotyped at 5 genetic markers, named mk1, mk2, mk3, mk4, and mk5. Because these fish are diploids they have two copies of each gene, and therefore, their genotype at a single markers consists of the allele or allelic type of the two gene copies at that marker. So, in this data set, for example, the mk1_1 column holds information about the first gene copy at marker mk1 and, mk1_2 holds information about the second gene copy at marker mk1. Different alleles are specified as different positive integer values, (like 228) The data set is in the merida-workshop-2022 Rstudio project, stored as a CSV file at data/tidy-intro/genotypes.csv. It looks like this: river,indiv,mk1_1,mk1_2,mk2_1,mk2_2,mk3_1,mk3_2,mk4_1,mk4_2,mk5_1,mk5_2 BALGA,BALGA_001,311,311,228,228,234,234,184,164,211,211 BALGA,BALGA_002,311,225,228,228,230,226,184,184,NA,NA BALGA,BALGA_006,311,311,228,228,234,234,184,168,219,215 BALGA,BALGA_007,311,311,228,220,230,218,184,164,215,215 BALGA,BALGA_008,311,311,228,228,234,230,168,164,NA,NA BALGA,BALGA_009,315,315,220,220,238,230,184,184,231,207 BALGA,BALGA_012,311,311,228,220,230,230,184,164,227,211 BCHNC,BCHNC_001,311,311,228,228,234,230,184,184,227,211 BCHNC,BCHNC_002,311,259,228,228,238,226,184,184,227,215 BCHNC,BCHNC_003,311,311,228,212,238,226,NA,NA,227,227 BCHNC,BCHNC_004,NA,NA,NA,NA,NA,NA,NA,NA,207,207 BCHNC,BCHNC_005,311,299,232,228,234,234,184,184,219,203 BCHNC,BCHNC_006,NA,NA,228,228,NA,NA,168,168,215,215 BCHNC,BCHNC_007,315,311,228,228,234,234,168,168,215,211 BCHNC,BCHNC_008,NA,NA,228,228,NA,NA,168,168,231,219 BCHNC,BCHNC_009,349,311,236,228,234,214,184,168,219,219 BCHNC,BCHNC_010,311,311,232,228,238,230,184,168,219,215 BEMME,BEMME_001,311,311,228,228,238,226,184,184,NA,NA BEMME,BEMME_002,349,299,232,220,246,234,184,168,211,203 BEMME,BEMME_003,311,311,236,228,238,234,184,168,NA,NA BEMME,BEMME_006,311,311,236,228,238,234,184,168,NA,NA BEMME,BEMME_008,349,349,228,228,234,230,184,184,215,211 BEMME,BEMME_009,311,311,228,220,238,238,184,184,235,203 BEMME,BEMME_010,311,311,228,228,242,234,168,168,223,203 BEMME,BEMME_011,307,307,228,228,238,234,184,168,231,215 BMTMA,BMTMA_001,NA,NA,228,220,234,234,196,168,223,211 BMTMA,BMTMA_003,311,311,228,228,234,230,208,196,223,219 BMTMA,BMTMA_005,311,253,228,228,242,230,NA,NA,211,203 BMTMA,BMTMA_006,311,311,228,228,234,226,196,184,223,211 BMTMA,BMTMA_007,NA,NA,228,220,234,230,196,184,223,223 BMTMA,BMTMA_008,311,311,228,228,242,238,196,188,223,223 BMTMA,BMTMA_009,NA,NA,228,228,234,230,200,184,223,219 BMTMA,BMTMA_010,311,311,232,232,230,230,196,184,223,211 BMTMA,BMTMA_011,NA,NA,228,228,NA,NA,184,168,223,203 2.3 Reading data in the tidyverse The package ‘readr’ has a large number of functions for reading in data of different types. They all start with read_. These functions typically read things into tidyverse-specific data frames that are called “tibbles,” about which we will hear more later. We can read the genotypes.csv file using the readr function, read_csv(). Note: this looks like the base R function read.csv(), but it is not the same!! Most notably, read_csv() will never automatically convert strings to factors (Rejoice!) read_csv() is faster than read.csv() read_csv() is more likely to tell you if there are unexpected irregularities found while reading your data set. It also has facilities to tell you exactly where those data-reading problems occurred. read_csv() will return data in a tibble rather than simply a data frame. (More about this soon!). These characteristics are shared amongst all the read_*() functions (such as read_tsv(), read_table(), read_table2() and read_delim()) in the ‘readr’ package. A delightful overview of the readr package can be found on the “readr Cheatsheet” from RStudio available here or by clicking the thumbnail below. Reading the data set with read_csv() is simple: genos &lt;- read_csv(&quot;data/tidy-intro/genotypes.csv&quot;) And then we can display it like this: genos ## # A tibble: 34 × 12 ## river indiv mk1_1 mk1_2 mk2_1 mk2_2 mk3_1 mk3_2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 311 311 228 228 234 234 ## 2 BALGA BALGA_002 311 225 228 228 230 226 ## 3 BALGA BALGA_006 311 311 228 228 234 234 ## 4 BALGA BALGA_007 311 311 228 220 230 218 ## 5 BALGA BALGA_008 311 311 228 228 234 230 ## 6 BALGA BALGA_009 315 315 220 220 238 230 ## 7 BALGA BALGA_012 311 311 228 220 230 230 ## 8 BCHNC BCHNC_001 311 311 228 228 234 230 ## 9 BCHNC BCHNC_002 311 259 228 228 238 226 ## 10 BCHNC BCHNC_003 311 311 228 212 238 226 ## # … with 24 more rows, and 4 more variables: ## # mk4_1 &lt;dbl&gt;, mk4_2 &lt;dbl&gt;, mk5_1 &lt;dbl&gt;, mk5_2 &lt;dbl&gt; Each row coresponds to one individual, and each column corresponds to one of the two gene copies at a genetic marker. 2.4 A word on tibbles It is worth noting how it is that this tibble, this special type of a data frame is displayed when you print it: The first line, # A tibble: 34 × 12, tells how many rows (34) and columns (12) are in the tibble. By default, only the first ten rows are printed. Beneath the name of each column, the type of data in the column is listed. In this case, &lt;dbl&gt; means numeric data and &lt;chr&gt; means the column is a vector of character (string) data (i.e. text). Other possibilities are &lt;int&gt; for integer, &lt;fct&gt; for factor, and &lt;list&gt; for a column that is a list (Richard will talk more about that!) If there are too many columns for the width of the screen, only the contents of the first few columns are printed. The remaining columns are just shown beneath by their names and types. If there a a very large number of columns, the names and types of some of the remaining ones are not shown. In this way, you can quickly get a sense for how a data set is structured by simply printing it to the screen without running the risk of bombing your whole R console by having it print all the lines of a very large data frame! While the readr package always returns a tibble, the as_tibble() function from the ‘tibble’ package provides an easy way to turn an existing data frame into a tibble. For example, to witness how much easier it is to see what is contained in the built-in R data set, iris, by converting it into a tibble, the reader is invited to evaluate the following two lines in their R console and compare the output: iris # the whole data set gets spit out and you end up at bottom of it as_tibble(iris) # a nice tibble summary gets printed Finally, the ‘tibble’ package provides the function tibble() that allows the user to create a tibble from vectors. It is much like the built-in R function, data.frame() except that it deals appropriately with list vectors. For more about tibbles, the best place if you are new is the tibble chapter, in the R For Data Science book. 2.5 Back to “tidying” up our genotype data Our small data set genos might seem pretty neat and tidy—it is easy to look at as a human, and it would not take a whole lot of space to print it out on a page. However, in the current format, doing a number of summaries of the data would require somewhat specialized code. For example, if you wanted to count up the number of occurrences of each allelic type (i.e. each number, like 311) at each of the markers, within each of the rivers, it would not be straightforward! Just imagine counting alleles at markers mk_1: the alleles are in two different columns, and you would have to account for fish being from different rivers. While one could certainly write base R code to traverse the data frame and pick out the appropriate columns and rows to make these summaries, the tidyverse approach is to recognize that converting the data to a different format (often called a “long” format), will make it easier to do most any operation on the data. The main problem of the current data set, genos, is that the observed values at all the markers are the same—they are all just different alleles; however, they occupy lots of different columns. A general principle in tidying data is to strive for having only a single column to represent a certain type of observation. In the case of genos, the column headers mk1_1, mk1_2, ... , mk_5_2 tell us which particular markers and gene copies have a particular allelic type. But another way to represent that is to have a column that gives the marker and the gene copy. Perhaps it is best to simply show what a tidy, long version of these data look like. The core tidyverse package ‘tidyr’ provides many functions for tidying data sets. We will use the pivot_longer() function to turn the column names into data values in new columns: genos_long &lt;- genos %&gt;% pivot_longer( cols = c(mk1_1:mk5_2), names_to = c(&quot;marker&quot;, &quot;gene_copy&quot;), names_sep = &quot;_&quot;, values_to = &quot;allele&quot; ) genos_long ## # A tibble: 340 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk1 1 311 ## 2 BALGA BALGA_001 mk1 2 311 ## 3 BALGA BALGA_001 mk2 1 228 ## 4 BALGA BALGA_001 mk2 2 228 ## 5 BALGA BALGA_001 mk3 1 234 ## 6 BALGA BALGA_001 mk3 2 234 ## 7 BALGA BALGA_001 mk4 1 184 ## 8 BALGA BALGA_001 mk4 2 164 ## 9 BALGA BALGA_001 mk5 1 211 ## 10 BALGA BALGA_001 mk5 2 211 ## # … with 330 more rows Wow, that is a wildly different format! We now only have 5 columns: river: tells us what river the fish is from indiv: the ID of the individual fish marker: the name of the marker (i.e. mk_1, or mk_2, etc.) gene_copy: whether we are looking at the first (1) or the second (2) gene copy of the fish. allele: the allelic type of the gene_copy at the marker in the individual fish from the particular river. Though our data set has only 5 columns, it now has 340 rows. As a consequence, it is not really possible to “look at” a large portion of the data set as a human (as was possible before). Furthermore, if you don’t know how to handle data like these it can seem daunting. But, once you learn the tools of the tidyverse (and particularly of the package ‘dplyr’) this format lets you learn the same, simple approach that will work for many, many different data sets. The ‘tidyr’ package has a number of functions for tidying data. Check out the RStudio tidyr cheatsheet for a great overview of ‘tidyr’ features. 2.6 What the heck is that %&gt;% thingie?! Before we proceed, we want to make sure that everyone understands what the %&gt;% in the above code block is doing. The packages of the tidyverse all import the R symbol %&gt;% from the ‘magrittr’ package. This is referred to as the tidyverse “pipe” because it allows the user to “pipe” the output of one function into the input for another function (in the same way that experience Unix computer users will “pipe” output from one command into the input for another command). In short, the %&gt;% symbol takes whatever is on the left of it and it supplies that as the first argument of the function that is to the right of it. The thing on the left can be an R object or the output of a functio. For a simple, contrived example, suppose you wanted to add the integers from 1 to 10, take the square root of the result, and then calculate the sin of that result, and then take the log of it. Without piping capability, you could write that operation in a horribly nested set of parentheses: log(sin(sqrt(sum(1:10)))) ## [1] -0.09905 which is hard to read because the direction of operations runs right to left. It is much easier to read and understand what is going on by piping results from one function to the next: 1:10 %&gt;% sum() %&gt;% sqrt() %&gt;% sin() %&gt;% log() ## [1] -0.09905 Operating upon tibbles with the pipe let’s you chain multiple different functions/operations upon the data in an easy-to-read fashion. And, it let’s you check intermediate results along the way as you build up more complex expressions. It is a crucial capability in the tidyverse. We will see the tidyverse pipe used extensively in the following. 2.7 The ‘dplyr’ package: the heart of the tidyverse ‘dplyr’ is the tidyverse package that offers the most functionality for operating on data sets. The main ‘dplyr’ functions for performing actions on a tibble are described as “verbs”: and their names are meant to describe the actions they do. There are a lot of other functions within ‘dplyr.’ We can’t cover them all here, but, once again, the RStudio dplyr cheatsheet is the perfect place for an overview. Here, we will cover just the main verbs, namely: select(), filter(), arrange() change the extent of columns, or rows, or the sort order of rows, respectively. mutate() allows for the creation of new columns that are functions of existing columns (and have the same length as the existing columns). summarise() allows for summarising existing columns down to a single value. group_by() allows the ‘dplyr’ verbs to operate on multiple subsets of data defined by the values of grouping variables. I dare say this is the most important concept in all the tidyverse. left_join() the simplest of a whole family of joins (left_join(), full_join(), inner_join(), etc.), that combine rows of two tibbles together according to shared values in certain columns. 2.7.1 Non-standard Evaluation (NSE): You don’t wrap column names in quotation marks Importantly, in the tidyverse, when you operate on a column of a tibble, it is customary to refer to that column by its name, rather than by the number that describes its position (this makes the code more resilient to things like ordering of columns in a tibble). Within the tidyverse, names of columns are given as if they were variable names—they should almost never be wrapped in quotation marks. 2.7.2 select() to select columns of a tibble The select function is used to choose which columns of a tibble are returned. They get returned in a tibble of their own. Examples: # return a tibble with just the river and indiv of # the original genos tibble genos %&gt;% select(river, indiv) ## # A tibble: 34 × 2 ## river indiv ## &lt;chr&gt; &lt;chr&gt; ## 1 BALGA BALGA_001 ## 2 BALGA BALGA_002 ## 3 BALGA BALGA_006 ## 4 BALGA BALGA_007 ## 5 BALGA BALGA_008 ## 6 BALGA BALGA_009 ## 7 BALGA BALGA_012 ## 8 BCHNC BCHNC_001 ## 9 BCHNC BCHNC_002 ## 10 BCHNC BCHNC_003 ## # … with 24 more rows Ranges of columns can be selected by putting a colon between the name of the column at the start of the range and the name of the column at the end of the range. Furthermore, adding a - to a column name in select() will remove it. And adding a - before a column name range (surrounded by parentheses) will remove all those columns. For example: # remove the columns mk1_1 to mk3_2 from the original # genos tibble genos %&gt;% select(-(mk1_1:mk3_2)) ## # A tibble: 34 × 6 ## river indiv mk4_1 mk4_2 mk5_1 mk5_2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 184 164 211 211 ## 2 BALGA BALGA_002 184 184 NA NA ## 3 BALGA BALGA_006 184 168 219 215 ## 4 BALGA BALGA_007 184 164 215 215 ## 5 BALGA BALGA_008 168 164 NA NA ## 6 BALGA BALGA_009 184 184 231 207 ## 7 BALGA BALGA_012 184 164 227 211 ## 8 BCHNC BCHNC_001 184 184 227 211 ## 9 BCHNC BCHNC_002 184 184 227 215 ## 10 BCHNC BCHNC_003 NA NA 227 227 ## # … with 24 more rows 2.7.3 filter() to retain certain rows The filter() function takes any vectorized logical expression involving columns of the tibble. Rows for which that expression evaluate to TRUE are retained, and those that evaluate to FALSE are not. Examples: # only keep rows with river code &quot;BALGA&quot; genos_long %&gt;% filter(river == &quot;BALGA&quot;) ## # A tibble: 70 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk1 1 311 ## 2 BALGA BALGA_001 mk1 2 311 ## 3 BALGA BALGA_001 mk2 1 228 ## 4 BALGA BALGA_001 mk2 2 228 ## 5 BALGA BALGA_001 mk3 1 234 ## 6 BALGA BALGA_001 mk3 2 234 ## 7 BALGA BALGA_001 mk4 1 184 ## 8 BALGA BALGA_001 mk4 2 164 ## 9 BALGA BALGA_001 mk5 1 211 ## 10 BALGA BALGA_001 mk5 2 211 ## # … with 60 more rows # only keep data from markers mk3 and mk4 genos_long %&gt;% filter(marker %in% c(&quot;mk3&quot;, &quot;mk4&quot;)) ## # A tibble: 136 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk3 1 234 ## 2 BALGA BALGA_001 mk3 2 234 ## 3 BALGA BALGA_001 mk4 1 184 ## 4 BALGA BALGA_001 mk4 2 164 ## 5 BALGA BALGA_002 mk3 1 230 ## 6 BALGA BALGA_002 mk3 2 226 ## 7 BALGA BALGA_002 mk4 1 184 ## 8 BALGA BALGA_002 mk4 2 184 ## 9 BALGA BALGA_006 mk3 1 234 ## 10 BALGA BALGA_006 mk3 2 234 ## # … with 126 more rows # remove rows for which allele is NA genos_long %&gt;% filter(!is.na(allele)) ## # A tibble: 300 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk1 1 311 ## 2 BALGA BALGA_001 mk1 2 311 ## 3 BALGA BALGA_001 mk2 1 228 ## 4 BALGA BALGA_001 mk2 2 228 ## 5 BALGA BALGA_001 mk3 1 234 ## 6 BALGA BALGA_001 mk3 2 234 ## 7 BALGA BALGA_001 mk4 1 184 ## 8 BALGA BALGA_001 mk4 2 164 ## 9 BALGA BALGA_001 mk5 1 211 ## 10 BALGA BALGA_001 mk5 2 211 ## # … with 290 more rows 2.7.4 arrange() to reorder the rows of the tibble For example, perhaps you wish to see which of the alleles have the highest numbers. Then: genos_long %&gt;% arrange(desc(allele)) ## # A tibble: 340 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BCHNC BCHNC_009 mk1 1 349 ## 2 BEMME BEMME_002 mk1 1 349 ## 3 BEMME BEMME_008 mk1 1 349 ## 4 BEMME BEMME_008 mk1 2 349 ## 5 BALGA BALGA_009 mk1 1 315 ## 6 BALGA BALGA_009 mk1 2 315 ## 7 BCHNC BCHNC_007 mk1 1 315 ## 8 BALGA BALGA_001 mk1 1 311 ## 9 BALGA BALGA_001 mk1 2 311 ## 10 BALGA BALGA_002 mk1 1 311 ## # … with 330 more rows The ‘dplyr’ function desc() means: arrange rows in descending order of the variable named within it. You can arrange/sort rows according to multiple columns, as well. For example to sort things that show individual genotypes in adjacent rows, but with rivers in reverse alphabetical order, we could do: genos_long %&gt;% arrange(desc(river), indiv, marker, gene_copy) ## # A tibble: 340 × 5 ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BMTMA BMTMA_001 mk1 1 NA ## 2 BMTMA BMTMA_001 mk1 2 NA ## 3 BMTMA BMTMA_001 mk2 1 228 ## 4 BMTMA BMTMA_001 mk2 2 220 ## 5 BMTMA BMTMA_001 mk3 1 234 ## 6 BMTMA BMTMA_001 mk3 2 234 ## 7 BMTMA BMTMA_001 mk4 1 196 ## 8 BMTMA BMTMA_001 mk4 2 168 ## 9 BMTMA BMTMA_001 mk5 1 223 ## 10 BMTMA BMTMA_001 mk5 2 211 ## # … with 330 more rows 2.7.5 mutate() to create new columns With the mutate() function, it is easy to create a new column that is a function of other columns. The syntax is like: # create a new column named new_column that is a function # or existing columns col1, col2 genos_long %&gt;% mutate(new_column_name = some_function(col1, col2)) In this capacity, ‘dplyr’ is very particular: If the function returns the same number of rows as the tibble, it works. If the function returns a single value, that single value is replicated in each row of the new column. Otherwise, mutate() throws an error. Simple examples. Try these on your own: # make a column, &quot;long_marker_names&quot; that give # a long name in the form &quot;Marker X&quot; for each marker genos_long %&gt;% mutate(long_marker_names = str_replace(marker, &quot;mk&quot;, &quot;Marker &quot;)) # make a new column called gc_ab that gives the gene copies # as &quot;a&quot; and &quot;b&quot;, rather than &quot;1&quot; and &quot;2&quot; genos_long %&gt;% mutate( gc_ab = case_when( gene_copy == 1 ~ &quot;a&quot;, gene_copy == 2 ~ &quot;b&quot;, TRUE ~ NA_character_ ) ) # make a new column called season that tells us that # all of these data were collected in the &quot;Autumn&quot; genos_long %&gt;% mutate(season = &quot;Autumn&quot;) 2.7.6 Compute a single value from many rows: summarise() While mutate() always returns a new column that is the same length as all the other columns in the tibble, summarise() does something quite different: it allows you to make a column that will have only a single row in it. We will see more examples later on, but for now, consider one of the simplest ways to summarise a tibble: count how many rows are in it. That is done with the ‘dplyr’ function n(): genos_long %&gt;% summarise(num_rows = n()) ## # A tibble: 1 × 1 ## num_rows ## &lt;int&gt; ## 1 340 See how that summarise() function takes an entire tibble, and returns a single value (the number of rows of the original tibble). This behavior might not seem all that useful, but as we will see, when combined with grouping it is extraordinarily useful. 2.7.7 group_by() to operate tidily on subsets of a tibble One of the most important aspects of working with tibbles is that of grouping them by variables. In fact, without grouping the “tidy” or “long” format would be nearly impossible to deal with; however with grouping, a huge number of problems can be solved upon tidy data within the tidyverse. Much of the time, we want to apply functions to separate parts of the data set, each part in turn. An example might help: suppose that we want to compute the allele frequencies from genos_long. In other words, we want to count how many times within each river, each allele, at each locus is observed. Each time we say something like at each locus, or within each river, we are implicitly talking about grouping our data. Thus, if we “group our data by river,” we are conceptually, breaking our data set into four distinct tibbles: one with all the rows in which river == \"BALGA\" one with all the rows in which river == \"BCHNC\" one with all the rows in which river == \"BEMME\" one with all the rows in which river == \"BMTMA\" Then (and this is the really important part!) if we pass such a “grouped” tibble to mutate() or summarise(), then those functions will operate on each of those four separate groups, independently, but it will still return the result as a single tibble. To group a tibble, we use the group_by() function. Here we group genos_long by river: genos_long %&gt;% group_by(river) ## # A tibble: 340 × 5 ## # Groups: river [4] ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk1 1 311 ## 2 BALGA BALGA_001 mk1 2 311 ## 3 BALGA BALGA_001 mk2 1 228 ## 4 BALGA BALGA_001 mk2 2 228 ## 5 BALGA BALGA_001 mk3 1 234 ## 6 BALGA BALGA_001 mk3 2 234 ## 7 BALGA BALGA_001 mk4 1 184 ## 8 BALGA BALGA_001 mk4 2 164 ## 9 BALGA BALGA_001 mk5 1 211 ## 10 BALGA BALGA_001 mk5 2 211 ## # … with 330 more rows From the output that gets printed out, it looks like almost nothing has changed about the tibble, except there is this extra line: # Groups: river [4] This tells us that after running genos_long through group_by(river), the result is a tibble that is grouped by the river column, and there are 4 groups. But, now, if we count the number of rows in each of those groups we start to see how powerful this approach can be: genos_long %&gt;% group_by(river) %&gt;% summarise(num_rows = n()) ## # A tibble: 4 × 2 ## river num_rows ## &lt;chr&gt; &lt;int&gt; ## 1 BALGA 70 ## 2 BCHNC 100 ## 3 BEMME 80 ## 4 BMTMA 90 OK! That is telling us about the sizes of those 4 groups, and we can conceptually think of each group as being a “sub-tibble” with the number of rows as listed above. Note that you can group tibbles by more than one column. When grouped on multiple columns, the groups are formed from all the observed combinations of values in the different columns. For example: genos_long %&gt;% group_by(river, marker, allele) ## # A tibble: 340 × 5 ## # Groups: river, marker, allele [93] ## river indiv marker gene_copy allele ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BALGA BALGA_001 mk1 1 311 ## 2 BALGA BALGA_001 mk1 2 311 ## 3 BALGA BALGA_001 mk2 1 228 ## 4 BALGA BALGA_001 mk2 2 228 ## 5 BALGA BALGA_001 mk3 1 234 ## 6 BALGA BALGA_001 mk3 2 234 ## 7 BALGA BALGA_001 mk4 1 184 ## 8 BALGA BALGA_001 mk4 2 164 ## 9 BALGA BALGA_001 mk5 1 211 ## 10 BALGA BALGA_001 mk5 2 211 ## # … with 330 more rows apparently groups the data into 93 different groups. Now, we are ready to try computing allele frequencies (i.e. counting alleles) at different markers. Remember, we want to count how many times within each river and at each locus that each allele is observed. (Note we also don’t want to count missing data for this so we can filter it out in the beginning). Hence: genos_long %&gt;% filter(!is.na(allele)) %&gt;% group_by(river, marker, allele) %&gt;% summarise(num_alleles = n()) ## `summarise()` has grouped output by &#39;river&#39;, &#39;marker&#39;. You can override using the `.groups` argument. ## # A tibble: 84 × 4 ## # Groups: river, marker [20] ## river marker allele num_alleles ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 BALGA mk1 225 1 ## 2 BALGA mk1 311 11 ## 3 BALGA mk1 315 2 ## 4 BALGA mk2 220 4 ## 5 BALGA mk2 228 10 ## 6 BALGA mk3 218 1 ## 7 BALGA mk3 226 1 ## 8 BALGA mk3 230 6 ## 9 BALGA mk3 234 5 ## 10 BALGA mk3 238 1 ## # … with 74 more rows Wow! That is fast and easy. It is relatively straightforward to think about how summarise() works on grouped tibbles, but it is just as important to know that mutate() also operates on the separate groups of a grouped tibble. Note, that the result above has the notation: # Groups: river, marker [20] So, the result is still a grouped tibble, but in this case it is grouped by river, marker. We had originally grouped it by river, marker, allele, BUT by default, the summarise() function will return a tibble in which the last grouping variable (in this case allele) is no longer in effect. In our case, this is convenient, because such a grouping is good for computing relative frequencies of the alleles. We can just add that on to the chain with the pipe. (And while we are at it, sort things by allele frequency.) genos_long %&gt;% filter(!is.na(allele)) %&gt;% group_by(river, marker, allele) %&gt;% summarise(num_alleles = n()) %&gt;% mutate(freq = num_alleles / sum(num_alleles)) %&gt;% ungroup() %&gt;% arrange(river, marker, desc(freq)) ## `summarise()` has grouped output by &#39;river&#39;, &#39;marker&#39;. You can override using the `.groups` argument. ## # A tibble: 84 × 5 ## river marker allele num_alleles freq ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 BALGA mk1 311 11 0.786 ## 2 BALGA mk1 315 2 0.143 ## 3 BALGA mk1 225 1 0.0714 ## 4 BALGA mk2 228 10 0.714 ## 5 BALGA mk2 220 4 0.286 ## 6 BALGA mk3 230 6 0.429 ## 7 BALGA mk3 234 5 0.357 ## 8 BALGA mk3 218 1 0.0714 ## 9 BALGA mk3 226 1 0.0714 ## 10 BALGA mk3 238 1 0.0714 ## # … with 74 more rows A few things are worth noting: sum(num_allele) sums the number of alleles in each river at each marker (because that is how the tibble is grouped). Hence, num_allele / sum(num_allele) gives the relative frequency of each allele (in the river at the marker). ungroup() removes any grouping criteria from a tibble. It is handy if you don’t want to have any groups in your tibble any longer. The arrange function lets us sort the rows in a useful fashion. 2.7.7.1 Exercises for you The reason that the tidy format is wonderful is because you can get a number of different results by following the same principles, but grouping things differently. Try using the tools of ‘dplyr’ to compute the following. A skeleton has been provided. Put the missing code into the ... in each. A tibble with the relative frequency of missing data within each river genos_long %&gt;% group_by(...) %&gt;% summarise(fract_miss = sum(is.na(allele) / n()) A tibble with the relative frequency of missing data at each marker genos_long %&gt;% group_by(...) %&gt;% summarise(fract_miss = ...) 2.7.8 Joins Joins are what you use to add columns from one tibble to another tibble by matching the values in one or more shared columns. We will just show the simplest join, the left_join(). Suppose that we have data about each fish, that tells us: whether they were caught in the autumn or the spring whether they were a juvenile or an adult the length in centimeters. We have an example of such a data set that we can read in: fish_meta &lt;- read_csv(&quot;data/tidy-intro/fish-metadata.csv&quot;) fish_meta ## # A tibble: 1,252 × 4 ## indiv season stage length ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BEMME_001 autumn juvenile 12.9 ## 2 BEMME_002 spring juvenile 6.7 ## 3 BEMME_003 spring adult 23 ## 4 BEMME_004 autumn adult 25.5 ## 5 BEMME_005 autumn juvenile 10 ## 6 BEMME_006 spring juvenile 11.3 ## 7 BEMME_008 spring juvenile 10 ## 8 BEMME_009 spring juvenile 10.2 ## 9 BEMME_010 autumn juvenile 9.3 ## 10 BEMME_011 spring adult 21 ## # … with 1,242 more rows OK! This has season, stage, and length for 1,252 fish. That is a lot more than the 34 fish we have genotype data for. So, how can we pick out the relevant rows from fish_meta to join them on to the columns of genos_long? We use left_join(). Note that in both genos_long and in fish_meta, the individual ID is in the column indiv. This means that we can join the two tibbles by the indiv column in each tibble. Thus: genos_long2 &lt;- genos_long %&gt;% left_join(fish_meta, by = &quot;indiv&quot;) genos_long2 ## # A tibble: 340 × 8 ## river indiv marker gene_copy allele season stage ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 BALGA BALGA_001 mk1 1 311 spring juve… ## 2 BALGA BALGA_001 mk1 2 311 spring juve… ## 3 BALGA BALGA_001 mk2 1 228 spring juve… ## 4 BALGA BALGA_001 mk2 2 228 spring juve… ## 5 BALGA BALGA_001 mk3 1 234 spring juve… ## 6 BALGA BALGA_001 mk3 2 234 spring juve… ## 7 BALGA BALGA_001 mk4 1 184 spring juve… ## 8 BALGA BALGA_001 mk4 2 164 spring juve… ## 9 BALGA BALGA_001 mk5 1 211 spring juve… ## 10 BALGA BALGA_001 mk5 2 211 spring juve… ## # … with 330 more rows, and 1 more variable: ## # length &lt;dbl&gt; Now, you could do all sorts of things like explore allele frequency differences in different seasons or from different life stages. 2.7.9 Wrap Up We have just scratched the surface of what is possible with the tidyverse. One thing we haven’t mentioned much is that the columns of a tibble can, themselves, be list-vectors. This can be very powerful when doing statistical modeling, as the outputs of different models can be stored in lists that are themselves columns of tibbles. Manipulating such lists is done with the purrr package, which is part of the tidyverse. Richard will give us an example of that for the rest of the session. 2.8 Modeling within the Tidyverse Load required packages library(viridis) # preferred color palette for graphing ## Loading required package: viridisLite Import bird data. The data provides information on the species richness of birds and butterflies and NDVI at 30 sites. Our goal is to run a linear model that estimates the magnitude, direction, and uncertainty in the species richness-NDVI relationship for each taxa and then visualize the relationship. data_all &lt;- read_csv(&quot;data/data.richard.new.csv&quot;) ## New names: ## * `` -&gt; ...1 ## Rows: 60 Columns: 5 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Site, Taxa ## dbl (3): ...1, NDVI, Richness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. data_all ## # A tibble: 60 × 5 ## ...1 Site NDVI Taxa Richness ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Site.1 0.66 Birds 14 ## 2 2 Site.2 0.4 Birds 10 ## 3 3 Site.3 0.36 Birds 9 ## 4 4 Site.4 0.82 Birds 16 ## 5 5 Site.5 0.55 Birds 12 ## 6 6 Site.6 0.29 Birds 7 ## 7 7 Site.7 0.51 Birds 11 ## 8 8 Site.8 0.74 Birds 16 ## 9 9 Site.9 1 Birds 25 ## 10 10 Site.10 0.76 Birds 16 ## # … with 50 more rows The old way is to run a model on each taxa separately: # subset the Birds data model_birds &lt;- lm(Richness ~ NDVI, data = data_all[data_all$Taxa == &quot;Birds&quot;, ]) # subset the Butterflies model_butterflies &lt;- lm(Richness ~ NDVI, data = data_all[data_all$Taxa == &quot;Butterflies&quot;, ]) Extracting results from the summary file is not easy. # You have to remember that subscript [[4]] corresponds to the main results key_results_birds &lt;- as.data.frame(summary(model_birds)[[4]]) The broom package presents model results in a tidy way. library(broom) broom::tidy() returns parameter estimates. birds_tidy &lt;- model_birds %&gt;% tidy() birds_tidy ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 0.952 1.22 0.780 4.42e- 1 ## 2 NDVI 19.9 1.77 11.2 6.75e-12 broom::glance() includes R2, AIC, and more. birds_glance &lt;- model_birds %&gt;% glance() birds_glance ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.819 0.812 1.70 127. 6.75e-12 ## # … with 7 more variables: df &lt;dbl&gt;, logLik &lt;dbl&gt;, ## # AIC &lt;dbl&gt;, BIC &lt;dbl&gt;, deviance &lt;dbl&gt;, ## # df.residual &lt;int&gt;, nobs &lt;int&gt; broom::augment() returns fitted values birds_augment &lt;- model_birds %&gt;% augment() birds_augment ## # A tibble: 30 × 8 ## Richness NDVI .fitted .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 14 0.66 14.1 -0.0743 0.0334 1.73 3.41e-5 ## 2 10 0.4 8.91 1.09 0.111 1.72 2.90e-2 ## 3 9 0.36 8.11 0.890 0.136 1.72 2.49e-2 ## 4 16 0.82 17.3 -1.26 0.0583 1.71 1.79e-2 ## 5 12 0.55 11.9 0.113 0.0484 1.73 1.17e-4 ## 6 7 0.29 6.72 0.282 0.188 1.73 3.90e-3 ## 7 11 0.51 11.1 -0.0920 0.0603 1.73 9.98e-5 ## 8 16 0.74 15.7 0.335 0.0389 1.73 8.18e-4 ## 9 25 1 20.8 4.17 0.152 1.50 6.35e-1 ## 10 16 0.76 16.1 -0.0625 0.0425 1.73 3.12e-5 ## # … with 20 more rows, and 1 more variable: ## # .std.resid &lt;dbl&gt; To run models or implement any function on the different groups within a data frame, one must master tidyr::nest() and purrr::map(). nest() creates a list of data frames with each corresponding to a different group. Example: create a data frame for each taxa. data_taxa &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() data_taxa ## # A tibble: 2 × 2 ## # Groups: Taxa [2] ## Taxa data ## &lt;chr&gt; &lt;list&gt; ## 1 Birds &lt;tibble [30 × 4]&gt; ## 2 Butterflies &lt;tibble [30 × 4]&gt; The grouping variable is left out of the nested data frame. All other data is now in a separate data frame, nested with the whole data frame. You can inspect what lies behind the “data” column by using unnest() to display the original data. data_original_birds &lt;- data_taxa %&gt;% ungroup() %&gt;% slice(n = 1) %&gt;% unnest(data) # slice(n = 1) means extract the first row of the data_taxa data frame data_original_birds ## # A tibble: 30 × 5 ## Taxa ...1 Site NDVI Richness ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Birds 1 Site.1 0.66 14 ## 2 Birds 2 Site.2 0.4 10 ## 3 Birds 3 Site.3 0.36 9 ## 4 Birds 4 Site.4 0.82 16 ## 5 Birds 5 Site.5 0.55 12 ## 6 Birds 6 Site.6 0.29 7 ## 7 Birds 7 Site.7 0.51 11 ## 8 Birds 8 Site.8 0.74 16 ## 9 Birds 9 Site.9 1 25 ## 10 Birds 10 Site.10 0.76 16 ## # … with 20 more rows map() applies a function to each nested data frame. Example: find the mean number of species of each taxa mean_taxa &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(mean_richness = map(data, function(x) { mean(x %&gt;% pull(Richness)) } )) # map indicates the data frame column with the nested data (i.e., data) and applies a function to that data. # data becomes the &quot;x&quot; in the function. # pull() extracts the variable of interest and returns a vector. mean_taxa ## # A tibble: 2 × 3 ## # Groups: Taxa [2] ## Taxa data mean_richness ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; ## 1 Birds &lt;tibble [30 × 4]&gt; &lt;dbl [1]&gt; ## 2 Butterflies &lt;tibble [30 × 4]&gt; &lt;dbl [1]&gt; The results are mutated as another list column in the original data frame. They can be unnested. mean_taxa &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(mean_richness = map(data, function(x) { mean(x %&gt;% pull(Richness)) } )) %&gt;% unnest(mean_richness) mean_taxa ## # A tibble: 2 × 3 ## # Groups: Taxa [2] ## Taxa data mean_richness ## &lt;chr&gt; &lt;list&gt; &lt;dbl&gt; ## 1 Birds &lt;tibble [30 × 4]&gt; 14.2 ## 2 Butterflies &lt;tibble [30 × 4]&gt; 10.7 More complicated functions are easier to specify outside of the pipe. Example: find the sites with the highest and lowest bird and butterfly species richness # Each element of the list column data will correspond to data frame &quot;x&quot; in the function. get_sites &lt;- function(x) { x %&gt;% filter(Richness %in% c(max(Richness), min(Richness))) %&gt;% mutate(Type = ifelse(Richness == max(Richness), &quot;High&quot;, &quot;Low&quot;)) %&gt;% arrange(desc(Richness)) } # The function get.sites is applied to each element in the list column data. sites_taxa &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(Types = map(data, get_sites)) %&gt;% unnest(Types) %&gt;% select(-data) sites_taxa ## # A tibble: 4 × 6 ## # Groups: Taxa [2] ## Taxa ...1 Site NDVI Richness Type ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Birds 9 Site.9 1 25 High ## 2 Birds 6 Site.6 0.29 7 Low ## 3 Butterflies 48 Site.18 0.81 20 High ## 4 Butterflies 32 Site.2 0.4 6 Low Another example, this time nesting the data based on sites and finding whether a site has more bird or butterfly species. get_taxa &lt;- function(x) { Top_taxa&lt;- x %&gt;% filter(Richness == max(Richness)) %&gt;% pull(Taxa) # Need to indicate sites where birds and butterflies have the same species richness if(length(Top_taxa) &gt; 1) { paste(Top_taxa[1], Top_taxa[2], sep=&quot; + &quot;) } else { Top_taxa } } sites_richness &lt;- data_all %&gt;% group_by(Site) %&gt;% nest() %&gt;% mutate(Animal = map(data, get_taxa)) %&gt;% unnest(Animal) %&gt;% select(-data) sites_richness ## # A tibble: 30 × 2 ## # Groups: Site [30] ## Site Animal ## &lt;chr&gt; &lt;chr&gt; ## 1 Site.1 Birds ## 2 Site.2 Birds ## 3 Site.3 Birds ## 4 Site.4 Birds ## 5 Site.5 Birds ## 6 Site.6 Butterflies ## 7 Site.7 Birds ## 8 Site.8 Birds ## 9 Site.9 Birds ## 10 Site.10 Birds ## # … with 20 more rows Are there any sites where birds and butterflies have equal species richness? site_animals &lt;- sites_richness %&gt;% filter(Animal == &quot;Birds + Butterflies&quot;) site_animals ## # A tibble: 1 × 2 ## # Groups: Site [1] ## Site Animal ## &lt;chr&gt; &lt;chr&gt; ## 1 Site.25 Birds + Butterflies Are there more sites with higher bird or butterfly species richness? richness_summary &lt;- sites_richness %&gt;% group_by(Animal) %&gt;% summarize(No_Sites = n()) richness_summary ## # A tibble: 3 × 2 ## Animal No_Sites ## &lt;chr&gt; &lt;int&gt; ## 1 Birds 26 ## 2 Birds + Butterflies 1 ## 3 Butterflies 3 Run a linear model to estimate the relationship between NDVI and richness for each taxa. “lm” can be specified as a function in map(). The model is mutated as a list column. We can mutate all model results by mapping tidy, glance, and augument onto each model. richness_models &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(Models = map(data, ~lm(Richness~NDVI, data=. ))) %&gt;% mutate(Tidied = map(Models, tidy), Glanced = map(Models, glance), Augmented = map(Models, augment)) richness_models ## # A tibble: 2 × 6 ## # Groups: Taxa [2] ## Taxa data Models Tidied Glanced Augmented ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 Birds &lt;tibbl… &lt;lm&gt; &lt;tibbl… &lt;tibble… &lt;tibble … ## 2 Butterflies &lt;tibbl… &lt;lm&gt; &lt;tibbl… &lt;tibble… &lt;tibble … Is NDVI more strongly associated with bird or butterfly species richness? Let’s compare the parameter estimates and their precision, which can be returned as a 95% confidence interval by specifying conf.int=TRUE and conf.level=0.95 (the default). richness_models &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(Models = map(data, ~lm(Richness~NDVI, data=. ))) %&gt;% mutate(Tidied = map(Models, tidy, conf.int = TRUE, conf.level = 0.95), Glanced = map(Models, glance), Augmented = map(Models, augment)) %&gt;% unnest(Tidied) parameter_estimates &lt;- richness_models %&gt;% select(Taxa, term, estimate, conf.low, conf.high) %&gt;% filter(term == &quot;NDVI&quot;) parameter_estimates ## # A tibble: 2 × 5 ## # Groups: Taxa [2] ## Taxa term estimate conf.low conf.high ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Birds NDVI 19.9 16.3 23.5 ## 2 Butterflies NDVI 12.7 8.91 16.4 Graph the results: compare the relationship with NDVI for both species. Rather than use the fitted values, predict richness for all NDVI values from 0 to 1 at intervals of 0.01. The precision of the prediction can be estimated as confidence intervals (interval=“confidence”) or prediction intervals (interval=“prediction”) richness_models &lt;- data_all %&gt;% group_by(Taxa) %&gt;% nest() %&gt;% mutate(Models = map(data, ~lm(Richness~NDVI, data=. ))) %&gt;% mutate(Tidied = map(Models, tidy), Glanced = map(Models, glance), Predictions = map(Models, augment, newdata = tibble(NDVI=seq(0,1,0.01)), se_fit=TRUE, interval=&quot;confidence&quot;)) # Rather than use the original NDVI values for prediction, # specify a new data frame (newdata) with the NDVI values of interest richness_predictions &lt;- richness_models %&gt;% unnest(Predictions) %&gt;% select(Taxa, NDVI, .fitted, .lower, .upper) %&gt;% rename(Richness = .fitted, low.95 = .lower, high.95 = .upper) p &lt;- richness_predictions %&gt;% ggplot(aes(x = NDVI, y = Richness, col = Taxa)) + geom_line(size = 2) + # Visualize confidence interval geom_ribbon(aes(ymin = low.95, ymax = high.95, fill = Taxa), show.legend=FALSE, color=NA, alpha=0.3) + # Add original data geom_point(data = data_all, aes(x = NDVI, y = Richness, col = Taxa), size=3) + scale_y_continuous(name = &quot;Species richness\\n&quot;) + scale_x_continuous(name = &quot;\\nNDVI&quot;) + scale_color_viridis_d() + scale_fill_viridis_d() + theme_classic() + theme(axis.line.x = element_line(colour = &quot;black&quot;), axis.line.y = element_line(colour=&quot;black&quot;), text = element_text(size = 12)) p #ggsave(&quot;Richness_vs_NDVI.pdf&quot;, device = &quot;pdf&quot;, width = 6, height = 3, units = &quot;in&quot;, dpi = 600) "],["reporg.html", "Session 3 Organizing Research Projects for Reproducibility 3.1 Goals for this session 3.2 Why do this? 3.3 What can this look like? 3.4 Reproducibility Edict #1: Don’t Save Your R Workspace and then Reload It 3.5 Paths, an important concept for reproducibility 3.6 The Working Directory 3.7 Reproducibility Edict #2: Don’t Change Directories in Your Code 3.8 Relative Paths 3.9 Reproducibility Edict #3: Always Use Relative Paths from the Project Directory to Files inside the Project Directory 3.10 Reproducibility Edict #4: Put data files into a data directory 3.11 Reproducibility Edict #5: Write Analysis Scripts as .R or .Rmd files 3.12 Download an example project to play with 3.13 Reproducibility Edict #6: Write your code with style! 3.14 What About Shared Data Sets? 3.15 Reproducibility Edict #7: Use git for Version Control 3.16 In the meantime, a short demo of git/GitHub on RStudio 3.17 What to keep under version control and what not to 3.18 Create a README to describe the parts of your analysis project 3.19 GitHub and Very large data sets", " Session 3 Organizing Research Projects for Reproducibility 3.1 Goals for this session Learn strategies to make our research and particularly our data analyses reproducible. Reproducible here means that we can easily send someone a directory filled with data and scripts, and it is nearly effortless for that person to re-run your entire analysis, regenerating all of your results, figures, and tables, and it is clear what all the different files and parts of your analysis project are, and the order that they must be run in. there is no part of the analysis that is done “by hand.” (i.e., no data formatting is done by copying and pasting columns of data in Excel.) The focus here is on using RStudio for make usefully reproducible projects within R. The ideal we will be striving for is to be able to start from a raw data set and then write a computer program that conducts all the cleaning, manipulation, and analysis of the data, and presentation of the results, in an automated fashion. 3.2 Why do this? Carrying out analyses in this way carries a number of advantages to the researcher: Newly-collected data can be integrated easily into your analysis. If a mistake is found in one section of your analysis, it is not terribly onerous to correct it and then re-run all the downstream analyses. Revising a manuscript to address referee comments can be done quickly. Years after publication, the exact steps taken to analyze the data will still be available should anyone ask you how, exactly, you did an analysis! If you have to conduct similar analyses and produce similar reports on a regular bias with new data each time, you might be able to do this readily by merely updating your data and then automatically producing the entire report. If someone finds an error in your work, they can fix it and then easily show you exactly what they did to fix it. Making research reproducible is good for the community: Others that would like to confirm your results can do so easily. Someone wanting to apply your methods to their own data can easily do so. Reproducibility makes it easier for everyone to learn more quickly from what their colleagues have done. Finally: Most journals require, at a minimum, that the raw data used in a paper be publicly archived and accessible. Increasingly, journals require all the analysis code as well. If you have emphasized reproducibility throughout your project, these last two are quite easy. 3.3 What can this look like? Using R, RStudio and RMarkdown, and freely available tools and services such as git and GitHub, it is possible to publish a reproducible project “website” that is part of a GitHub repository that holds everything needed to reproduce all the results. One example from a project I did in the last few years is here: https://eriqande.github.io/thompson-et-al-2020-chinook-salmon-migration-timing/index.html It took a fair bit to put that together, but I find that I go back to it all the time to direct colleagues and others to the code for certain analyses. 3.4 Reproducibility Edict #1: Don’t Save Your R Workspace and then Reload It R has a feature that lets you save your entire workspace, and then, at a later time, reload it so that you can keep working. Don’t do it. It is too easy to create R objects but not save the code that produced them when saving and loading .Rdata. “Save the code, not the workspace.” Also, in RStudio’s Global Options/Prefernces, make sure to disable automatic saving of your workspace in RStudio and never automatically load .Rdata when you open RStudio: 3.5 Paths, an important concept for reproducibility On your computer system, files can be stored inside directories. A file’s location relative to the root of your computer’s filesystem is called its absolute path. For example, on Windows: # the root is C: &quot;C:\\Documents and Settings\\Eric\\My Documents\\mydoc.txt&quot; Or on Mac: # the root is / /Users/eriq/Documents/git-repos/merida-workshop-2022/merida-workshop-2022.Rproj Absolute Paths are, almost by definition, NOT reproducible. Everyone has their computer set up differently. If I am trying to run a script that my friend, Devon, gave me, and one of his lines of code is: read_csv(&quot;/Users/devon/Documents/stuff/data.csv&quot;) Then, that will not work for me. In order to make that work, I would have to get the file data.csv separately, put the it somewhere on my computer, and then change the line of code above to reflect that file’s location on my computer. What a hassle!! 3.6 The Working Directory When R is running, it finds itself “stationed” in what is called the “working directory.” For any R session, at any time, that directory can be found with: getwd() ## [1] &quot;/Users/eriq/Documents/git-repos/merida-workshop-2022&quot; Everybody try that. 3.6.1 A Hugely Important Point! When you open an RStudio project, the working directory is always automatically set to the Project Directory—that is, the directory that holds the .Rproj file. If you have disabled automatic loading of the .Rdata file, then the R environment will also be completely empty—a blank slate!—when you open an RStudio project. That is how every analysis should start. 3.6.2 Skeletal anatomy of an RStudio Project The Project is basically everything that is inside the Project Directory. The .Rproj file is just a text file listing a few preferences, but your operating system recognizes it as an RStudio file. So, when you double-click it, it will open the project up in RStudio. (This is a good way to open RStudio projects…) 3.7 Reproducibility Edict #2: Don’t Change Directories in Your Code The companion function to getwd() is setwd() which lets you change the working directory. Generally you almost never want to use setwd() in your code. 3.8 Relative Paths A relative path is a path that starts from the working directory. For example, if the working directory contains a file called time-series.csv, then this will read it: read_csv(&quot;time-series.csv&quot;) If the working directory holds a directory called data, inside of which is a file called abundance.csv, then this will work: read_csv(&quot;data/abundance.csv&quot;) 3.9 Reproducibility Edict #3: Always Use Relative Paths from the Project Directory to Files inside the Project Directory You should plan on having an RStudio Project for every project that you work on, where, roughly, a project correponds to the work undertaken for a paper, publication, thesis chapter, report, etc. Everything needed to reproduce the analysis should be included withing the Project Directory. Assume the working directory is the Project Directory. Reading files, use relative paths. This way, the Project Directory has all the data and files needed to run the whole analysis. You can just give an RStudio Project to someone and they can run everything without changing paths, etc. # good read_csv(&quot;data/abundance.csv&quot;) read_csv(&quot;time-series.csv&quot;) source(&quot;R/my-functions.R&quot;) # bad: read_csv(&quot;/Users/eriq/files/data/abundance.csv&quot;) read_csv(&quot;C:/Documents and Settings/data/abundance.csv&quot;) source(&quot;../../../useful_stuff/my-functions.R&quot;) Note, the last “bad” example above uses a relative path to a file that is outside of the Project Directory. 3.10 Reproducibility Edict #4: Put data files into a data directory This data directory should be at the top level of the Project Directory. 3.11 Reproducibility Edict #5: Write Analysis Scripts as .R or .Rmd files These scripts/RMarkdown-documents/notebooks should be at the top level of the Project Directory These should be named with a leading number (I like three digits, like 001, 002, etc.) that gives the order in which they should be executed. Sometimes later scripts use, as inputs, outputs from earlier scripts, in which case the order that they are run in becomes paramount. 3.12 Download an example project to play with At this juncture, let’s all download a simple RStudio project to play with. The project is at: https://github.com/eriqande/small_project You can either: navigate there with a browser, click the green “code” button and “Download ZIP”; then expand that zip archive into a single Project Directory. When you have it, navigate into the small_project directory and double click the small_project.Rproj file to open the project in RStudio. if you have git, then you can open the project directly from RStudio using: File-&gt;New Project-&gt;Version Control-&gt;Git and then put: https://github.com/eriqande/small_project.git into the Repository URL box. 3.12.1 Let’s have a moment Take a moment to open up that project, and try to evaluate the notebooks: 001-read-and-format.Rmd 002-plot-data.Rmd Let’s discuss the structure of the project and have time for questions. In particular let’s talk about the R Notebook format: a combination of explanatory text and code chunks that provides a way of cleanly documenting your findings. 3.13 Reproducibility Edict #6: Write your code with style! R code is a lot easier to read when it is formatted in a consistent style. Your homework is to read the [Tidyverse Style Guide)[https://style.tidyverse.org/]. It takes about an hour and is extremely informative. Following the guidelines in the style guide will make it easier for you (and others) to read and understand your code. 3.14 What About Shared Data Sets? A quick sidebar about the ‘googlesheets4’ package. For some projects, you might want to retrieve data that your research team keeps centralized. Often that will be in a data base. In that case there are ways of accessing those data bases through R (but that is beyond the scope of today’s exploration.) Other times, pertinent information might be stored in a shared Google Sheet (that is Google’s Spreadsheet Format). For example, your lab might keep meta data about samples of birds or fish in a Google Sheet that is shared among lab members and is being frequently updated. For some projects (like for permit reporting requirements) it might be nice to have R code that directly accesses the current version of that document. You can! If you don’t already have ‘googlesheets4’ package, then: install.packages(&quot;googlesheets4&quot;) Imagine that someone has shared this Google Sheet with you. Click the link to view it. This is a publicly accessible sheet of data, so no authentication is needed to view it. Thus you can do: library(googlesheets4) # this is just storing the web address of the sheet sheet_url &lt;- &quot;https://docs.google.com/spreadsheets/d/1-7qG0X8M0TmyzB4Jaad6pNhV_SmcOY6T5JJk53dFU50/edit?usp=sharing&quot; # this tells R to not bother with Google authentication for this session gs4_deauth() fish_sheet &lt;- read_sheet(sheet_url) ## ✓ Reading from &quot;wgs-chinook-samples&quot;. ## ✓ Range &#39;wgs-chinook-samples&#39;. fish_sheet ## # A tibble: 160 × 8 ## vcf_name ID_Berk NMFS_DNA_ID BOX_ID BOX_POSITION ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 DPCh_plate… CH_Plat… T144767 T1512 1A ## 2 DPCh_plate… CH_Plat… T144804 T1512 5F ## 3 DPCh_plate… CH_Plat… T145109 T1515 7G ## 4 DPCh_plate… CH_Plat… T145118 T1515 8H ## 5 DPCh_plate… CH_Plat… T144863 T1513 1A ## 6 DPCh_plate… CH_Plat… T144906 T1513 6D ## 7 DPCh_plate… CH_Plat… T145128 T1515 10B ## 8 DPCh_plate… CH_Plat… T145142 T1515 11H ## 9 DPCh_plate… CH_Plat… T144959 T1514 1A ## 10 DPCh_plate… CH_Plat… T144980 T1514 3F ## # … with 150 more rows, and 3 more variables: ## # Population &lt;chr&gt;, Concentration (ng/ul) &lt;dbl&gt;, ## # run_type &lt;chr&gt; If you are using this to access a sheet that is not publicly shared, then omit the gs4_deauth() command (in a new R session), and R will send you to your browser to authenticate with a google account. (It is quite simple and user friendly!) More information is at the googlesheets4 web page. 3.15 Reproducibility Edict #7: Use git for Version Control RStudio provides one the cleanest, nicest interfaces, anywhere, for using git. Instructions to install git can be found within the comprehensive online book, by Jenny Bryan, happygitwithr. Accomplishing that with everyone in the short time we have would be difficult. But I encourage those who do not yet use git and GitHub to run through the happygitwithr book to get it all set up and start using it. 3.16 In the meantime, a short demo of git/GitHub on RStudio Just to show, visually, what it looks like, using our small project example. 3.17 What to keep under version control and what not to In general, keep data and code under version control do not keep outputs and intermediate files under version control Exceptions that we will talk about: Very large data sets HTML files from RMarkdown that you might want to use in a website. 3.18 Create a README to describe the parts of your analysis project You should always have a README file that describes the project, so that people you give it to will have a starting point, and you will be able to refresh your memory if you come back to it after a long time. The best way to make such a README is to use an RMarkdown file, README.Rmd that creates a markdown file README.md that can be rendered by GitHub. Such a file can be made from within RStudio by choosing File-&gt;New File-&gt;R Markdown...-&gt;From Template-&gt;GitHub Document An exampleREADME.Rmdis in thesmall-project`. 3.19 GitHub and Very large data sets If you have a very large data file, that you don’t want to put on GitHub (or it is too large for GitHub to accept), you can use the the googledrive package. This lets you store large data sets on Google Drive, and then download them automatically from your R code, when you run the scripts in your project. This also allows for authentication so that you can keep private, large data sets on Google Drive, safely. Here is a link to a small data set that I put up, publicly available on Google Drive. It is a serialized R data object (.rds) file. If you click on that link, your browser will be sent to a page that asks if you want to download the data set. But, you could access it directly from within R with the ‘googledrive’ package like this. First, get the package if you don’t already have it. # if you don&#39;t have the googledrive package install.packages(&quot;googledrive&quot;) Then, download it and read it (with read_rds()) in one line: library(googledrive) ## ## Attaching package: &#39;googledrive&#39; ## The following objects are masked from &#39;package:googlesheets4&#39;: ## ## request_generate, request_make # since the data are publicly available on my google drive # we will not worry about authentication. To get private # data at a different link, you would remove this and # run through the authentication process. drive_deauth() # here is the URL for the google drive file. url &lt;- &quot;https://drive.google.com/file/d/1071-7teo1dIPl9cJNigTYrBGN7I0E08U/view?usp=sharing&quot; download_results &lt;- drive_download(url, overwrite = TRUE) ## File downloaded: ## • &#39;RoSA-popgen-survey-data.rds&#39; ## &lt;id: 1071-7teo1dIPl9cJNigTYrBGN7I0E08U&gt; ## Saved locally as: ## • &#39;RoSA-popgen-survey-data.rds&#39; readr::read_rds(download_results$local_path[1]) ## # A tibble: 3,223 × 4 ## Indiv Population rosa pop_name ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 CH12366 IGH LLLLLLLL Iron Gate hatchery fall ## 2 CH12367 IGH LLLLLLLL Iron Gate hatchery fall ## 3 CH12368 IGH LLLLLLLL Iron Gate hatchery fall ## 4 CH12369 IGH LLLLLLLL Iron Gate hatchery fall ## 5 CH12370 IGH LLLLLLLL Iron Gate hatchery fall ## 6 CH12371 IGH LLLLLLLL Iron Gate hatchery fall ## 7 CH12372 IGH LLLLLLLL Iron Gate hatchery fall ## 8 CH12373 IGH LLLLLLLL Iron Gate hatchery fall ## 9 CH12374 IGH LLLLLLLL Iron Gate hatchery fall ## 10 CH12375 IGH LLLLLLLL Iron Gate hatchery fall ## # … with 3,213 more rows More details can be found at the googledrive web page "],["envdata.html", "Session 4 Finding and Obtaining Environmental Data for Conservation 4.1 Environmental Layers. A general overview", " Session 4 Finding and Obtaining Environmental Data for Conservation Types of environmental data: direcly observed (satellite data) Interpolated or modeled etc. Discussion of CRSs. 4.1 Environmental Layers. A general overview This section provides a brief narrative of Marius Someveille’s PowerPoint presentation. We are trying to succinctly capture the main points in this web/bookdown version so that it might easily be translated with Google Translate. Environmental layers in this context are raster data they carry information about particular “cells” or “pixels” of the earth’s surface. All of these terms will be defined in detail later. For now, here are a few pictures of what environmental layers might look like when they are plotted and displayed: 4.1.1 Environmental layers in conservation genetics 4.1.1.1 Landscape connectivity analysis (gene flow) Connectivity is essential for long term persistence of populations and metapopulations as it affects gene flow. Functional connectivity is the degree to which individuals move through a landscape, which is affected by landscape features and environmental heterogeneity. Landscape genetics tests which landscape and environmental factors best explain genetic connectivity among populations and highlights which landscape features need to be maintained or enhanced to facilitate connectivity (gene flow) for different species. For example, Robertson et al. (2018) studied frogs in multiple locations in North America, shown here: The positions of samples are shown here on a map that was created from land cover data. However, many other layers area available, giving information of, for example, topography, temperature, and moisture levels in the different locations. Landscape genetics approaches were used to identify which of these features were most associated with connectivity between populations in each of the different regions (i.e, different colors on the map). They summarized those results as follows: 4.1.1.2 Environmental Niche Analysis Ruegg et al. (2021) shows another example of the use of environmental layers in conservation genetics. In this case, genetic differences resolved separate breeding populations of the willow flycatcher in North America, and also made it possible to identify where birds from each breeding population spend their winters, as shown in the following figure (in which the different breeding populations are denoted by different colors): With the breeding and wintering areas for each subpopulation identified, other environmental layers (not shown) were used to identify the environmental and climatic conditions that defined the birds breeding and wintering niches for the subpopulations. Population trends were found to be associated with overlap between breeding and wintering niches, with the subpopulation showing the least niche overlap being the only subpopulation not showing a decreasing trend. 4.1.1.3 Genome-environment association to study vulnerability Associations between genetic variation and environmental variation (that we will hear about from Christen Bossu!) can be used to identify places where there is expected to be a mismatch between expected genetic patterns and environmental conditions after changes due to climate change. One of the first papers documenting this was Bay et al. (2018). In their study, a variety of environmental predictors (BIO18, BIO15, etc.), whose values across space are represented by environmental layer data, were found to be associated with genetic variation, as shown in the following montage of figures from the paper: 4.1.2 Environmental layers as raster data 4.1.3 Coordinate system and projections 4.1.4 Parameters of raster data 4.1.5 Examples of climate layers Source Method Variables Extent Spatial resolution Temporal resolution WorldClim Interpolation of observations from weather stations Temperature Precipitation Solar radiation BIOCLIM Global 30 sec (~1km2) 2.5 min 5 min 10 min (~340 km2) Monthly 1970 – 2000 Chelsa Quasi-mechanistical downscale of global circulation model Temperature Precipitation BIOCLIM Global 30 arc sec (~1km2) Monthly 1979 – 2018 Prism Statistical model using weather station data and atmospheric process Temperature Precipitation USA 30 arc sec (~1km2) Monthly 1981–2010 4.1.6 Examples of atmospheric layers Source Method Variables Extent Spatial resolution Temporal resolution NOAA Global Forecast System Global atmospheric model Wind direction Wind speed Global 0.5 deg 6 times / day 2011 – present WorldClim Interpolation of observations from weather stations Wind speed Water vapor pressure Global 30 sec (~1km2) 2.5 min 5 min 10 min (~340 km2) Monthly 1970 – 2000 4.1.7 Examples of vegetation layers Source Method Variables Extent Spatial resolution Temporal resolution VIP30 (NASA) Remote sensing using spectrometer on board satellites NDVI EVI Global 0.05 deg ~5.6km Monthly 1981 – 2014 Copernicus (ESA) Remote sensing using spectrometer on board satellites NDVI EVI Global 300m 10-day Present GEDI (NASA) Remote sensing using lidar (laser ranging) onboard the ISS Vertical profile of vegetation: canopy height, canopy vertical structure, ground elevation 51.6 deg N and S latitudes 25m Annual Present 4.1.8 Examples of landscape layers Source Method Variables Extent Spatial resolution Temporal resolution SRTM (NASA) Remote sensing using radar onboard satellites Elevation Global 1 arc-second (~30m) NA MODIS (NASA) Remote sensing using spectrometer onboard satellites Snow cover Global 30 arc-sec (~1km2) Copernicus (ESA) remote sensing + random forest for classification Land cover Global 100m NA NASA Gridded Population of the World Input data are census tables Human population density Global 30 arc-sec (~1km2) NA 2000–2020 References Bay RA, Harrigan RJ, Le Underwood V et al. (2018) Genomic signals of selection predict climate-driven population declines in a migratory bird. Science, 359, 83–86. Robertson JM, Murphy MA, Pearl CA et al. (2018) Regional variation in drivers of connectivity for two frog species (rana pretiosa and r. Luteiventris) from the US pacific northwest. Molecular ecology, 27, 3242–3256. Ruegg K, Anderson EC, Somveille M et al. (2021) Linking climate niches across seasons to assess population vulnerability in a migratory bird. Global Change Biology. "],["spatdata.html", "Session 5 Handling and Visualizing Spatial Data in R 5.1 Getting Environmental (Raster) data into R 5.2 Playing around with the Yucatan Jay 5.3 How about population densities? 5.4 Now, How about finding roads?", " Session 5 Handling and Visualizing Spatial Data in R 5.1 Getting Environmental (Raster) data into R Some of the data sources listed in the tables from the previous session can be accessed with the geodata package. library(geodata) ## Loading required package: terra ## terra version 1.4.11 ## ## Attaching package: &#39;terra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## src ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract # get worldclim data for mexico at 0.5 minutes # this fails for me on Mac Mojave because of a certification issue. Lame. # But I can download it directly #mex_tmin &lt;- worldclim_country(&quot;Mexico&quot;, &quot;tmin&quot;, res = 0.5, path = &quot;/tmp&quot;) #worldclim_country(&quot;Mexico&quot;, &quot;tmax&quot;, res = 0.5, path = &quot;/tmp&quot;) # After downloading library(terra) #mex_tmin &lt;- rast(&quot;~/Downloads/MEX_wc2.1_30s_tmin.tif&quot;) mex_tmax &lt;- rast(&quot;~/Downloads/MEX_wc2.1_30s_tmax.tif&quot;) #mex_prec &lt;- rast(&quot;~/Downloads/MEX_wc2.1_30s_prec.tif&quot;) #mex_bio &lt;- rast(&quot;~/Downloads/MEX_wc2.1_30s_bio.tif&quot;) #plot(mex_tmin) # looks reasonable, let&#39;s crop it: yp_extent &lt;- ext(c(-91, -86.5, 19.5, 21.75 )) #yp_tmin &lt;- crop(mex_tmin, yp_extent) yp_tmax &lt;- crop(mex_tmax, yp_extent) #yp_prec &lt;- crop(mex_prec, yp_extent) #yp_bio &lt;- crop(mex_bio, yp_extent) plot(yp_tmax) That is all happy. Let’s put this into a data folder somewhere dir.create(&quot;data/spatial/worldclim&quot;, recursive = TRUE, showWarnings = FALSE) writeRaster(yp_tmax, &quot;data/spatial/worldclim/tmax.tif&quot;, overwrite = TRUE) 5.2 Playing around with the Yucatan Jay library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:terra&#39;: ## ## intersect, union ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union library(tidyverse) yjay_full &lt;- read_tsv(&quot;data/spatial/Cyanocorax-yucatanicus.tsv.gz&quot;) ## Warning: One or more parsing issues, see `problems()` ## for details ## Rows: 25135 Columns: 50 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;\\t&quot; ## chr (31): datasetKey, occurrenceID, kingdom, phylu... ## dbl (14): gbifID, individualCount, decimalLatitude... ## lgl (2): coordinatePrecision, typeStatus ## dttm (3): eventDate, dateIdentified, lastInterpreted ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # that is a big data set, with a lot of columns, and observations from # a lot of different sources. Let&#39;s whittle it down to something # that is easier to see. yjay_thin &lt;- yjay_full %&gt;% filter( str_detect(collectionCode, &quot;EBIRD&quot;), # keep just the EBIRD-associated records year == 2017 # from the year 2017 ) %&gt;% select( # whittle it down to just a few columns of data gbifID, decimalLongitude, decimalLatitude, day, month, year, individualCount, stateProvince, locality ) # let&#39;s make a quick map: library(rnaturalearth) library(rnaturalearthdata) library(sf) ## Linking to GEOS 3.8.1, GDAL 3.2.1, PROJ 7.2.1 mex_states &lt;- ne_states( country = &quot;mexico&quot;, returnclass = &quot;sf&quot; ) ggplot() + geom_sf(data = mex_states) + geom_point(data = yjay_thin, aes(x = decimalLongitude, y = decimalLatitude), colour = &quot;red&quot;) + theme_bw() + coord_sf(xlim = c(-95, -85), ylim = c(12, 22)) Cool. Let’s focus on that in the core area: ggplot() + geom_sf(data = mex_states) + geom_point(data = yjay_thin, aes(x = decimalLongitude, y = decimalLatitude), colour = &quot;red&quot;) + theme_bw() + coord_sf(xlim = c(-94, -86), ylim = c(16, 22)) Cool. Now, it might be fun to look at collection dates. ggplot() + geom_sf(data = mex_states) + geom_point(data = yjay_thin, aes(x = decimalLongitude, y = decimalLatitude), colour = &quot;red&quot;) + theme_bw() + coord_sf(xlim = c(-94, -86), ylim = c(16, 22)) + facet_wrap(~month) 5.3 How about population densities? library(geodata) #pop &lt;- population(2020, 0.5, path = &quot;/tmp&quot;) # the above fails because of the certificate issues on my laptop, but # the error message gives me the URL: pop_url &lt;- &quot;https://biogeo.ucdavis.edu/data/geodata/pop/gpw_v4_population_density_rev11_2020_30s.tif&quot; # so we can just download that from a browser and then read it pop &lt;- rast(&quot;~/Downloads/gpw_v4_population_density_rev11_2020_30s.tif&quot;) yp_pop &lt;- crop(pop, yp_extent) plot(yp_pop) OK, that is pretty cool. 5.4 Now, How about finding roads? The NYU Faculty Archive has a great repository of street shapefiles by province in Mexico. Here is the link for the page for Yucatan: https://archive.nyu.edu/handle/2451/37126 And this downloads the zipped shapefile: https://archive.nyu.edu/bitstream/2451/37126/1/nyu_2451_37126.zip After download that and unzipping it we get: #yuc_roads &lt;- st_read(&quot;~/Downloads/nyu_2451_37126/yuc_eje_vial.shp&quot;) Cool. What do we have in there? #yuc_roads %&gt;% # count(TIPOVIAL) Stuff here. Marius and I brainstormed on this. We will do an example that involves the Yucatan Jay (or, better yet, ask Richard if a more interesting/iconic species is available). ECA has downloaded occurrences from GBIF (3 Mb compressed) and has put them into data. We can use that and some of the raster data and some vector data to do a little exploration of how to use the sf package in combination with ggplot2. Also, some filtering in the tidyverse. Ideas: Do some filtering of the data set (tidyverse reprise) Talk about lubridate for dealing with dates and times facet wrap over months to look at occurrences, etc. Some very light geometrical operations, for example get all the observations within X km of some features (like highways). Explore the use of different raster backgrounds. "],["geassoc.html", "Session 6 Genetic and Environmental Associations 6.1 Background 6.2 Motivation 6.3 Housekeeping 6.4 Running Gradient Forest 6.5 Basic Gradient Forest Preparation 6.6 Defining “Genomic Vulnerability/Genomic offset” Given Future Climate Predictions", " Session 6 Genetic and Environmental Associations 6.1 Background Now that we know how to get environmental layers and plot them in space, for many of the species we are working on, we are not entirely sure which environmental variables are important drivers of populaiton genetic structure or underlying phenotypic traits of interest. Let’s back up. Now, we have access to multiple layers of environmental layers that span a species range: And we might have a raster of population genetic structure such as the Willow Flycatcher genoscape, a spatially explicit map of population structure. What we want to investigate is how environmental variables are correlated with genetic variation. Note, this is not to look at candidate genes, but rather what combination of environmental variables shapes overall population genetic structure of a species. A recent example illustrated that not only is geography (“isolation by distance”) strongly correlated genetic structure, but environment plays a role in differentiation too. Specifically, precipitation is a strong predictor of Yellow Warbler genetic structure. 6.2 Motivation The R package gradientForest quantifies multi-species compositional turnover along environmental gradients. In the era of next-generation sequencing, where genome-wide data is common, we can extend this to determine which variants are associated with specific environmental variables. So rather than thinking of the large-scale species turnover in space, we can map fine-scale allele-frequency changes along environemntal gradients. Below is code for quantifying and visualizing intraspecific gene-environment variation across space. As part of The Bird Genoscape Project, we compare present-day gene-environment associations to predicted future changes to identify regions of genomic vulnerability. For now, this tutorial focuses on analyzing and plotting the allelic variation as estimated from gradient forest for a single species, the Willow Flycatcher, but is easily adpated to include other species and methods for esimating variation. 6.3 Housekeeping We will need the following R packages. gradientForest requires the dependancy extendedForest, an R package classification and regression based on forest trees using random inputs. With the newest version of R, extended forest is installed automatically as a dependency. Install and load the following packages in Rstudio. You can use the Packages function to the right to install most packages except for gradient forest, which we will install from R-Forge. 1) You will need XCode if you have a MAC, at minimum Command Line tools 2) If you have a PC, you’ll need Rtools 3) You will also need gfortran on your computer, which you can download from here: https://gcc.gnu.org/wiki/GFortran install.packages(&quot;gradientForest&quot;, repos=&quot;http://R-Forge.R-project.org&quot;) library(gradientForest) library(raster) library(gstat) library(rgdal) library(RColorBrewer) library(rasterVis) library(sf) library(tidyverse) #install.packages(&quot;gradientForest&quot;, repos=&quot;http://R-Forge.R-project.org&quot;) library(gradientForest) library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## hour, isoweek, mday, minute, month, quarter, ## second, wday, week, yday, year ## The following objects are masked from &#39;package:terra&#39;: ## ## copy, shift ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last ## The following object is masked from &#39;package:purrr&#39;: ## ## transpose library(raster) ## Loading required package: sp ## ## Attaching package: &#39;raster&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select library(gstat) library(rgdal) ## Please note that rgdal will be retired by the end of 2023, ## plan transition to sf/stars/terra functions using GDAL and PROJ ## at your earliest convenience. ## ## rgdal: version: 1.5-27, (SVN revision 1148) ## Geospatial Data Abstraction Library extensions to R successfully loaded ## Loaded GDAL runtime: GDAL 3.2.1, released 2020/12/29 ## Path to GDAL shared files: /Users/eriq/Library/R/4.1/library/rgdal/gdal ## GDAL binary built with GEOS: TRUE ## Loaded PROJ runtime: Rel. 7.2.1, January 1st, 2021, [PJ_VERSION: 721] ## Path to PROJ shared files: /Users/eriq/Library/R/4.1/library/rgdal/proj ## PROJ CDN enabled: FALSE ## Linking to sp version:1.4-5 ## To mute warnings of possible GDAL/OSR exportToProj4() degradation, ## use options(&quot;rgdal_show_exportToProj4_warnings&quot;=&quot;none&quot;) before loading sp or rgdal. ## Overwritten PROJ_LIB was /Users/eriq/Library/R/4.1/library/rgdal/proj ## ## Attaching package: &#39;rgdal&#39; ## The following object is masked from &#39;package:terra&#39;: ## ## project library(sf) library(RColorBrewer) library(rasterVis) ## Loading required package: lattice library(tidyverse) And we will need to load a realistic, polygon shapefile (an outline of the Willow Flycatcher’s breeding range in North America for this exercise) and set its projection. Note, you will need to adjust the path to the shapefile below. This is the path to my shape file on my computer. wifl_shape=st_read(&quot;data/6.0/WIFLrev.shp&quot;) ggplot(wifl_shape) + geom_sf(fill=NA) + theme_bw() That will produce this map: wifl_shape=st_read(&quot;data/6.0/WIFLrev.shp&quot;) ## Reading layer `WIFLrev&#39; from data source ## `/Users/eriq/Documents/git-repos/merida-workshop-2022/data/6.0/WIFLrev.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 5 features and 14 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -128.4 ymin: 29.04 xmax: -67.01 ymax: 53.46 ## Geodetic CRS: WGS 84 ggplot(wifl_shape) + geom_sf(fill=NA) + theme_bw() FIGURE 6.1: A basic outline of Willow Flycatcher breeding range we will use for this exercise 6.4 Running Gradient Forest We will use gradient forest to test whether a subset of genomic variation can be explained by environmental variables and to visualize climate-associated genetic variation across the breeding range of the Willow Flycatcher. To run gradient forest we need two main files. First we need the environmental data/climate variables associated with each population. Populations are defined by their latitude and longitude. Note to those interested in collecting environmental variables for their own species. For each sampling location, we obtained environmental data from publically available databases. Specifically, we chose variables with known impacts on bird physiology and ecology and that were publically available at high resolution. These variables included 19 climate variables downloaded from WorldClim (Hijmans et al. 2005) as well as vegetation indices (Carroll et al. 2004) (NDVI and NDVIstd), Tree Cover (Sexton et al. 2013) and elevation data from the Global Land Cover Facility (http://www.landcover.org). We also downloaded and used a measure of surface moisture characteristics from the NASA Scatterometer Climate Record Pathfinder project (QuickSCAT, downloaded from scp.byu.edu). Read in the environmental variables, and take a look at the dimensions and the actual data. Ewifl&lt;-read_csv(&#39;wiflforest.env.csv&#39;) dim(Ewifl) head(Ewifl) Ewifl&lt;-read_csv(&#39;data/6.0/wiflforest.env.csv&#39;) ## Rows: 21 Columns: 23 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (23): BIO1, BIO2, BIO3, BIO4, BIO5, BIO6, BIO7,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. dim(Ewifl) ## [1] 21 23 How many sites do we have environmental data for? How many environmental variables? Second, we have genomic data formatted as allele frequencies for each population in the same order. The published Willow Flycatcher data includes 37855 variants, and is too large to run a gradient forest analyses within the time frame of a tutorial, so we are supplying a subsetted dataset limited to 10,000 SNPs. Gwifl&lt;-read_csv(&#39;data/6.0/wiflforest.allfreq.sample.csv&#39;) dim(Gwifl) head(Gwifl) Gwifl&lt;-read_csv(&#39;data/6.0/wiflforest.allfreq.sample.csv&#39;) ## Rows: 21 Columns: 10000 ## ── Column specification ─────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (10000): V21252, V9139, V22510, V5860, V3386, V... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. dim(Gwifl) ## [1] 21 10000 head(Gwifl) ## # A tibble: 6 × 10,000 ## V21252 V9139 V22510 V5860 V3386 V14932 V14581 V9277 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.15 0.333 0.25 0.45 0.3 0.3 0.4 0.125 ## 2 0.107 0.25 0.25 0.571 0.308 0.607 0.357 0.143 ## 3 0.115 0.231 0.167 0.654 0.308 0.208 0.192 0.136 ## 4 0 0.1 0.0833 0.667 0.417 0.167 0 0.1 ## 5 0.25 0.667 0.125 0.625 0 0.75 0.25 0 ## 6 0.192 0.0833 0.0385 0.115 0.192 0.269 0.154 0.0909 ## # … with 9,992 more variables: V25872 &lt;dbl&gt;, ## # V23716 &lt;dbl&gt;, V25026 &lt;dbl&gt;, V11453 &lt;dbl&gt;, ## # V2266 &lt;dbl&gt;, V371 &lt;dbl&gt;, V21324 &lt;dbl&gt;, ## # V28422 &lt;dbl&gt;, V8773 &lt;dbl&gt;, V5906 &lt;dbl&gt;, ## # V15909 &lt;dbl&gt;, V20940 &lt;dbl&gt;, V15435 &lt;dbl&gt;, ## # V9026 &lt;dbl&gt;, V12438 &lt;dbl&gt;, V2181 &lt;dbl&gt;, ## # V23539 &lt;dbl&gt;, V19510 &lt;dbl&gt;, V15571 &lt;dbl&gt;, … These are allele frequencies for each site. What is the range of allele frequencies for allele V3386? You might need to write extra code to answer this question. Next we have some housekeeping to define the predictor variables in the regression, which are the environmental variables, and the response variables, which in this case are the frequencies of each variant, analyzed in gradient forest. preds &lt;- colnames(Ewifl) specs &lt;- colnames(Gwifl) nSites &lt;- dim(Gwifl)[1] nSpecs &lt;- dim(Gwifl)[2] # set depth of conditional permutation lev &lt;- floor(log2(nSites*0.368/2)) lev Now we run gradientForest with 10 bootstrapped trees to be generated by randomForest. This will take about 5 minutes. wiflforest=gradientForest(cbind(Ewifl,Gwifl), predictor.vars=preds, response.vars=specs, ntree=10, transform = NULL, compact=T,nbin=101, maxLevel=1,trace=T) preds &lt;- colnames(Ewifl) specs &lt;- colnames(Gwifl) nSites &lt;- dim(Gwifl)[1] nSpecs &lt;- dim(Gwifl)[2] # set depth of conditional permutation lev &lt;- floor(log2(nSites*0.368/2)) lev ## [1] 1 wiflforest=gradientForest(cbind(Ewifl,Gwifl), predictor.vars=preds, response.vars=specs, ntree=10, transform = NULL, compact=T,nbin=101, maxLevel=lev,trace=T) ## Calculating forests for 10000 species ## ...................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## .... ## Warning in randomForest.default(x = X, y = spec_vec, ## maxLevel = maxLevel, : The response has five or fewer ## unique values. Are you sure you want to do regression? ## ................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## .................................. ## Warning in randomForest.default(x = X, y = spec_vec, ## maxLevel = maxLevel, : The response has five or fewer ## unique values. Are you sure you want to do regression? ## ..................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## .............. ## Warning in randomForest.default(x = X, y = spec_vec, ## maxLevel = maxLevel, : The response has five or fewer ## unique values. Are you sure you want to do regression? ## ......................................... ## ........................... ## Warning in randomForest.default(x = X, y = spec_vec, ## maxLevel = maxLevel, : The response has five or fewer ## unique values. Are you sure you want to do regression? ## ............................ ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## ....................................................... ## .............................................. ##ntree=default is 10, more is better and reduces variance ##gradient forest bins variants, large scale patterns will be seen ##if one variant of large effect, may not be found in this analysis Now we can ask which environmental variables are significantly associated with the allelic structure of Willow Flycatchers (or at least the subset we are looking at). allpredictors=names(importance(wiflforest)) plot.gradientForest(wiflforest,plot.type=&quot;O&quot;) This first plot and most useful from gradientForest is the predictor overall importance plot, which shows a simple barplot of the ranked importances of the physical variables. This shows the mean accuracy importance and the mean importance weighted by SNP R^2. The most reliable importances are the R^2 weighted importances. #predictoroverallimportance plot(wiflforest,plot.type=&quot;Overall.Importance&quot;) FIGURE 6.2: Overall importance of predictor variables For the Willow Flycatcher subsetted data in this tutorial, what are the top 5 variables? Use the code below to look at Cuumulative importance, aka which variants are strongly associated with the top two variables. The cumulative importance is plotted separately for all species and averaged over all species. The cumulative importance can be interpreted as a mapping from an environmental gradient on to biological gradient. plot(wiflforest,plot.type=&quot;Cumulative.Importance&quot;) plot(wiflforest,plot.type=&quot;Cumulative.Importance&quot;) Which SNP has the greatest importance? For plotting purposes, only the first two top environmental variables are plotted. So the plots on the left are variants and turnover functions associated with Bio4, and the plots on the right are associated Bio3. You can confirm this by looking at the range of your top environemntal variables and comparing it to the x-axis in the plots above using this code below: Ewifl %&gt;% dplyr::select(BIO4,BIO3) ## # A tibble: 21 × 2 ## BIO4 BIO3 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 7888 43 ## 2 7792 44 ## 3 6365 50 ## 4 6468 47 ## 5 3298 54 ## 6 7032 44 ## 7 8676 36 ## 8 8762 36 ## 9 9542 30 ## 10 4173 50 ## # … with 11 more rows Note, while gradient forest can look at variants associated with environmental variables like this, we tend to choose the top uncorrelated environmental variables and run latent factor mixed models to identify candidate genes associated with each environmental variable. Why? In gradient forest, we “bin” the variation, so we are looking for large patterns, not individual variants. 6.5 Basic Gradient Forest Preparation So now that you see how gradient forest is run, let’s plot the results in space. Gradientforest here was based off of 21 locations in the WIFL range. I’ve randomly chosen 100,000 points within the WIFL range and gotten the evironmental data for these 100,000 sites, which will allow us to have a more continuous view of important environmental associations in WIFL space. Load your 100,000 random points with associated environmental predictors. This can be created with another script that was included with the tutorial material (bioclim&amp;randoms.R), but we’ll use the ready-made grid created specifically for Willow Flycatchers. birdgrid=read.table(&quot;wiflcurrentRH.csv&quot;,header=T,sep=&quot;,&quot;) %&gt;% filter(BIO4!=&quot;-9999&quot;) birdgrid #WIFL current random points, filter out &#39;bad&#39; points birdgrid=read.table(&quot;data/6.0/wiflcurrentRH.csv&quot;,header=T,sep=&quot;,&quot;) %&gt;% filter(BIO4!=&quot;-9999&quot;) #load your random points with assocaited env predictors ##Let&#39;s look at bird grid, untransformed environmental variable head(birdgrid) ## X Y BIO2 BIO3 BIO4 BIO5 BIO8 BIO9 BIO11 ## 1 -76.17 44.09 103 26 9755 260 93 -5 -60 ## 2 -103.38 42.94 164 36 9842 320 183 -43 -43 ## 3 -69.11 44.32 113 29 9282 261 28 187 -54 ## 4 -109.13 33.49 197 52 6719 287 175 121 9 ## 5 -106.01 33.40 169 46 7305 314 214 84 37 ## 6 -119.89 48.97 102 32 7206 167 -92 52 -98 ## BIO15 BIO17 BIO18 ## 1 14 201 224 ## 2 64 32 167 ## 3 14 256 256 ## 4 60 47 200 ## 5 65 42 156 ## 6 25 105 117 dim(birdgrid) ## [1] 99541 12 We don’t have to use all the predictors in the next steps, in this case, we’ll use the top 4 uncorrelated variables. These variables were selected by moving down the list of ranked importance for the full model and discarding variables highly correlated (|Pearson’s r| &gt; 0.7) with a variable of higher importance. You can see the correlations among environmental variables across your sample sites. While it is clearly hard to read here, if you export this and open in excel, you can color code and keep track of the correlations among the ranked variables. cor(Ewifl,method=&quot;pearson&quot;) cor(Ewifl,method=&quot;pearson&quot;) ## BIO1 BIO2 BIO3 BIO4 ## BIO1 1.000000 0.06378 0.41991 -0.3227 ## BIO2 0.063781 1.00000 0.20607 0.2063 ## BIO3 0.419906 0.20607 1.00000 -0.8978 ## BIO4 -0.322651 0.20626 -0.89779 1.0000 ## BIO5 0.704232 0.51230 -0.07301 0.3342 ## BIO6 0.817384 -0.18653 0.73399 -0.7881 ## BIO7 -0.321400 0.49669 -0.73233 0.9472 ## BIO8 0.410359 -0.07870 -0.33935 0.3986 ## BIO9 0.609772 0.29009 0.77332 -0.6709 ## BIO10 0.836811 0.21572 -0.07723 0.2463 ## BIO11 0.883843 -0.02400 0.75646 -0.7257 ## BIO12 -0.067882 -0.62811 0.11332 -0.4321 ## BIO13 -0.009306 -0.52714 0.28325 -0.5426 ## BIO14 -0.331476 -0.57503 -0.48588 0.1622 ## BIO15 0.471772 0.03587 0.62916 -0.5217 ## BIO16 -0.013761 -0.54400 0.27646 -0.5434 ## BIO17 -0.309207 -0.61532 -0.43194 0.0850 ## BIO18 -0.301191 -0.39732 -0.63220 0.4621 ## BIO19 0.057114 -0.50485 0.38306 -0.6482 ## NDVI_Mean -0.251826 -0.71683 -0.12471 -0.1923 ## NDVI_StDev -0.315948 -0.51217 -0.36790 0.1161 ## SRTM -0.558674 0.62444 0.04009 0.1726 ## TreeCover -0.229187 -0.31430 0.04665 -0.1977 ## BIO5 BIO6 BIO7 BIO8 ## BIO1 0.70423 0.81738 -0.32140 0.41036 ## BIO2 0.51230 -0.18653 0.49669 -0.07870 ## BIO3 -0.07301 0.73399 -0.73233 -0.33935 ## BIO4 0.33424 -0.78808 0.94724 0.39863 ## BIO5 1.00000 0.22968 0.41425 0.46587 ## BIO6 0.22968 1.00000 -0.79069 0.02865 ## BIO7 0.41425 -0.79069 1.00000 0.26626 ## BIO8 0.46587 0.02865 0.26626 1.00000 ## BIO9 0.32435 0.75801 -0.50484 -0.27460 ## BIO10 0.92631 0.37954 0.22775 0.63524 ## BIO11 0.35976 0.97870 -0.68894 0.08585 ## BIO12 -0.49614 0.29580 -0.58872 -0.12239 ## BIO13 -0.46821 0.38147 -0.65126 -0.17254 ## BIO14 -0.40528 -0.22475 -0.04476 0.13339 ## BIO15 0.10623 0.56762 -0.46399 0.05121 ## BIO16 -0.47815 0.38053 -0.65664 -0.18279 ## BIO17 -0.43514 -0.15673 -0.12715 0.09357 ## BIO18 -0.22777 -0.42054 0.24999 0.51220 ## BIO19 -0.43996 0.48703 -0.73221 -0.29859 ## NDVI_Mean -0.57552 0.03524 -0.39498 -0.01593 ## NDVI_StDev -0.38446 -0.19066 -0.06354 -0.07546 ## SRTM -0.19354 -0.54098 0.38416 -0.45175 ## TreeCover -0.41770 0.01015 -0.27224 -0.19531 ## BIO9 BIO10 BIO11 BIO12 ## BIO1 0.609772 0.83681 0.88384 -0.067882 ## BIO2 0.290086 0.21572 -0.02400 -0.628113 ## BIO3 0.773320 -0.07723 0.75646 0.113324 ## BIO4 -0.670908 0.24630 -0.72567 -0.432119 ## BIO5 0.324350 0.92631 0.35976 -0.496143 ## BIO6 0.758014 0.37954 0.97870 0.295805 ## BIO7 -0.504839 0.22775 -0.68894 -0.588719 ## BIO8 -0.274601 0.63524 0.08585 -0.122389 ## BIO9 1.000000 0.24982 0.78314 -0.006606 ## BIO10 0.249825 1.00000 0.48768 -0.342580 ## BIO11 0.783137 0.48768 1.00000 0.145899 ## BIO12 -0.006606 -0.34258 0.14590 1.000000 ## BIO13 0.119072 -0.34414 0.24629 0.960840 ## BIO14 -0.455211 -0.26195 -0.33897 0.562620 ## BIO15 0.419885 0.17396 0.59930 0.146088 ## BIO16 0.110219 -0.34938 0.24323 0.967026 ## BIO17 -0.409710 -0.28615 -0.28594 0.630604 ## BIO18 -0.729840 -0.06602 -0.46821 0.390028 ## BIO19 0.271765 -0.33226 0.34999 0.939213 ## NDVI_Mean -0.349208 -0.39821 -0.10654 0.625462 ## NDVI_StDev -0.437459 -0.27386 -0.29317 0.356228 ## SRTM -0.075564 -0.44200 -0.46197 -0.470134 ## TreeCover -0.202842 -0.36603 -0.06991 0.429829 ## BIO13 BIO14 BIO15 BIO16 ## BIO1 -0.009306 -0.33148 0.47177 -0.01376 ## BIO2 -0.527137 -0.57503 0.03587 -0.54400 ## BIO3 0.283245 -0.48588 0.62916 0.27646 ## BIO4 -0.542562 0.16217 -0.52172 -0.54341 ## BIO5 -0.468213 -0.40528 0.10623 -0.47815 ## BIO6 0.381466 -0.22475 0.56762 0.38053 ## BIO7 -0.651256 -0.04476 -0.46399 -0.65664 ## BIO8 -0.172536 0.13339 0.05121 -0.18279 ## BIO9 0.119072 -0.45521 0.41988 0.11022 ## BIO10 -0.344142 -0.26195 0.17396 -0.34938 ## BIO11 0.246291 -0.33897 0.59930 0.24323 ## BIO12 0.960840 0.56262 0.14609 0.96703 ## BIO13 1.000000 0.32879 0.36730 0.99905 ## BIO14 0.328790 1.00000 -0.59277 0.34565 ## BIO15 0.367297 -0.59277 1.00000 0.35526 ## BIO16 0.999051 0.34565 0.35526 1.00000 ## BIO17 0.404288 0.98685 -0.54574 0.42176 ## BIO18 0.215627 0.75711 -0.37197 0.22028 ## BIO19 0.972277 0.30823 0.33183 0.97502 ## NDVI_Mean 0.555532 0.47722 0.11569 0.57337 ## NDVI_StDev 0.226222 0.47804 -0.16079 0.24853 ## SRTM -0.434899 -0.23097 -0.31958 -0.44193 ## TreeCover 0.437128 0.14043 0.17095 0.45137 ## BIO17 BIO18 BIO19 NDVI_Mean ## BIO1 -0.30921 -0.30119 0.05711 -0.25183 ## BIO2 -0.61532 -0.39732 -0.50485 -0.71683 ## BIO3 -0.43194 -0.63220 0.38306 -0.12471 ## BIO4 0.08500 0.46213 -0.64819 -0.19234 ## BIO5 -0.43514 -0.22777 -0.43996 -0.57552 ## BIO6 -0.15673 -0.42054 0.48703 0.03524 ## BIO7 -0.12715 0.24999 -0.73221 -0.39498 ## BIO8 0.09357 0.51220 -0.29859 -0.01593 ## BIO9 -0.40971 -0.72984 0.27176 -0.34921 ## BIO10 -0.28615 -0.06602 -0.33226 -0.39821 ## BIO11 -0.28594 -0.46821 0.34999 -0.10654 ## BIO12 0.63060 0.39003 0.93921 0.62546 ## BIO13 0.40429 0.21563 0.97228 0.55553 ## BIO14 0.98685 0.75711 0.30823 0.47722 ## BIO15 -0.54574 -0.37197 0.33183 0.11569 ## BIO16 0.42176 0.22028 0.97502 0.57337 ## BIO17 1.00000 0.73995 0.38600 0.55921 ## BIO18 0.73995 1.00000 0.05866 0.41008 ## BIO19 0.38600 0.05866 1.00000 0.50178 ## NDVI_Mean 0.55921 0.41008 0.50178 1.00000 ## NDVI_StDev 0.53567 0.44991 0.18247 0.72177 ## SRTM -0.28863 -0.24335 -0.41100 -0.44004 ## TreeCover 0.21803 0.12935 0.41019 0.73548 ## NDVI_StDev SRTM TreeCover ## BIO1 -0.31595 -0.55867 -0.22919 ## BIO2 -0.51217 0.62444 -0.31430 ## BIO3 -0.36790 0.04009 0.04665 ## BIO4 0.11607 0.17262 -0.19766 ## BIO5 -0.38446 -0.19354 -0.41770 ## BIO6 -0.19066 -0.54098 0.01015 ## BIO7 -0.06354 0.38416 -0.27224 ## BIO8 -0.07546 -0.45175 -0.19531 ## BIO9 -0.43746 -0.07556 -0.20284 ## BIO10 -0.27386 -0.44200 -0.36603 ## BIO11 -0.29317 -0.46197 -0.06991 ## BIO12 0.35623 -0.47013 0.42983 ## BIO13 0.22622 -0.43490 0.43713 ## BIO14 0.47804 -0.23097 0.14043 ## BIO15 -0.16079 -0.31958 0.17095 ## BIO16 0.24853 -0.44193 0.45137 ## BIO17 0.53567 -0.28863 0.21803 ## BIO18 0.44991 -0.24335 0.12935 ## BIO19 0.18247 -0.41100 0.41019 ## NDVI_Mean 0.72177 -0.44004 0.73548 ## NDVI_StDev 1.00000 -0.18123 0.48145 ## SRTM -0.18123 1.00000 -0.10359 ## TreeCover 0.48145 -0.10359 1.00000 For the remaining time in this tutorial we are focusing on the uncorrelated variables determined for the full Willow Flycatcher dataset (Ruegg et al. 2017), BIO11 (Mean temperature of coldest quarter), BIO5 (Max temperature of warmest month), BIO4 (Temperature seasonality (standard deviation *100)), and BIO17 (Precipitation of driest quarter). predictors_uncor&lt;-c(&quot;BIO11&quot;,&quot;BIO5&quot;,&quot;BIO4&quot; ,&quot;BIO17&quot;) Predict across your random sites using your Forest model: currentgrid=cbind(birdgrid[,c(&quot;X&quot;,&quot;Y&quot;)], predict(wiflforest,birdgrid[,predictors_uncor])) currentgrid predictors_uncor&lt;-c(&quot;BIO11&quot;,&quot;BIO5&quot;,&quot;BIO4&quot; ,&quot;BIO17&quot;) # currentgrid=cbind(birdgrid[,c(&quot;X&quot;,&quot;Y&quot;)], predict(wiflforest,birdgrid[,predictors_uncor])) #predict across your random sites using your forest ##Let&#39;s look at currentgrid, transformed environmental variables- see how they were transformed head(currentgrid) ## X Y BIO11 BIO5 BIO4 ## 1 -76.17 44.09 0.0006540 0.0009918 0.022438 ## 2 -103.38 42.94 0.0014597 0.0032041 0.022612 ## 3 -69.11 44.32 0.0007082 0.0010598 0.021111 ## 4 -109.13 33.49 0.0052866 0.0024756 0.004016 ## 5 -106.01 33.40 0.0071168 0.0032002 0.004840 ## 6 -119.89 48.97 -0.0001436 -0.0024641 0.004690 ## BIO17 ## 1 0.004702 ## 2 0.000772 ## 3 0.005522 ## 4 0.001241 ## 5 0.001157 ## 6 0.002248 dim(currentgrid) ## [1] 99541 6 We can summarize the variation using principal components. We then convert those PC axes to RGB values for plotting. PCs=prcomp(currentgrid[,3:6]) #run your PCs on your top four uncorrelated variables a1 &lt;- PCs$x[,1] a2 &lt;- PCs$x[,2] a3 &lt;- PCs$x[,3] r &lt;- a1+a2 g &lt;- -a2 b &lt;- a3+a2-a1 r &lt;- (r-min(r)) / (max(r)-min(r)) * 255 g &lt;- (g-min(g)) / (max(g)-min(g)) * 255 b &lt;- (b-min(b)) / (max(b)-min(b)) * 255 wiflcols=rgb(r,g,b,max=255) wiflcols2=col2rgb(wiflcols) wiflcols3=t(wiflcols2) gradients=cbind(currentgrid[c(&quot;X&quot;,&quot;Y&quot;)],wiflcols3) The multi-dimensional biological space can be plotted by taking the principle component of the transformted grid and presenting the first two dimensions in a biplot. You’ve already defined the PCs above and the RGB color palette. Different coordinate positions in the biplot represent differing allele frequency compositions as associated with the predictors. nvs &lt;- dim(PCs$rotation)[1] vec &lt;- c(&quot;BIO11&quot;,&quot;BIO5&quot;,&quot;BIO4&quot; ,&quot;BIO17&quot;) lv &lt;- length(vec) vind &lt;- rownames(PCs$rotation) %in% vec scal &lt;- 50 xrng &lt;- range(PCs$x[, 1], PCs$rotation[, 1]/scal) *1.1 yrng &lt;- range(PCs$x[, 2], PCs$rotation[, 2]/scal) *.3 plot((PCs$x[, 1:2]), xlim = xrng, ylim = yrng, pch = &quot;.&quot;, cex = 4, col = rgb(r, g, b, max = 255), asp = 1) points(PCs$rotation[!vind, 1:2]/scal, pch = &quot;+&quot;) arrows(rep(0, lv), rep(0, lv), PCs$rotation[vec,1]/scal, PCs$rotation[vec, 2]/scal, length = 0.0625) jit &lt;- 0.0015 text(PCs$rotation[vec, 1]/scal + jit * sign(PCs$rotation[vec,1]), PCs$rotation[vec, 2]/scal + jit * sign(PCs$rotation[vec,2]), labels = vec) #Getting the PCs and Converting Them to RGB Color Values PCs=prcomp(currentgrid[,3:6]) #run your PCs on your top four variables # set up a colour palette for the mapping a1 &lt;- PCs$x[,1] a2 &lt;- PCs$x[,2] a3 &lt;- PCs$x[,3] r &lt;- a1+a2 g &lt;- -a2 b &lt;- a3+a2-a1 r &lt;- (r-min(r)) / (max(r)-min(r)) * 255 g &lt;- (g-min(g)) / (max(g)-min(g)) * 255 b &lt;- (b-min(b)) / (max(b)-min(b)) * 255 wiflcols=rgb(r,g,b,max=255) wiflcols2=col2rgb(wiflcols) wiflcols3=t(wiflcols2) gradients=cbind(currentgrid[c(&quot;X&quot;,&quot;Y&quot;)],wiflcols3) ##Biplot specifics nvs &lt;- dim(PCs$rotation)[1] vec &lt;- c(&quot;BIO11&quot;,&quot;BIO5&quot;,&quot;BIO4&quot; ,&quot;BIO17&quot;) lv &lt;- length(vec) vind &lt;- rownames(PCs$rotation) %in% vec scal &lt;-100 xrng &lt;- range(PCs$x[, 1], PCs$rotation[, 1]/scal) *1.2 yrng &lt;- range(PCs$x[, 2], PCs$rotation[, 2]/scal) *1.2 plot((PCs$x[, 1:2]), xlim = xrng, ylim = yrng, pch = &quot;.&quot;, cex = 4, col = rgb(r, g, b, max = 255), asp = 1) points(PCs$rotation[!vind, 1:2]/scal, pch = &quot;+&quot;) arrows(rep(0, lv), rep(0, lv), PCs$rotation[vec,1]/scal, PCs$rotation[vec, 2]/scal, length = 0.0625) jit &lt;- 0.0015 text(PCs$rotation[vec, 1]/scal + jit * sign(PCs$rotation[vec,1]), PCs$rotation[vec, 2]/scal + jit * sign(PCs$rotation[vec,2]), labels = vec) FIGURE 6.3: A principle component plot of the top 4 uncorrelated variables Save this image using the code chunk below. What environmental variables are associated with the colors green, pink and blue? Knowing this will help with the interpretation of plotting this in space. pdf(&quot;stored_results/6.0/WIFL.PCA_uncorrelated.pdf&quot;) nvs &lt;- dim(PCs$rotation)[1] vec &lt;- c(&quot;BIO11&quot;,&quot;BIO5&quot;,&quot;BIO4&quot; ,&quot;BIO17&quot;) lv &lt;- length(vec) vind &lt;- rownames(PCs$rotation) %in% vec scal &lt;-100 xrng &lt;- range(PCs$x[, 1], PCs$rotation[, 1]/scal) *1.2 yrng &lt;- range(PCs$x[, 2], PCs$rotation[, 2]/scal) *1.2 plot((PCs$x[, 1:2]), xlim = xrng, ylim = yrng, pch = &quot;.&quot;, cex = 4, col = rgb(r, g, b, max = 255), asp = 1) points(PCs$rotation[!vind, 1:2]/scal, pch = &quot;+&quot;) arrows(rep(0, lv), rep(0, lv), PCs$rotation[vec,1]/scal, PCs$rotation[vec, 2]/scal, length = 0.0625) jit &lt;- 0.0015 text(PCs$rotation[vec, 1]/scal + jit * sign(PCs$rotation[vec,1]), PCs$rotation[vec, 2]/scal + jit * sign(PCs$rotation[vec,2]), labels = vec) dev.off() ## quartz_off_screen ## 2 With this, we can draw our first gradient forest map: wiflmap=gradients coordinates(wiflmap)=~X+Y #setting the coordinates proj4string(wiflmap) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) plot(wiflmap,col=rgb(r,g,b,max=255),pch=15) lines(wifl_shape) wiflcols=rgb(r,g,b,max=255) wiflcols2=col2rgb(wiflcols) wiflcols3=t(wiflcols2) gradients=cbind(currentgrid[c(&quot;X&quot;,&quot;Y&quot;)],wiflcols3) wiflmap=gradients coordinates(wiflmap)=~X+Y #setting the coordinates proj4string(wiflmap) &lt;- CRS(&quot;+proj=longlat +datum=WGS84&quot;) par(mar=c(0,0,0,0)) plot(wiflmap,col=rgb(r,g,b,max=255),pch=15) FIGURE 6.4: Our gradient forest model of SNP variation in the Willow Flycatcher breeding range How would you interpret the different colors on thie WIFL map? Is the endangered southwest Willow Flycatcher locally adapted? Let’s plot this another way, on a map. To do this, we need some additional shapefiles coastlines &lt;- st_read(&quot;data/6.0/ne_shapefiles/ne_10m_coastline.shp&quot;) domain &lt;- c( xmin = -140, xmax = -55, ymin = 15, ymax = 50 ) coast_cropped &lt;- st_crop(coastlines, domain) ggplot(coast_cropped) + geom_sf() + coord_sf() countries_cropped &lt;- st_read(&quot;data/6.0/ne_shapefiles/ne_10m_admin_0_boundary_lines_land.shp&quot;) %&gt;% st_crop(domain) states_cropped &lt;- st_read(&quot;data/6.0/ne_shapefiles/ne_10m_admin_1_states_provinces_lines.shp&quot;) %&gt;% st_crop(domain) The domain here allows you to crop the shapefiles to just your species range. library(sf) coastlines &lt;- st_read(&quot;data/6.0/ne_shapefiles/ne_10m_coastline.shp&quot;) ## Reading layer `ne_10m_coastline&#39; from data source ## `/Users/eriq/Documents/git-repos/merida-workshop-2022/data/6.0/ne_shapefiles/ne_10m_coastline.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 4133 features and 3 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -180 ymin: -85.22 xmax: 180 ymax: 83.63 ## Geodetic CRS: WGS 84 domain &lt;- c( xmin = -140, xmax = -55, ymin = 15, ymax = 50 ) coast_cropped &lt;- st_crop(coastlines, domain) ## Warning: attribute variables are assumed to be ## spatially constant throughout all geometries This code allows you to plot, but also change the domain to what will encompass your species shapefile. Suffice it to say I went through a fair number of iterations to ge it right. ggplot(coast_cropped) + geom_sf() + coord_sf() Once you have the shapefiles and cropped domain, you can create a map, project the map, and then crop it to what region you want. Using the above code, the domain can/should be larger than what you will be plotting. You can further crop later! library(ggspatial) countries_cropped &lt;- st_read(&quot;data/6.0/ne_shapefiles/ne_10m_admin_0_boundary_lines_land.shp&quot;) %&gt;% st_crop(domain) ## Reading layer `ne_10m_admin_0_boundary_lines_land&#39; from data source `/Users/eriq/Documents/git-repos/merida-workshop-2022/data/6.0/ne_shapefiles/ne_10m_admin_0_boundary_lines_land.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 462 features and 18 fields ## Geometry type: MULTILINESTRING ## Dimension: XY ## Bounding box: xmin: -141 ymin: -55.12 xmax: 141 ymax: 70.08 ## Geodetic CRS: WGS 84 ## Warning: attribute variables are assumed to be ## spatially constant throughout all geometries states_cropped &lt;- st_read(&quot;data/6.0/ne_shapefiles/ne_10m_admin_1_states_provinces_lines.shp&quot;) %&gt;% st_crop(domain) ## Reading layer `ne_10m_admin_1_states_provinces_lines&#39; from data source `/Users/eriq/Documents/git-repos/merida-workshop-2022/data/6.0/ne_shapefiles/ne_10m_admin_1_states_provinces_lines.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 10114 features and 21 fields ## Geometry type: MULTILINESTRING ## Dimension: XY ## Bounding box: xmin: -178.1 ymin: -49.25 xmax: 178.4 ymax: 81.13 ## Geodetic CRS: WGS 84 ## Warning: attribute variables are assumed to be ## spatially constant throughout all geometries mapg &lt;- ggplot() + geom_sf(data = coast_cropped) + geom_sf(data = countries_cropped, fill = NA) + geom_sf(data = states_cropped, fill = NA) + ggspatial::layer_spatial(wiflmap,col=rgb(r,g,b,max=255),pch=15) + #geom_spatial_point(data=Epabu, aes(Long,Lat),col=&quot;black&quot;) + geom_sf(data=wifl_shape,fill=NA) + theme_bw() #Now, let&#39;s see how our developing map looks under such a projection. We just define `lamproj` as a coordinate reference system and pass it in to `coord_sf()` lamproj &lt;- &quot;+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-100 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs&quot; #Plotting, but cutting out rectangle (Don&#39;t need a lot of extra space) rectangled &lt;-mapg + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;)+ coord_sf( crs = st_crs(lamproj), xlim = c(-2200476.8, 2859510), ylim = c(-1899797, 1809910.8), expand = FALSE) rectangled FIGURE 6.5: Our gradient forest model of SNP variation in the Willow Flycatcher breeding range on a map Rename this file. I put the plot I plotted in here, but let’s make sure this works for you!! ggsave(&quot;stored_results/6.0/WIFL.GF_map.pdf&quot;) ## Saving 7 x 5 in image 6.6 Defining “Genomic Vulnerability/Genomic offset” Given Future Climate Predictions The plots above are the genotype-environment relationship given present-day climate rasters. However, we know rapid fluctuations in temperature and precipitation associated with climate change can alter the suitability of particular regions making it necessary for individuals to either adapt, disperse or die if the changes are extreme enough. Those species that possess standing genetic variation for climate-related traits (i.e. have adaptive capacity) are most likely to have the ability to adapt to rapidly changing environments. Those species that unable to adapt are deemed most vulneraable. To investigate which populations might be most vulnerable to future climate change, we defined the metric “genomic vulnerability” as the mismatch between current and predicted future genomic variation based on genotype-environment relationships modeled across contemporary populations. We followed the method presented in Fitzpatrick and Keller (2015) to calculate genomic vulnerability using an extension of the gradient forest analysis. Populations with the greatest mismatch are least likely to adapt quickly enough to track future climate shifts, potentially resulting in population declines or extirpations. Here, we read in the future climate predictions for each random point. Since different amounts of heat-trapping gases released into the atmosphere by human activities produce different projected increases in Earth’s temperature, we provided two such predicted models: the 2.6 emission predictions and the 8.5 emission predictions for the year 2050. The lowest recent emissions pathway (RCP), 2.6, assumes immediate and rapid reductions in emissions and would result in about 2.5°F of warming in this century. The highest emissions pathway, RCP 8.5, roughly similar to a continuation of the current path of global emissions increases, is projected to lead to more than 8°F warming by 2100, with a high-end possibility of more than 11°F. Each of these is a composite of multiple prediction models and have rather different predictions. future1&lt;-read.table(&quot;data/6.0/wiflfuture2-6-2050.csv&quot;,header=T,sep=&quot;,&quot;) %&gt;% filter(BIO4!=&quot;-9999&quot;) head(future1) future2&lt;-read.table(&quot;data/6.0/wiflfuture8-5-2050.csv&quot;,header=T,sep=&quot;,&quot;) %&gt;% filter(BIO4!=&quot;-9999&quot;) head(future2) future1&lt;-read.table(&quot;data/6.0/wiflfuture2-6-2050.csv&quot;,header=T,sep=&quot;,&quot;) %&gt;% filter(BIO4!=&quot;-9999&quot;) head(future1) ## X Y BIO2 BIO3 BIO4 BIO5 BIO8 BIO9 BIO11 ## 1 -76.17 44.09 102 26 9584 283 65 138 -34 ## 2 -103.38 42.94 166 36 9942 342 171 -15 -24 ## 3 -69.11 44.32 112 29 9209 282 42 173 -31 ## 4 -109.13 33.49 202 51 6941 310 195 133 24 ## 5 -106.01 33.40 174 46 7508 338 235 109 53 ## 6 -119.89 48.97 101 31 7244 192 -70 82 -79 ## BIO15 BIO17 BIO18 ## 1 16 209 221 ## 2 63 35 155 ## 3 17 259 273 ## 4 69 33 202 ## 5 71 30 153 ## 6 33 97 107 future2&lt;-read.table(&quot;data/6.0/wiflfuture8-5-2050.csv&quot;,header=T,sep=&quot;,&quot;) %&gt;% filter(BIO4!=&quot;-9999&quot;) head(future2) ## X Y BIO2 BIO3 BIO4 BIO5 BIO8 BIO9 BIO11 ## 1 -76.17 44.09 101 26 9652 299 46 175 -19 ## 2 -103.38 42.94 166 35 10238 359 183 -6 -15 ## 3 -69.11 44.32 111 29 9211 295 45 209 -18 ## 4 -109.13 33.49 203 51 7078 323 208 142 33 ## 5 -106.01 33.40 175 45 7602 351 249 127 63 ## 6 -119.89 48.97 101 30 7521 210 -64 111 -70 ## BIO15 BIO17 BIO18 ## 1 16 215 224 ## 2 62 41 153 ## 3 19 257 269 ## 4 69 30 198 ## 5 75 27 152 ## 6 37 87 90 We then use our gradient forest model to transform these into “biological space” just as we did for random points across contemporary environments. futuregrid=cbind(future1[,c(&quot;X&quot;,&quot;Y&quot;)],predict(wiflforest,future1[,predictors_uncor])) futuregrid=cbind(future1[,c(&quot;X&quot;,&quot;Y&quot;)],predict(wiflforest,future1[,predictors_uncor])) Finally, we calculate genomic vulnerability. Mathematically, for a given point this is the Euclidean distance between the current biological space (currentgrid) and the future biological space (futuregrid): coords&lt;-birdgrid[,c(&quot;X&quot;,&quot;Y&quot;)] euc &lt;- matrix(data=NA,nrow=nrow(futuregrid),ncol=3) for(j in 1:nrow(currentgrid)) { euc[j,] &lt;- c(as.numeric(coords[j,]),as.numeric(dist(rbind(currentgrid[j,],futuregrid[j,])))) } euc &lt;- data.frame(euc) names(euc) &lt;- c(&quot;Long&quot;,&quot;Lat&quot;,&quot;Vulnerability&quot;) coords&lt;-birdgrid[,c(&quot;X&quot;,&quot;Y&quot;)] euc &lt;- matrix(data=NA,nrow=nrow(futuregrid),ncol=3) for(j in 1:nrow(currentgrid)) { euc[j,] &lt;- c(as.numeric(coords[j,]),as.numeric(dist(rbind(currentgrid[j,],futuregrid[j,])))) } euc &lt;- data.frame(euc) names(euc) &lt;- c(&quot;Long&quot;,&quot;Lat&quot;,&quot;Vulnerability&quot;) This again takes a moment. Once we have this metric, we can easily map genomic vulnerability across the range. First by choosing a color palette, then by converting a dataframe of distances to a spatial polygon and giving it the same extent as the map it’s going to be plotted on and finally, mapping in ggplot. colramp2=brewer.pal(n = 9, name = &quot;RdYlBu&quot;)[c(1:9)] Futcolors&lt;-colorRampPalette(colramp2) euc_sp&lt;-st_as_sf(euc,coords=c(&quot;Long&quot;, &quot;Lat&quot;)) st_crs(euc_sp) &lt;- st_crs(coast_cropped) mapg &lt;- ggplot() + geom_sf(data = coast_cropped) + geom_sf(data = countries_cropped, fill = NA) + geom_sf(data = states_cropped, fill = NA) + geom_point(data=euc,aes(x=Long,y=Lat,color=Vulnerability),pch=15) + scale_colour_gradientn(colours = Futcolors(100))+ #geom_spatial_point(data=Epabu, aes(Long,Lat),col=&quot;black&quot;) + geom_sf(data=wifl_shape,fill=NA) + theme_bw() mapg colramp2=brewer.pal(n = 9, name = &quot;RdYlBu&quot;)[c(1:9)] Futcolors&lt;-colorRampPalette(colramp2) euc_sp&lt;-st_as_sf(euc,coords=c(&quot;Long&quot;, &quot;Lat&quot;)) st_crs(euc_sp) &lt;- st_crs(coast_cropped) mapg &lt;- ggplot() + geom_sf(data = coast_cropped) + geom_sf(data = countries_cropped, fill = NA) + geom_sf(data = states_cropped, fill = NA) + geom_point(data=euc,aes(x=Long,y=Lat,color=Vulnerability),pch=15) + scale_colour_gradientn(colours = Futcolors(100))+ #geom_spatial_point(data=Epabu, aes(Long,Lat),col=&quot;black&quot;) + geom_sf(data=wifl_shape,fill=NA) + theme_bw() mapg FIGURE 6.6: Heat map of genomic vulnerability under the 2050 2.6 emission future prediction How does this look if we plot the second scenario of higher gas emissions? Compare and contrast the two scenarios, keeping in mind that the colors are scaled within each scenario. futuregrid2=cbind(future2[,c(&quot;X&quot;,&quot;Y&quot;)],predict(wiflforest,future2[,predictors_uncor])) coords&lt;-birdgrid[,c(&quot;X&quot;,&quot;Y&quot;)] euc2 &lt;- matrix(data=NA,nrow=nrow(futuregrid2),ncol=3) for(j in 1:nrow(currentgrid)) { euc2[j,] &lt;- c(as.numeric(coords[j,]),as.numeric(dist(rbind(currentgrid[j,],futuregrid2[j,])))) } euc2 &lt;- data.frame(euc2) names(euc2) &lt;- c(&quot;Long&quot;,&quot;Lat&quot;,&quot;Vulnerability&quot;) euc_sp2&lt;-st_as_sf(euc2,coords=c(&quot;Long&quot;, &quot;Lat&quot;)) st_crs(euc_sp2) &lt;- st_crs(coast_cropped) mapg2 &lt;- ggplot() + geom_sf(data = coast_cropped) + geom_sf(data = countries_cropped, fill = NA) + geom_sf(data = states_cropped, fill = NA) + geom_point(data=euc2,aes(x=Long,y=Lat,color=Vulnerability),pch=15) + scale_colour_gradientn(colours = Futcolors(100))+ #geom_spatial_point(data=Epabu, aes(Long,Lat),col=&quot;black&quot;) + geom_sf(data=wifl_shape,fill=NA) + theme_bw() mapg2 FIGURE 6.7: Heat map of genomic vulnerability under the 2050 8.5 emission future prediction "],["conclude.html", "Session 7 Concluding Remarks / Parting Words", " Session 7 Concluding Remarks / Parting Words Something here. Bay RA, Harrigan RJ, Le Underwood V et al. (2018) Genomic signals of selection predict climate-driven population declines in a migratory bird. Science, 359, 83–86. Robertson JM, Murphy MA, Pearl CA et al. (2018) Regional variation in drivers of connectivity for two frog species (rana pretiosa and r. Luteiventris) from the US pacific northwest. Molecular ecology, 27, 3242–3256. Ruegg K, Anderson EC, Somveille M et al. (2021) Linking climate niches across seasons to assess population vulnerability in a migratory bird. Global Change Biology. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
